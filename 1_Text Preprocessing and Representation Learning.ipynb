{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> MSBA 6461: Advanced AI for Natural Language Processing </center>\n",
    "<center> Summer 2025, Mochen Yang </center>\n",
    "\n",
    "## <center> Text Preprocessing and Representation Learning </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [What is NLP?](#nlp_intro)\n",
    "    - [Types of NLP Tasks and Their Applications](#nlp_application)\n",
    "    - [Some Basic NLP Terminologies](#nlp_terminology)\n",
    "1. [Why is NLP Challenging?](#challenge)\n",
    "    - [Language is Complex](#lang_complex)\n",
    "    - [Key Technical Challenges of NLP](#tech_challenge)\n",
    "1. [Text Pre-processing](#preprocess)\n",
    "1. [Bag-of-Words Representation](#bow)\n",
    "    - [Binary, Frequency, and TF-IDF](#bow_methods)\n",
    "    - [Implementation](#bow_implementation)\n",
    "    - [Feature Selection](#bow_feature_selection)\n",
    "    - [Exercise](#bow_exercise)\n",
    "1. [Word Embeddings](#embedding)\n",
    "    - [Basic Idea of Learning Word Embeddings](#embedding_idea)\n",
    "    - [Learning Word Embeddings](#embedding_learning)\n",
    "    - [Two Architectures: Skip-Gram and Continuous Bag-of-Words](#embedding_model)\n",
    "    - [Implementation](#embedding_implementation)\n",
    "1. [Additional Resources](#resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Natural Language Processing (NLP)? <a name=\"nlp_intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Natural Language Processing (NLP) is one of the most important branches of machine learning and artificial intelligence. \n",
    "- The overarching goal of NLP is to **process and \"understand\" human languages**.\n",
    "- It is often treated as \"benchmark\" for artificial intelligence (together with image recognition / computer vision). This is because we believe that the use of rich and structured languages in communications is a sign of strong intelligence. So, if we can teach machines how to use language to communicate, we have, in a sense, created artificial intelligence.\n",
    "- I use the term NLP interchangeably with \"text mining\", \"text analytics\", \"natural language understanding\", etc. Their differences are not that important for our purposes.\n",
    "- NLP is both simple and hard. It is simple because all of what you have learned about machine learning can be applied here. It is hard because there are plenty of nuances in human languages that make it difficult for machines to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of NLP Tasks and Their Applications <a name=\"nlp_application\"></a>\n",
    "\n",
    "There are a large number of common NLP tasks and applications. Here are some representative examples:\n",
    "- **Sentiment analysis:** \n",
    "    - Based on content of a text, predict its sentiment (e.g., positive vs. negative). \n",
    "    - Applications: customer support, social media analytics, [stock market prediction](http://cs229.stanford.edu/proj2011/GoelMittal-StockMarketPredictionUsingTwitterSentimentAnalysis.pdf), ...\n",
    "- **Topic modeling:**\n",
    "    - Automatically discover prevalent topics from a collection of texts (typically in an unsupervised manner).\n",
    "    - Applications: automated news curation, recommender systems, ...\n",
    "- **Machine translation:**\n",
    "    - Input texts in language A, output translated texts in language B. \n",
    "- **Question Answering:**\n",
    "    - Input a question / query, output an answer in natural / human language.\n",
    "    - Applications: conversational AI, Chat Bot, ...\n",
    "- **Voice AI:**\n",
    "    - Perform actions based on voice commands.\n",
    "    - Application: Apple Siri, Google Assistant, Amazon Alexa, ...\n",
    "- **Large Language Models:**\n",
    "    - Does (almost) all of the above. Current state-of-the-art of NLP - an AI that understands you, communicates with you, and does what you request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Basic NLP Terminologies <a name=\"nlp_terminology\"></a>\n",
    "\n",
    "- **Document**: a piece of text which you are interested in analyzing.\n",
    "    - E.g., an article, a tweet, a Facebook post\n",
    "- **Corpus**: a collection of documents. This is your \"dataset\".\n",
    "- **Word Token**: a single word.\n",
    "- **Vocabulary**: the collection of all unique word tokens in a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is NLP Challenging? <a name=\"challenge\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language is Complex <a name=\"lang_complex\"></a>\n",
    "\n",
    "Language is so much more than just a string of texts. It is:\n",
    "- Highly context-dependent. Different words do not necessarily imply different meanings, and the same word does not imply same meaning in all contexts.\n",
    "    - Different words, same meaning: ill vs. sick;\n",
    "    - Same word, different meanings: bear (the animal vs. the action);\n",
    "    - Same word, related meanings: book (the physical book vs. the action of booking)\n",
    "    - Think about all the puns\n",
    "- Often ambiguous and subtle. \n",
    "    - ![image](images/ambiguity.jpg)\n",
    "    - Think about the following two sentences:\n",
    "        - \"Margaret invited Susan for a visit, and she gave her a good lunch.\" Who gave who a good lunch?\n",
    "        - \"Margaret invited Susan for a visit, but she told her she had to go to work.\" Who had to go to work?\n",
    "        - Understanding these sentences require knowledge of \"social norm\". Even humans are sometimes confused...\n",
    "- Varying with different cultures.\n",
    "    - Example: there is no concept of \"verb tenses\" in Chinese.\n",
    "- Not always a straightforward representation of meaning / intent. Same sentence can mean different things when said in different ways.\n",
    "    - **_He_** went to that store.\n",
    "    - He **_went_** to that store.\n",
    "    - He went to **_that_** store.\n",
    "    - He went to that **_store_**.\n",
    "- Often \"noisy\":\n",
    "    - Contains misspellings, filler words, redundancies, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Technical Challenges of NLP <a name=\"tech_challenge\"></a>\n",
    "\n",
    "From a technical perspective, NLP is difficult mainly because of two reasons:\n",
    "1. **Representation**: How to turn texts into numbers that capture meaningful information of the texts?\n",
    "    - Representation learning is a constant theme in NLP and in machine learning in general. It is about how to best represent your data for the learning task;\n",
    "    - This shouldn't be a completely new concept. You have seen it before outside the context of neural networks (think about SVM);\n",
    "    - We will discuss two representation approaches in NLP: (1) the bag-of-words approach (captures existence/frequency but almost no semantic meaning) and (2) the embedding approach (captures _some_ semantic meaning).\n",
    "2. **Modeling Technique/Architecture**: How to design appropriate modeling techniques that accommodate (and perhaps take advantage of) the unique properties of human language?\n",
    "    - As an example, recurrent neural networks (discussed in a later lecture) are successful for NLP because it is _sequential_, which nicely captures the sequential nature of language;\n",
    "    - The idea of mapping the modeling technique to the unique characteristics of the data goes beyond NLP. For example, think about convolutional neural nets (CNNs) and image recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-processing <a name=\"preprocess\"></a>\n",
    "\n",
    "In many NLP tasks, before we generate numerical representations of texts, we would want to first perform some common pre-processing on it. Here, we will introduce a few common pre-processing steps and when to use them or not.\n",
    "\n",
    "<font color=\"blue\">It is important to realize: none of the pre-processing steps are \"mandatory\" or cannot be changed - whether and how to do them always depend on your specific machine learning task.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common text pre-processing steps:\n",
    "1. **Tokenization**: break down each document into single word tokens.\n",
    "    - When to use: when the unit of analysis for your task is words.\n",
    "    - When not to use: when the unit of analysis is not words, but perhaps characters or sentences.\n",
    "    - Modern NLP models (such as LLMs) need very complex tokenization modules. See [Andrej Karpathy's Lecture on Tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE) as an example.\n",
    "2. **Lower-casing**: transform each word to lower-case.\n",
    "    - When to use: when casing does not provide important information to your task. Lower-casing reduces total number of unique tokens.\n",
    "    - When not to use: when casing carries important information. E.g., capitalization of special abbreviations (think about name entity recognition).\n",
    "3. **Punctuation removal**: remove punctuations.\n",
    "    - When to use: when your task only cares about the actual words.\n",
    "    - When not to use: when punctuation carries important information. E.g., the question mark in a question classification task.\n",
    "3. **Stop-words removal**: remove \"filler\" words, such as \"the\", \"a\" in English.\n",
    "    - When to use: when filler words do not carry actual meanings. Removing them again reduces the total number of unique tokens.\n",
    "    - When not to use: when filler words carry important semantic meanings.\n",
    "4. **Stemming**: reduce words to their semantic \"roots\". E.g., {\"engineer\", \"engineering\", \"engineered\"} $\\Rightarrow$ \"engine\"\n",
    "    - When to use: when your task only cares about word stems.\n",
    "    - When not to use: when different forms of the same stem carry important information (usually the case).\n",
    "    - In practice, we don't do stemming that often because it gets rid of important information.\n",
    "5. **N-Gram generation**: generate phrases that contain N _consecutive_ words. E.g., 2-grams of the \"welcome to my class\" include \"welcome to\", \"to my\", and \"my class\".\n",
    "    - When to use: when word phrases have important information beyond their constituent words. E.g., \"customer service\" together has more information than just \"customer\" + \"service\".\n",
    "    - When not to use: be mindful that there can be a lot of N-grams! If they don't provide much additional value, adding them can drastically increase the number of features and make your algorithms much slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words Representation <a name=\"bow\"></a>\n",
    "\n",
    "Bag-of-Words is a class representation strategy to turn text into numbers. \n",
    "- Simple, commonly used way of representing textual data.\n",
    "- Each document is represented by a set of individual words. Different words represent different attributes / features (columns).\n",
    "- Importance of words in a document is reflected by a numeric value.\n",
    "- Several ways to construct such numeric value:\n",
    "    - Binary\n",
    "    - Frequency\n",
    "    - TF-IDF\n",
    "\n",
    "Let's illustrate how each one works with the following mini corpus of 3 documents:\n",
    "\n",
    "![Corpus](images/corpus.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary, Frequency, and TF-IDF <a name=\"bow_methods\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary Representation**: for each word in the vocabulary, 1 if it appears in a document, 0 otherwise. You have seen this representation approach before! Recall binary representation of shopping basket (e.g., for association rule mining).\n",
    "\n",
    "![Binary](images/corpus_binary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Term Frequency Representation**: words that appear multiple times in a document are likely to be more important to understand that document. Let's enhance the binary representation by taking into account the frequency.\n",
    "\n",
    "![Frequency](images/corpus_frequency.png)\n",
    "\n",
    "This representation is also called a **term frequency matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**: words that appear in many documents of a corpus are less important than words that only appear in a few documents. \n",
    "- Words that only appear in a few documents effectively distinguish those documents from the rest, and therefore should bear more weight in representing the documents that contain them.\n",
    "- TF-IDF is a representation that encodes both term frequency information and term \"uniqueness\" information.\n",
    "\n",
    "The TF-IDF value of a specific word $w$ in a document $D$ is computed as follows:\n",
    "- TF: term frequency, the number of times that $w$ appears in $D$;\n",
    "- IDF: inverse document frequency, $IDF = log(\\frac{N}{N_w})$, where $N$ is the total number of documents in corpus and $N_w$ is the number of documents that contain $w$.\n",
    "- TF-IDF value: $TF \\cdot IDF$.\n",
    "- Higher TF means a word appears more frequently in a document; Higher IDF means a word appear more uniquely in a document.\n",
    "- Additionally, there are other ways to compute the IDF value (usually for technical reasons). E.g., $IDF = log(\\frac{N}{N_w+1})$ to avoid division by 0, or $IDF = 1 + log(\\frac{N}{N_w})$ to force a lower bound of IDF of 1.\n",
    "\n",
    "![TF-IDF](images/corpus_tfidf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation <a name=\"bow_implementation\"></a>\n",
    "\n",
    "We will use the `scikit-learn` package and, for stemming, use the `nltk` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following code and run it to install nltk if you haven't already\n",
    "# %pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "# need pandas only to print out the TF-IDF matrix\n",
    "import pandas as pd\n",
    "# need \"punkt\" later on for stemming\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just use the toy corpus as an example\n",
    "corpus = [\n",
    "    'Welcome to data analytics!',\n",
    "    'Data analytics study data.',\n",
    "    'Data Mining finds patterns from data.',\n",
    "    'Text Mining finds patterns from text.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analytics</th>\n",
       "      <th>analytics study</th>\n",
       "      <th>data</th>\n",
       "      <th>data analytics</th>\n",
       "      <th>data mining</th>\n",
       "      <th>finds</th>\n",
       "      <th>finds patterns</th>\n",
       "      <th>mining</th>\n",
       "      <th>mining finds</th>\n",
       "      <th>patterns</th>\n",
       "      <th>patterns data</th>\n",
       "      <th>patterns text</th>\n",
       "      <th>study</th>\n",
       "      <th>study data</th>\n",
       "      <th>text</th>\n",
       "      <th>text mining</th>\n",
       "      <th>welcome</th>\n",
       "      <th>welcome data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.412640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334067</td>\n",
       "      <td>0.412640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523381</td>\n",
       "      <td>0.523381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.325334</td>\n",
       "      <td>0.412645</td>\n",
       "      <td>0.526772</td>\n",
       "      <td>0.325334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.412645</td>\n",
       "      <td>0.412645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.491805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385254</td>\n",
       "      <td>0.303739</td>\n",
       "      <td>0.303739</td>\n",
       "      <td>0.303739</td>\n",
       "      <td>0.303739</td>\n",
       "      <td>0.303739</td>\n",
       "      <td>0.385254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.261242</td>\n",
       "      <td>0.261242</td>\n",
       "      <td>0.261242</td>\n",
       "      <td>0.261242</td>\n",
       "      <td>0.261242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.331352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.662704</td>\n",
       "      <td>0.331352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   analytics  analytics study      data  data analytics  data mining  \\\n",
       "0   0.412640         0.000000  0.334067        0.412640     0.000000   \n",
       "1   0.325334         0.412645  0.526772        0.325334     0.000000   \n",
       "2   0.000000         0.000000  0.491805        0.000000     0.385254   \n",
       "3   0.000000         0.000000  0.000000        0.000000     0.000000   \n",
       "\n",
       "      finds  finds patterns    mining  mining finds  patterns  patterns data  \\\n",
       "0  0.000000        0.000000  0.000000      0.000000  0.000000       0.000000   \n",
       "1  0.000000        0.000000  0.000000      0.000000  0.000000       0.000000   \n",
       "2  0.303739        0.303739  0.303739      0.303739  0.303739       0.385254   \n",
       "3  0.261242        0.261242  0.261242      0.261242  0.261242       0.000000   \n",
       "\n",
       "   patterns text     study  study data      text  text mining   welcome  \\\n",
       "0       0.000000  0.000000    0.000000  0.000000     0.000000  0.523381   \n",
       "1       0.000000  0.412645    0.412645  0.000000     0.000000  0.000000   \n",
       "2       0.000000  0.000000    0.000000  0.000000     0.000000  0.000000   \n",
       "3       0.331352  0.000000    0.000000  0.662704     0.331352  0.000000   \n",
       "\n",
       "   welcome data  \n",
       "0      0.523381  \n",
       "1      0.000000  \n",
       "2      0.000000  \n",
       "3      0.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the TfidfVectorizer function allows you to perform most of the common pre-processing steps, and generate bag-of-words representations\n",
    "# documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "# For example, let's do tokenization, lower-casing, stop-words removal, extract up to 2-grams, and get the TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase = True, \n",
    "    tokenizer = None,  # by default, it does word tokenization AND punctuation removal. You can replace it with a function that does other types of tokenziation\n",
    "    stop_words = 'english',  # remove common English stopwords (it has a dictionary in the backend)\n",
    "    ngram_range = (1,2),  # extract 1-gram (single tokens) and 2-gram (phrases of 2 words)\n",
    "    use_idf = True  # means that we want to get the TF-IDF, rather than just TF\n",
    ")\n",
    "\n",
    "# Now apply it to the corpus and get the TF-IDF matrix\n",
    "tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Next, print it out in a nice readable format (this step is just to show you what it looks like, it's usually not needed)\n",
    "df = pd.DataFrame(tfidf.todense(), columns = vectorizer.get_feature_names())\n",
    "df\n",
    "\n",
    "# In case you are wonder why the word \"analytics\" have different TF-IDF values in documents 1 and 2, check the \"norm\" parameter in the TfidfVectorizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' welcom to data analyt !', ' data analyt studi data .', ' data mine find pattern from data .', ' text mine find pattern from text .']\n"
     ]
    }
   ],
   "source": [
    "# The TfidfVectorizer doesn't perform stemming (and we don't do stemming all that much in practice)\n",
    "# To do it, we need the help of the PorterStemmer from the nltk package\n",
    "# The implementation here is a bit awkward, just to show you what the stemmed documents look like\n",
    "ps = PorterStemmer()\n",
    "corpus_stemmed = []\n",
    "for doc in corpus:\n",
    "    # stemmer only works on words, so we have to tokenize first in nltk...\n",
    "    words = word_tokenize(doc)\n",
    "    doc_stemmed = ''\n",
    "    for w in words:\n",
    "        w_stemmed = ps.stem(w)\n",
    "        doc_stemmed += ' ' + w_stemmed\n",
    "    corpus_stemmed.append(doc_stemmed)\n",
    "print(corpus_stemmed)\n",
    "\n",
    "# Then use the same TfidfVectorizer to get TF-IDF matrix (code omitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection <a name=\"bow_feature_selection\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you turn a corpus into a numeric matrix (e.g., using TF-IDF), with rows representing different data points (documents) and columns representing different features (tokens), then everything you have learned becomes readily applicable.\n",
    "- Classification: e.g., sentiment prediction\n",
    "- Clustering: e.g., topic modeling\n",
    "- ...\n",
    "\n",
    "One unique characteristic of the representation generated by bag-of-words approach is that there are usually a large number of columns, but for each row, only a small number of cells have non-zero values. This is a kind of **sparsity** in your data. To make sure that your ML algorithms still runs efficiently and performs well, it becomes very important to do feature selection. Generally speaking, filter approach is more appropriate here because wrapper approach can be computationally too expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise <a name=\"bow_exercise\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practice text pre-processing on the Facebook post dataset that you will using for Assignment 1. Download the \"FB_posts_labeled.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7961\n"
     ]
    }
   ],
   "source": [
    "# import the dataset\n",
    "posts = []\n",
    "for line in open(\"../datasets/FB_posts_labeled.txt\"):\n",
    "    if 'postId' not in line:\n",
    "        pid, message, Appreciation, Complaint, Feedback = line.rstrip('\\n').split('\\t')\n",
    "        posts.append(message)\n",
    "print(len(posts))\n",
    "# the Appreciation, Complaint, Feedback columns contain class labels. You may want to store them in lists as well (code ignored here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, use TfidfVectorizer to do preprocessing and produce TFIDF. Decide for yourself which processing steps you want.\n",
    "# How many features do you get after preprocessing?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, build a multi-class classification model (e.g., using a decision tree approach)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy your model on the unlabeled data\n",
    "# Remember that you need to preprocess the unlabeled texts in the same way as you did for the labeled texts - this can be done by applying the same TfidfVectorizer to \"transform\" the unlabeled texts (see documentation for TfidfVectorizer)\n",
    "unlabel_posts = []\n",
    "for line in open(\"../datasets/FB_posts_unlabeled.txt\"):\n",
    "    if 'postId' not in line:\n",
    "        pid, message = line.rstrip('\\n').split('\\t')\n",
    "        unlabel_posts.append(message)\n",
    "print(len(unlabel_posts))\n",
    "\n",
    "# transform the texts into TF-IDF matrix\n",
    "\n",
    "# Deploy your model to make predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings <a name=\"embedding\"></a>\n",
    "\n",
    "Bag-of-Words representation is straightforward, easy to understand, and indeed captures some meaningful information about the texts (specifically, frequency information). However, it suffers from very serious limitations - **it captures very little about the semantic meaning of different words.** Imagine you take each document of a corpus and then _randomly shuffle the sequence of words_ in it, it won't change the TF-IDF representation _at all_. \n",
    "\n",
    "This means bag-of-words representation does not account for any meaningful information that's encoded in the _sequence_ of words in a document. Fundamentally, this is because bag-of-word approach essentially _represent each word in a vocabulary simply by an index_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Idea of Word Embeddings <a name=\"embedding_idea\"></a>\n",
    "\n",
    "The key idea of \"word embeddings\" is to **represent each word in a vocabulary by a numeric vector**, rather than a single number (e.g., an index, as in bag-of-word). The word \"embedding\" is a mathematical concept - it means represent some entity in a multi-dimensional space.\n",
    "\n",
    "<font color=\"blue\">Intuitions for why using a vector representation:</font>\n",
    "- Compared to a single number representation, a vector of numbers can \"encode\" richer meanings of the word;\n",
    "- Mathematically speaking, a vector of $N$ numbers marks a unique location in the $N$-dimensional space. You can conceptually think of this $N$-dimensional space as \"the space of all possible words\";\n",
    "- A vector representation makes it meaningful to compute the \"distance\" between two words. As we will show later, such distance turns out to capture the semantic similarity between two words (i.e., how the meanings of two words are similar or dissimilar). This is very useful for NLP tasks. In fact, most NLP deep learning models use word embeddings (rather than TF-IDF values) as input.\n",
    "\n",
    "Finally, it's useful to clarify that the concept of \"embeddings\" is very general and can be applied to other things, like sentence, paragraph, document, and non-language objects such as graph, etc. Here we focus on word embeddings because individual words are usually the \"unit\" of language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Word Embeddings <a name=\"embedding_learning\"></a>\n",
    "\n",
    "So, how do we actually get word embeddings? More specifically:\n",
    "- For each word $W$ in the vocabulary, suppose we want to learn a $D$-dimensional embedding, i.e., we want to represent $W$ by vector $(w_1, w_2, \\ldots, w_D)$.\n",
    "- Instead of assigning these $D$ numbers, we want to _learn them from data_. The \"data\" here simply means natural language, i.e., we want to learn the embedding representation from natural language;\n",
    "- We need to design a proper model architecture for this learning task.\n",
    "\n",
    "<font color=\"red\">OK, but what is the learning task that will give us word embeddings?</font> Recall that the goal of learning word embeddings is that the vector representation can capture some semantic meaning of the word. So what does \"semantic meaning\" mean? Think about how you might teach a young child the meaning of a word (hint: you put that word in sentences / scenarios / contexts).\n",
    "\n",
    "In linguistics, a basic observation of language is that **to understand the meaning of a word, you need to understand the context surrounding the focal word**. In other words, the semantic meaning of a word is strongly associated with the context, i.e., the words that appear before and after the focal word. <font color=\"blue\">This is the basis for learning word embedding: we want to design a learning task that associates a focal word with its context, and the learning task produces embeddings as a result.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Architectures: Skip-Gram and Continuous Bag-of-Words <a name=\"embedding_model\"></a>\n",
    "\n",
    "More specifically, there are two prevalent architectures for learning word embeddings: Continuous Bag-of-Words and Skip-Gram. To describe them, let's first clarify a few notations:\n",
    "- $V$: vocabulary, i.e., the collection of all words in some corpus you are using;\n",
    "- $C$: width of the context, i.e., the number of words before and after a focal word that you consider to be its context. This is chosen by the user;\n",
    "- $W_t$: the focal word;\n",
    "- $\\{W_{t-C}, \\ldots, W_{t-1}, W_{t+1}, \\ldots, W_{t+C}\\}$: words in the context.\n",
    "\n",
    "\n",
    "**Continuous Bag-of-Words Model**: use the context to predict the focal word.\n",
    "- Training task: maximize accuracy of predicting the focal word.\n",
    "- Architecture: usually a basic feed forward neural network with one input layer, 1 hidden layer, and 1 output layer. Shown as below.\n",
    "- Input: context words $\\{W_{t-C}, \\ldots, W_{t-1}, W_{t+1}, \\ldots, W_{t+C}\\}$, each having a $D$-dimensional embedding representation;\n",
    "- Output: $\\Pr(W_t | W_{t-C}, \\ldots, W_{t-1}, W_{t+1}, \\ldots, W_{t+C})$, computed via softmax;\n",
    "- Loss function: this gets a bit technical. There are multiple ways to formulate the loss function. For example, you can treat this task as a _multi-class classification_ and use cross-entropy between actual $W_t$ (one-hot encoded) and the probability computed via softmax. Alternatively, you can treat it as a _binary classification task_ and simply use sigmoid (focal word gets label 1 and all other words get label 0). This is also known as _negative sampling_.\n",
    "\n",
    "![CBOW](images/CBOW.png)\n",
    "\n",
    "Image source: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf) Figure 1.  The \"projection layer\" is simply a hidden layer that sum over the word embedding vectors of all context words.\n",
    "\n",
    "**Skip-Gram Model**: use the focal word to predict its context.\n",
    "- Training task: maximize accuracy of predicting the context words.\n",
    "- Architecture: usually a basic feed forward neural network with one input layer, 1 hidden layer, and 1 output layer. Shown as below.\n",
    "- Input: focal word $W_t$, one-hot encoding representation;\n",
    "- Output: $\\Pr(W_o | W_t)$, computed via softmax, where $W_o \\in \\{W_{t-C}, \\ldots, W_{t-1}, W_{t+1}, \\ldots, W_{t+C}\\}$ is a specific context word;\n",
    "- Loss function: Same as CBOW, you can treat it as multi-class classification and use cross-entropy, or treat it as binary classification and use sigmoid.\n",
    "\n",
    "![Skip-Gram](images/SG.png)\n",
    "\n",
    "Image source: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf) Figure 1.  The \"projection layer\" here simply stores the word embedding vector of input word. Also, unlike what's depicted here, you are not trying to \"simultaneously\" predict all the contexts words at once - you are predicting them one at a time.\n",
    "\n",
    "<font color=\"red\">Question: looking back at the learning task behind CBOW or Skip-Gram, is it supervised learning, unsupervised learning, or something else?</font> Hint: think about what are the input data and what are the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation <a name=\"embedding_implementation\"></a>\n",
    "\n",
    "Now, let's learn some word embeddings from data. Note that there are various implementations available (e.g., see [Word2Vec](https://www.tensorflow.org/tutorials/text/word2vec) for an implementation based on `tensorflow`), and there are pre-trained word embeddings ready for use (e.g., [Google's Word2Vec embeddings](https://code.google.com/archive/p/word2vec/) and [GloVe](https://nlp.stanford.edu/projects/glove/)). We are going to use the `gensim` package because it provides an easy-to-use function. The downside, however, is that it masks the various technical details behind the curtain. For those interested in digging deeper, I refer you to the [Word2Vec](https://www.tensorflow.org/tutorials/text/word2vec) tutorial.\n",
    "\n",
    "I will use the labeled Facebook posts as an example. However, embedding training does not need the content classes, so you can also train embeddings on all posts (labeled and unlabeled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following code and run it to install gensim if you haven't already\n",
    "# %pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the following packages\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the textual data and perform the following pre-processing steps: lowercase, tokenize, remove stop words and punctuations\n",
    "# Note that we don't use TfidfVectorizer here because gensim's Word2Vec function require lists of word tokens as input, not a TF-IDF matrix\n",
    "texts = []\n",
    "for line in open(\"../datasets/FB_posts_labeled.txt\"):\n",
    "    if 'postId' not in line:\n",
    "        text = line.rstrip('\\n').split('\\t')[1]\n",
    "        processed_text = []\n",
    "        # Lowercasing\n",
    "        text = text.lower()\n",
    "        # Tokenization with NLTK\n",
    "        tokens = nltk.tokenize.word_tokenize(text)\n",
    "        # Remove stop words and punctuations with NLTK\n",
    "        for token in tokens:\n",
    "            if token not in nltk.corpus.stopwords.words('english') and token not in string.punctuation:\n",
    "                processed_text.append(token)\n",
    "        # put processed text back into a list (remove the cases where nothing left after pre-processing)\n",
    "        if len(processed_text) > 0:\n",
    "            texts.append(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7961\n",
      "[['great'], ['yum', 'yum'], ['yummm'], ['sweet'], ['nice'], ['nice'], ['winner'], ['awesome'], ['yay'], ['gmo']]\n"
     ]
    }
   ],
   "source": [
    "# Check results\n",
    "print(len(texts))\n",
    "print(texts[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Gensim to train word embeddings\n",
    "# Here is the documentation: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "model = Word2Vec(sentences = texts,  # input should be a list of lists of tokens, like our output from preprocessing\n",
    "                 vector_size = 128,  # dimension of embedding (this parameter may be named size if you are using an older version of Gensim)\n",
    "                 window = 2,  # size of context window\n",
    "                 min_count = 1,  # remove very infrequent words\n",
    "                 sg = 1,  # skip-gram, set to 0 if you want CBOW\n",
    "                 workers = 4)  # parallel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.27951396  0.04518483  0.14703202  0.06643444 -0.16648538 -0.3112482\n",
      " -0.28273353 -0.16407007 -0.18235622 -0.16050574  0.29904884  0.48090428\n",
      "  0.22937815 -0.56048524  0.41103232  0.07420124  0.27748072  0.13929774\n",
      "  0.46483663 -0.00558215 -0.5188526   0.21942575  0.6100703   0.22987051\n",
      " -0.19763136  0.41336045  0.30242968 -0.3826717  -0.06097787  0.22525214\n",
      " -0.2110981   0.14601848  0.02881903 -0.03323489 -0.22210634  0.43910885\n",
      "  0.36395574  0.09123889 -0.15808675 -0.17459849  0.43612778  0.27393544\n",
      " -0.09280729  0.02944438  0.65702415  0.33855242 -0.37338603  0.30026218\n",
      " -0.13906154 -0.11820845 -0.43921047  0.19765656 -0.15390289  0.25952098\n",
      " -0.11998459 -0.23531444  0.05618233  0.21987693  0.24325812 -0.32636687\n",
      " -0.31630152 -0.18367891 -0.14578986 -0.33956698 -0.33355927 -0.14406939\n",
      " -0.28394958  0.20856343 -0.25876647 -0.10684247 -0.05088499 -0.11928397\n",
      " -0.17044857 -0.22787368  0.11344094  0.2747096  -0.2483298   0.1316946\n",
      " -0.07170007  0.00473584 -0.72508925 -0.19478427  0.10218559  0.06099844\n",
      "  0.07704532 -0.4686608   0.22053221  0.1651183   0.02814141  0.04975092\n",
      "  0.12986074  0.13960364  0.22117025 -0.0539746   0.40933004  0.33550784\n",
      " -0.50802517  0.17182481  0.666068    0.10600447 -0.19108395 -0.11346254\n",
      "  0.49655783  0.2713527   0.4718027  -0.09771032  0.43070215 -0.11202471\n",
      " -0.53624475  0.08095467 -0.11101097  0.13051262  0.3062282   0.39132756\n",
      " -0.06047982 -0.27531457 -0.15565756 -0.4652689   0.4101478   0.5943775\n",
      "  0.24756059  0.14752792  0.35038042 -0.03462468 -0.10724276 -0.6417642\n",
      " -0.2310988  -0.01511978]\n"
     ]
    }
   ],
   "source": [
    "# take a look at one particular word embedding\n",
    "vector = model.wv['southwest']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89967453\n"
     ]
    }
   ],
   "source": [
    "# We can quantify the similarity between two words by taking the cosine distance between their embeddings\n",
    "print(model.wv.similarity('happy', 'great'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('united', 0.9768410325050354),\n",
       " ('american', 0.9739261865615845),\n",
       " ('delta', 0.9716732501983643),\n",
       " ('flying', 0.952656090259552),\n",
       " ('airline', 0.9221729040145874),\n",
       " ('fly', 0.918926477432251),\n",
       " ('disappointed', 0.9073857069015503),\n",
       " ('airlines', 0.9002877473831177),\n",
       " ('petsmart', 0.8937267661094666),\n",
       " ('life', 0.890683114528656)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can try to find words that are semantically similar to a given word (i.e., synonym)\n",
    "# Under the hood, it is calculating the cosine distance between the focal word and all other words, then pick top N\n",
    "model.wv.most_similar(positive = ['southwest'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources <a name=\"resource\"></a>\n",
    "\n",
    "- Original research paper on word embeddings: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf);\n",
    "- Additional technical details and extensions of word embeddings: [Learning Word Embedding](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html) and [word2vec Explained: Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method](https://arxiv.org/pdf/1402.3722.pdf);\n",
    "- A nice blog post about self-supervised learning: [Self-Supervised Representation Learning](https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html).\n",
    "- A step by step illustration of the maths behind SG and CBOW models: [blog post](https://mochenyang.github.io/mochenyangblog/machine-learning/exposition/2022/07/12/Word-Embedding.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "33da5c9f9241751bd42d0b7dcfd19844a4f76f3f046b70cb7b5ef5690765ed82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
