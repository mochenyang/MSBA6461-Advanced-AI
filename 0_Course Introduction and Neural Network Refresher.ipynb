{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> MSBA 6461: Advanced AI for Natural Language Processing </center>\n",
    "<center> Summer 2025, Mochen Yang </center>\n",
    "\n",
    "## <center> Course Introduction and Neural Network Refresher </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Course Introduction](#intro)\n",
    "1. [Refresher on Neural Networks](#nn)\n",
    "    - [Feedforward Neural Network Architecture](#nn_basic)\n",
    "        - [Layers](#nn_basic_layers)\n",
    "        - [Weights](#nn_basic_weights)\n",
    "        - [Activation Functions](#nn_basic_activation)\n",
    "    - [Training Neural Networks](#nn_train)\n",
    "        - [Objective Function](#nn_train_objective)\n",
    "        - [Gradient Descent](#nn_train_gradient)\n",
    "        - [Batch, Epoch, and Stochastic Gradient Descent](#nn_train_sgd)\n",
    "    - [Build Neural Networks with Keras](#nn_implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Introduction <a name=\"intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Does This Course Cover?\n",
    "\n",
    "This course is designed to cover the following advanced topics in modern AI:\n",
    "1. **Natural Language Processing** (a.k.a. natural language understanding, text mining, text analytics, etc.).\n",
    "    - How to process textual data and build machine learning models for useful tasks.\n",
    "    - Topics range from basic text processing to representation learning to neural-network-based predictive models.\n",
    "    - Introduction to advanced NLP techniques, including attention mechanism and transformer architecture.\n",
    "2. **Language Models**:\n",
    "    - Overview of key technical elements in language models.\n",
    "    - Business applications of language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives of This Course\n",
    "\n",
    "I seek to achieve three goals in this course:\n",
    "1. Demystifying the \"AI Hype\":\n",
    "    - Most of the topics covered in this course have been hailed as \"AI\". But the what it means for a technology to be \"AI\" keeps changing;\n",
    "    - By diving into the technical details behind them, I hope to give you a more sober understanding of what they are and what they can / cannot do.\n",
    "2. Build working intuitions of advanced theoretical concepts, including:\n",
    "    - word embedding, recurrent neural network, LSTM, GRU, encoder-decoder architecture, attention, transformer, etc.\n",
    "3. Train technical skills to use the techniques discussed in this course:\n",
    "    - {`sklearn`, `nltk`, `keras`, `numpy`, `pytorch`}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (My Suggestions for) How to Learn in This Course\n",
    "\n",
    "Please consider the following suggestions when developing your own study plan:\n",
    "1. Ask questions, then ask more questions:\n",
    "    - These are complex / advance / challenging topics that are not easy to understand, and we go reasonably deep in this course;\n",
    "    - At some point, you will get confused. That's when you need to speak up (during class, schedule appointments with me, etc.) to get further clarifications;\n",
    "    - I reserve the right to say \"I don't know\".\n",
    "2. Be a critical learner. Always bear in mind that the topics we cover are \"cutting edge\":\n",
    "    - With the exception of the reinforcement learning module (which is relatively well established), almost all of the NLP topics we cover are developed in the past decade (many developed in the past 3-5 years);\n",
    "    - It means that you should not treat them as \"absolute truth\", but rather \"our current state-of-the-art\";\n",
    "    - Hopefully this prevents you from thinking \"deep learning solves all my problems\";\n",
    "3. Get your hands dirty:\n",
    "    - For me, the best way to learn complex algorithm / technical concept is to implement it myself;\n",
    "    - Coding in this course is often more than calling some well-written functions in some packages. It is also a mechanism for learning (e.g., we will try to implement the components of a transfomer model step by step);\n",
    "    - Spend time examining the code (and practice yourself) not only helps you with technical skills, but also facilitates the understanding of the underlying concepts and algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Are You Evaluated in This Course\n",
    "\n",
    "- There is no formal test.\n",
    "    - Therefore, you don't need to memorize anything. Focus on understanding and practicing yourself.\n",
    "- There will be two assignments ($50\\% \\times 2 = 100\\%$)\n",
    "    - Assignment 1: build a text classification model to predict the content category of Facebook posts. This offers you an opportunity to apply various NLP techniques we will cover in this course. For this assignment, your score depends objectively on (1) model performance and (2) exploration effort.\n",
    "    - Assignment 2: deep dive on one self-selected LLM-related topic and write a report about it. More details on Canvas. Make this assignment useful for YOU. By the end of this assignment, ideally, you should have become a decent expert on this topic, and the report you have written should easily become a high-quality blog post.\n",
    "\n",
    "As usual, final letter grade will be assigned based on the following cutoffs:\n",
    "\n",
    "![grade cutoffs](images/grade.png)\n",
    "\n",
    "\n",
    "<font color=\"blue\">On the use of large language models, such as ChatGPT, for assignments</font>: You are free to use them. Be aware of the potential risk, and disclose your usage clearly in the submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refresher on Neural Networks <a name=\"nn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network Architecture <a name=\"nn_basic\"></a>\n",
    "\n",
    "Feedforward neural networks (a.k.a. multilayer perceptron) are the most basic neural network architecture. This part offers a brief review of its key components, training strategy, and implementation using `keras`. Understanding of feedforward neural networks will be important for more advanced topics (e.g. recurrent neural networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers <a name=\"nn_basic_layers\"></a>\n",
    "\n",
    "A feedforward neural network consists of multiple **layers** connected one after another. Here is what a fully-connected feedforward neural network with three layers looks like.\n",
    "\n",
    "![NN](images/nn.png)\n",
    "\n",
    "Typically, a feedforward neural network has 1 input layer, 1 or more hidden layers in the middle, and 1 output layer. Each layer can contain one or more **neurons** (in the above case, there are 2, 3, 1 neurons in each of the three layers). Neurons in different layers have different roles:\n",
    "- Neurons in the input layer typically takes the inputs and pass them on to subsequent layers;\n",
    "- Neurons in the hidden layer **aggregates** information from the previous layer, apply some form of **transformation**, then pass on the resulting information;\n",
    "- Neurons in the output layer **aggregates** information from the previous layer, apply some form of **transformation**, then output the results (i.e., predictions).\n",
    "\n",
    "<font color=\"blue\">Intuition of a feedforward neural network</font>: takes input information, aggregates and transforms them in various different ways and steps, and finally produces some predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights <a name=\"nn_basic_weights\"></a>\n",
    "\n",
    "Weights in a neural network describe **how information from a previous layer is aggregated**. In a fully-connected feedforward neural network, there is a weight $w_{ij}$ between neurons $i$ and $j$ of two consecutive layers.\n",
    "\n",
    "Take neuron $n_3$ in the above figure as an example. It is connected with both input neurons, and will receive $w_{13}x_1 + w_{23}x_2$ as its input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions <a name=\"nn_basic_activation\"></a>\n",
    "\n",
    "A neuron doesn't just pass on the input it receives - it also transform it using a so called **activation function $f(.)$**. Neurons in different layers often use different activation functions (to achieve different purposes):\n",
    "\n",
    "- Activation functions in the **input layer** is typically just an identity function (i.e., pass on what you receive, no actual transformation);\n",
    "- Activation functions in the **hidden layer** is typically some kind of nonlinear continuous function;\n",
    "- Activation functions in the **output layer** also tend to be certain nonlinear continuous function, but the choice is often determined by the desired type of output (e.g., predicted probabilities in classification tasks and real values in numeric prediction tasks).\n",
    "\n",
    "<font color=\"red\">Question: why do we need activation function? Why can't each neuron simply passes on the (aggregated) information it receives?</font>\n",
    "\n",
    "**Common choices of activation functions and their characteristics**:\n",
    "1. Identity function: \n",
    "\n",
    "$$f(x) = x$$\n",
    "\n",
    "- Often used in input layer, passes on the information it receives without any actual transformation.\n",
    "\n",
    "2. Rectified Linear Unit (ReLU): \n",
    "\n",
    "$$f(x) = \\max\\{x, 0\\}$$\n",
    "\n",
    "- Often used in hidden layers, passes on the information it receives as long as it is positive; otherwise pass on 0.\n",
    "- Motivated by the biological neurons in our brain. It \"fires\" a signal to other connected neurons when the signal it receives is strong enough.\n",
    "\n",
    "3. Sigmoid function (logistic function): \n",
    "\n",
    "$$f(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "    \n",
    "- Often used in the output layer for a binary classification task, because the output of a sigmoid function is always between 0 and 1, which is suitable to represent a probability.\n",
    "- It also has a very nice mathematical property: $f'(x) = f(x)(1-f(x))$. In other words, one can compute the derivative of a sigmoid function very easily.\n",
    "\n",
    "4. Softmax: \n",
    "\n",
    "$$f(x_1, x_2, \\ldots, x_k) = \\left[ \\frac{e^{x_1}}{\\sum_{i = 1}^{k} e^{x_i}}, \\frac{e^{x_2}}{\\sum_{i = 1}^{k} e^{x_i}}, \\ldots, \\frac{e^{x_k}}{\\sum_{i = 1}^{k} e^{x_i}} \\right]$$\n",
    "    \n",
    "- Often used in the output layer for a $k$-class classification task. This is a straightforward generalization of the sigmoid function to multiple dimensions.\n",
    "- The output conveniently represents the probability of belonging to each of the $k$ classes.\n",
    "    \n",
    "5. Hyperbolic tangent:\n",
    "\n",
    "$$f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} \\doteq \\tanh(x)$$\n",
    "\n",
    "- Often used in the hidden layers, especially in deep networks when there are a large number of hidden layers.\n",
    "- Like sigmoid, it also has a nice property regarding derivative: $f'(x) = 1 - f^2(x)$.\n",
    "- Its output is always between -1 and 1. Therefore, it helps prevent the information being \"blown up\" over many hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Neural Networks <a name=\"nn_train\"></a>\n",
    "\n",
    "How to train a neural network using (labeled) data? I.e., how do we learn the weights in the network $\\{w_{ij}\\}$ from data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function <a name=\"nn_train_objective\"></a>\n",
    "\n",
    "Like many other machine learning models (e.g., linear regression), neural network is trained by optimizing certain **objective function**. An objective function is essentially a goodness-of-fit metric - it describes how well a model fits a given set of data. Therefore, training a neural network is a process of optimizing some well-defined objective function, in order to fit the model to the training data.\n",
    "\n",
    "<font color=\"blue\">This should not be confused with _performance metrics_ that we use to evaluate or fine-tune neural networks (e.g., accuracy, precision/recall, RMSE...)</font>\n",
    "\n",
    "For data instance $i \\in \\{1, \\ldots, N\\}$, denote the matrix of input data as $\\boldsymbol{X_i}$ and the neural network's predictions as $\\widehat{y_i}$, we can write $\\widehat{y_i} = F(\\boldsymbol{X_i}; \\boldsymbol{W})$, where $\\boldsymbol{W}$ are the weights to be learned and $F(.)$ is some complex function that links input to output via weights and activations. **Here are some common choices of objective function $L(\\widehat{y}, y)$**:\n",
    "\n",
    "1. Quadratic loss:\n",
    "\n",
    "$$L(\\widehat{y}, y) = \\frac{1}{2} \\sum_{i=1}^{N} (\\widehat{y_i} - y_i)^2$$\n",
    "\n",
    "- Often used for numeric prediction tasks. This is the same as OLS;\n",
    "- The constant 1/2 is there only for mathematical convenience;\n",
    "- <font color=\"red\">Question: </font> Why do we take square of the errors $\\widehat{y_i} - y_i$, rather than just summing over the errors or the absolute errors?\n",
    "\n",
    "2. (Binary) Cross-Entropy loss:\n",
    "\n",
    "$$L(\\widehat{y}, y) = - \\sum_{i=1}^{N} (y_i\\log(\\widehat{y_i}) + (1-y_i)\\log(1-\\widehat{y_i}))$$\n",
    "\n",
    "- Often used for binary classification tasks. This is the same as the log likelihood of a logit regression objective function;\n",
    "- Note that here $y_i \\in \\{0,1\\}$ and $\\widehat{y_i}$ is a predicted probability;\n",
    "- <font color=\"blue\">Intuition: </font> $L(\\widehat{y}, y)$ is smaller if $\\widehat{y_i}$ is closer to 1 when $y_i = 1$ and closer to 0 when $y_i = 0$.\n",
    "\n",
    "3. (Categorical) Cross-Entropy loss\n",
    "$$L(\\widehat{y}, y) = - \\sum_{i=1}^{N} \\sum_{j=1}^{k} y_{ij}\\log(\\widehat{y_{ij}})$$\n",
    "\n",
    "- Often used for $k$-class classification tasks. This is a direct generalization of binary cross-entropy;\n",
    "- Notationally, $y_{ik}$ is a binary indicator of whether data point $i$ belongs to class $j$ or not (i.e., one-hot encoded), and $\\widehat{y_{ij}}$ is the model's predicted probability that data point $i$ belongs to class $j$.\n",
    "- <font color=\"blue\">Intuition: </font> $L(\\widehat{y}, y)$ is smaller when the predicted probability is high for the correct class.\n",
    "\n",
    "<font color=\"blue\">Note that all of the above objective functions are formulated as \"losses\", i.e., smaller value indicates better fit.</font> This is why we often call them as **loss functions** and we want to **minimize the loss functions**. More formally, training a neural network amounts to solving the following minimization problem:\n",
    "\n",
    "$$\\text{Find weights } \\boldsymbol{W} \\text{ such that } L(\\widehat{y}, y) \\text{ is minimized.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent <a name=\"nn_train_gradient\"></a>\n",
    "\n",
    "So, given a set of data and a well-defined loss function to be minimized. How do we actually minimize it? This is where **training algorithms** come into play. There are a large number of training algorithms developed in the optimization research. One of the most classic is **gradient descent**.\n",
    "\n",
    "<font color=\"blue\">Intuition of how gradient descent works: </font> Move in the _opposite direction_ of the gradient, by a certain distance.\n",
    "\n",
    "![gradient descent](images/gradient_descent.png)\n",
    "\n",
    "- Gradient descent works because the opposite of the gradient gives the most effective direction to reduce value of a function;\n",
    "- The distance to move is obviously important. Move too little, then it may take a long time to reach the minimum. Move to much, you may \"go pass\" the minimum. The distance is partially controlled by _step size_ or _learning rate_, which is a hyper-parameter to be set by the user and fine-tuned;\n",
    "- Intuitively, it makes sense to dynamically change the learning rate. For example, maybe start with a large learning rate, then gradually decrease it. This is the idea behind _adaptive learning rates_.\n",
    "\n",
    "There are a large number of other optimization algorithms, many building on the basic gradient descent, such as momentum, Adam, Adagrad, RMSprop, ... Check out \"Additional Resources\" for details if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for general guidelines on how to fine-tune a neural network model. I highly recommend this GitHub repo: [Deep Learning Tuning Playbook](https://github.com/google-research/tuning_playbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch, Epoch, and Stochastic Gradient Descent <a name=\"nn_train_sgd\"></a>\n",
    "\n",
    "**The problem with basic gradient descent discussed above**: In principle, you can perform gradient descent on the entire set of training data (for multiple iterations) until it's done. However, this can be computationally very expensive in practice especially if the size of training data is very large, because calculating the gradient of loss function $L(\\widehat{y}, y)$ involves calculating the gradients associated with each data point then add them up. \n",
    "\n",
    "**The solution - stochastic gradient descent**: don't using all the training data every time you want to calculate a gradient. Instead, use a subset of it each time, called a **batch** of training data points, to calculate gradient. For instance, if you have 100 training data points and adopt a batch size of 20, then you can perform 5 gradient descent iterations (i.e., parameter updates) during one scan pass of the training dataset. Finally, you can scan pass the same training dataset multiple times during the entire process of gradient descent. Each time you scan through the training dataset is called an **epoch**.\n",
    "\n",
    "<font color=\"blue\">Note:</font> sometimes, people refer to stochastic gradient descent specifically as the version where every time you only use a single data point to calculate gradient and perform update (i.e., set batch size to 1), and refer to the case described above as **mini-batch gradient descent** (i.e., set batch size somewhere between 1 and total training data size). These are mostly terminology differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Networks with Keras <a name=\"nn_implementation\"></a>\n",
    "\n",
    "Let's try some examples. For demonstration, I will implement a feedforward neural network for classification using the classic [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105, 4) (45, 4) (105, 3) (45, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read in the dataset\n",
    "x = []\n",
    "y = []\n",
    "for line in open(\"../datasets/iris.csv\"):\n",
    "    data = line.rstrip('\\n').split(',')\n",
    "    x.append(data[0:4])\n",
    "    y.append(data[4])\n",
    "\n",
    "# convert feature lists to a numpy array\n",
    "x = np.array(x, dtype = float)\n",
    "# one-hot encode the label (3 classes) - a manual approach\n",
    "label = np.zeros((len(y), 3))\n",
    "for i in range(len(y)):\n",
    "    if y[i] == 'Iris-setosa':\n",
    "        label[i][0] = 1\n",
    "    if y[i] == 'Iris-versicolor':\n",
    "        label[i][1] = 1\n",
    "    if y[i] == 'Iris-virginica':\n",
    "        label[i][2] = 1\n",
    "label = np.array(label, dtype = float)\n",
    "\n",
    "# training-validation split (70% training)\n",
    "x_train, x_val, label_train, label_val = train_test_split(x, label, test_size = 0.3)\n",
    "print(x_train.shape, x_val.shape, label_train.shape, label_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's specify the neural network structure using the Sequential class of Keras\n",
    "model_nn = keras.Sequential()\n",
    "# First, add the input layer, specify the shape of input (a feature vector of 4 elements)\n",
    "model_nn.add(keras.layers.InputLayer(input_shape = (4,)))\n",
    "# Second, add the hidden layers (as many as you'd like, I'm only adding 1 hidden layer here for demonstration)\n",
    "# I'm adding 8 neurons in this layer, with ReLU activation\n",
    "model_nn.add(keras.layers.Dense(units = 8, activation = 'relu'))\n",
    "# Finally, specify the output layer - must have 3 units here because there are 3 classes to predict\n",
    "model_nn.add(keras.layers.Dense(units = 3, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, configure the training process\n",
    "model_nn.compile(loss = keras.losses.CategoricalCrossentropy(),  # This is the loss function\n",
    "                 optimizer='sgd',  # This is the optimization algorithm\n",
    "                 metrics=['accuracy'])   # This is the metric you want it to report (note, it is not maximizing this metric per se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 67\n",
      "Trainable params: 67\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Check the model summary to make sure things are ok\n",
    "model_nn.summary()\n",
    "# Where does the two \"Param #\" come from? Can you explain how they are calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4/4 [==============================] - 0s 64ms/step - loss: 2.3660 - accuracy: 0.3086 - val_loss: 1.1593 - val_accuracy: 0.4000\n",
      "Epoch 2/30\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.1337 - accuracy: 0.3986 - val_loss: 1.0201 - val_accuracy: 0.6444\n",
      "Epoch 3/30\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.9928 - accuracy: 0.6495 - val_loss: 0.9175 - val_accuracy: 0.6667\n",
      "Epoch 4/30\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.8836 - accuracy: 0.6967 - val_loss: 0.8473 - val_accuracy: 0.6667\n",
      "Epoch 5/30\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.8496 - accuracy: 0.6567 - val_loss: 0.7937 - val_accuracy: 0.6667\n",
      "Epoch 6/30\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7768 - accuracy: 0.6867 - val_loss: 0.7511 - val_accuracy: 0.6667\n",
      "Epoch 7/30\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.7376 - accuracy: 0.6822 - val_loss: 0.7214 - val_accuracy: 0.6667\n",
      "Epoch 8/30\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.7070 - accuracy: 0.6833 - val_loss: 0.6938 - val_accuracy: 0.6667\n",
      "Epoch 9/30\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6954 - accuracy: 0.6578 - val_loss: 0.6744 - val_accuracy: 0.6667\n",
      "Epoch 10/30\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.6463 - accuracy: 0.6878 - val_loss: 0.6573 - val_accuracy: 0.6667\n",
      "Epoch 11/30\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6985 - accuracy: 0.6178 - val_loss: 0.6362 - val_accuracy: 0.6667\n",
      "Epoch 12/30\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6135 - accuracy: 0.6978 - val_loss: 0.6165 - val_accuracy: 0.6667\n",
      "Epoch 13/30\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5986 - accuracy: 0.7078 - val_loss: 0.6033 - val_accuracy: 0.6667\n",
      "Epoch 14/30\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5989 - accuracy: 0.6744 - val_loss: 0.5937 - val_accuracy: 0.6667\n",
      "Epoch 15/30\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6171 - accuracy: 0.6356 - val_loss: 0.5801 - val_accuracy: 0.6667\n",
      "Epoch 16/30\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5727 - accuracy: 0.6878 - val_loss: 0.5684 - val_accuracy: 0.6889\n",
      "Epoch 17/30\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5711 - accuracy: 0.6838 - val_loss: 0.5595 - val_accuracy: 0.6889\n",
      "Epoch 18/30\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5698 - accuracy: 0.6578 - val_loss: 0.5509 - val_accuracy: 0.6889\n",
      "Epoch 19/30\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5578 - accuracy: 0.6716 - val_loss: 0.5432 - val_accuracy: 0.6889\n",
      "Epoch 20/30\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.5612 - accuracy: 0.6671 - val_loss: 0.5333 - val_accuracy: 0.6889\n",
      "Epoch 21/30\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5496 - accuracy: 0.6710 - val_loss: 0.5251 - val_accuracy: 0.6889\n",
      "Epoch 22/30\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5281 - accuracy: 0.6771 - val_loss: 0.5164 - val_accuracy: 0.6889\n",
      "Epoch 23/30\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5001 - accuracy: 0.7165 - val_loss: 0.5061 - val_accuracy: 0.7111\n",
      "Epoch 24/30\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5292 - accuracy: 0.7503 - val_loss: 0.5009 - val_accuracy: 0.7111\n",
      "Epoch 25/30\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5238 - accuracy: 0.7367 - val_loss: 0.4904 - val_accuracy: 0.7778\n",
      "Epoch 26/30\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5150 - accuracy: 0.7778 - val_loss: 0.4830 - val_accuracy: 0.8000\n",
      "Epoch 27/30\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4956 - accuracy: 0.8210 - val_loss: 0.4758 - val_accuracy: 0.8444\n",
      "Epoch 28/30\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5024 - accuracy: 0.8546 - val_loss: 0.4687 - val_accuracy: 0.8444\n",
      "Epoch 29/30\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4647 - accuracy: 0.8562 - val_loss: 0.4683 - val_accuracy: 0.8000\n",
      "Epoch 30/30\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4596 - accuracy: 0.8305 - val_loss: 0.4575 - val_accuracy: 0.8667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2030f042970>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, perform training\n",
    "model_nn.fit(x = x_train, y = label_train, validation_data = (x_val, label_val),\n",
    "             epochs = 30, batch_size = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8930898 , 0.08641573, 0.02049459]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "model_nn.predict([[5.1,3.5,1.4,0.2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, please try this out using the [Breast Cancer Detection](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28original%29) dataset yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(478, 9) (205, 9) (478, 2) (205, 2)\n"
     ]
    }
   ],
   "source": [
    "# Data import and pre-processing\n",
    "x = []\n",
    "y = []\n",
    "for line in open(\"../datasets/breast_cancer.csv\"):\n",
    "    # get rid of header line\n",
    "    if 'Class' not in line:\n",
    "        data = line.rstrip('\\n').split(',')\n",
    "        x.append(data[0:9])\n",
    "        y.append(data[9])\n",
    "\n",
    "# convert feature lists to a numpy array\n",
    "x = np.array(x, dtype = float)\n",
    "# one-hot encode the label (2 classes) - a manual approach\n",
    "label = np.zeros((len(y), 2))\n",
    "for i in range(len(y)):\n",
    "    if y[i] == '2':\n",
    "        label[i][0] = 1\n",
    "    if y[i] == '4':\n",
    "        label[i][1] = 1\n",
    "label = np.array(label, dtype = float)\n",
    "\n",
    "# training-validation split (70% training)\n",
    "x_train, x_val, label_train, label_val = train_test_split(x, label, test_size = 0.3)\n",
    "print(x_train.shape, x_val.shape, label_train.shape, label_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources <a name=\"resource\"></a>\n",
    "\n",
    "- [Wiki page on activation functions](https://en.wikipedia.org/wiki/Activation_function). There are a lot of other activation functions;\n",
    "- A blog post about various gradient-descent-based optimization algorithms: [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
