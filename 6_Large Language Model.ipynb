{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> MSBA 6461: Advanced AI for Natural Language Processing </center>\n",
    "<center> Summer 2025, Mochen Yang </center>\n",
    "\n",
    "## <center> Large Language Models </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Large Language Models](#llm)\n",
    "    - [General Process of LLM Training](#llm_train)\n",
    "    - [Common LLM Architecture](#llm_arch)\n",
    "    - [LLM Use Cases](#llm_use)\n",
    "1. [Application Case: BERT](#bert)\n",
    "    - [What is BERT?](#bert_intro)\n",
    "    - [Use BERT](#bert_example)\n",
    "1. [Additional Resources](#resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Models <a name=\"llm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) are generative AI models that can process, \"understand\", and generate texts. They are \"large\" because they are often trained on enormous amount of textual data and have a huge number of parameters. They learn representations of languages, and can be further fine-tuned for a variety of different language tasks. Current major players in the LLM arena include:\n",
    "- [OpenAI (GPT Models)](https://openai.com/)\n",
    "- [Meta (Llama Models)](https://www.llama.com/)\n",
    "- [Anthropic (Claude Models)](https://www.claude.ai/)\n",
    "- [Mistral](https://mistral.ai/en)\n",
    "- [Deepseek](https://www.deepseek.com/)\n",
    "\n",
    "LLMs are perhaps the most exciting major advancement in NLP currently. This lecture is designed to provide a brief exposition of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Process of LLM Training <a name=\"llm_train\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly speaking, LLM Training consists of two distinct stages: **pre-training** and **post-training**. The two stages use different data / techniques and serve different purposes.\n",
    "- **Pre-training**: Pre-training is the process of representation learning from huge quantities of raw data, typically in an unsupervised manner (this stage is also referred to as unsupervised pre-training). The training task is similar to what we have discussed in sequence-to-sequence modeling (Notebook 3) -- autoregressively predicting next token based on previous tokens. The goal of pre-training is to obtain high-quality token representations;\n",
    "- **Post-training**: Post-training is the process of \"fine-tuning\" the LLM to perform certain specific tasks. It can be done via both **supervised learning** and **reinforcement learning**, with some differences in objectives:\n",
    "    - **Supervised post-training**: fine-tuning the LLM to _perform certain task via supervised learning_. The BERT demo in the second half of this notebook is a very simple example of this. In practice, LLM providers will fine-tune their pre-trained LLMs on a wide variety of different tasks;\n",
    "    - **Reinforcement post-training**: fine-tuning the LLM to _better align with human preferences via reinforcement learning_. For many tasks that are not completely objective, humans may have a preference for one response over another (even if both responses are technically correct, e.g., humans may prefer a more \"polite\" LLM than a more \"blunt\" one). Carrying out reinforcement learning with such human feedback signals can further adjust the LLMs to generate responses that are more human-acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common LLM Architecture <a name=\"llm_arch\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though LLMs are mostly based on transformers, they can follow different architectures, including **encoder-decoder**, **encoder-only** (e.g., BERT), and **decoder-only** (e.g., Mistral). \n",
    "\n",
    "1. The **encoder-decoder** architecture is what we have discussed from last lecture. It is sequence-to-sequence.\n",
    "2. The **encoder-only** architecture uses just the encoder part, and it is inherently an encoding model. That means it takes an input sequence and produces its encoded representation. Those representations can then be used to carry out task-specific fine-tuning (e.g., act as inputs to a classifier). It is _not_ sequence-to-sequence.\n",
    "3. The **decoder-only** architecture uses just the decoder part, but it can perform sequence-to-sequence tasks. The trick is to prepend the input sequence (the \"prompt\") ahead of the output sequence and give the entire thing to the decoder. It will learn to predict the next token in the autoregressive manner.\n",
    "\n",
    "Furthermore, because of the large scale of LLMs (transformers of billions of parameters), using them to generate responses (a.k.a. \"LLM inference\") can be very costly and slow. A powerful architectural innovation to address this issue is called **mixture-of-experts** (MoE). The idea of MoE is not to use the entire transformer network to process each input token, but instead use different parts of the network (each called an \"expert\") to deal with tokens of different types. To achieve effective MoE, there needs to be a separate \"routing\" model (often a gated neural network) that decides which \"expert\" to invoke for each token. However, because each expert is only a (sparse) subset of the entire transformer, MoE can significantly speed up LLM inference without sacrificing response quality. For more technical details of MoE, see [this paper](https://www.jmlr.org/papers/volume23/21-0998/21-0998.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Use Cases <a name=\"llm_use\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given LLM's wide ranges of capabilities, it is constructive to think of it not as a single tool, but as a [general purpose technology](https://en.wikipedia.org/wiki/General-purpose_technology). Its use cases include at least the following (and are quickly expanding over time):\n",
    "- **Completion**: this is the baseline use of a foundation LLM. Users send question prompts, LLM returns answers (sometimes using tools to do so).\n",
    "- **Retrieval Augmented Generation** (RAG): enhance LLM response with information retrieved from user-supplied files / databases. This allows LLM to refer to (potentially private) user-owned information when composing its answers.\n",
    "- **Fine-Tuning**: further modifying a foundation LLM with task-specific data. This is conceptually identical to supervised post-training, except that it is done by LLM users. It allows users to customize a foundation LLM to their own use cases.\n",
    "- **Agents**: in the above use cases of LLM, the model usually assumes a \"passive\" role, acting only when asked to do something. An Agentic LLM is more \"active\" (i.e., have some degree of \"agency\", hence the name) and can decide to do something on its own. As an example, OpenAI's [Operator](https://openai.com/index/introducing-operator/) can perform various tasks using a built-in browser (e.g., book a flight for you or manage your calender).\n",
    "- **Reasoning**: using LLM for reasoning tasks. This is at the current frontier of LLM research and development. OpenAI's o-series models and Deepseek's R-series models achieve reasoning by carrying out (sometimes implicitly) chain-of-thought processes before generating actual answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Case: BERT <a name=\"bert\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is BERT? <a name=\"bert_intro\"></a>\n",
    "\n",
    "BERT stands for _**B**idirectional **E**ncoder **R**epresentations from **T**ransformers_. It is a **language representation model**, which means it takes raw text and generate a meaningful representation (e.g., embedding) of it. It was developed by Google in 2018. With everything we have discussed so far, you are ready to make sense of all the key components of BERT:\n",
    "\n",
    "1. **B**idirectional means that the encoder makes uses of full self-attention where every position can attend to every other position;\n",
    "2. **E**ncoder **R**epresentations means that the model is aiming to generate representation of the input sequence, i.e., it is an encoder-only architecture;\n",
    "3. **T**ransformers means that BERT uses a transformer architecture with self-attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use BERT <a name=\"bert_example\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google has released a number of different BERT models, trained with different hyperparameters. [Here is a directory of all those models](https://www.tensorflow.org/tutorials/text/classify_text_with_bert#choose_a_bert_model_to_fine-tune). You see that each model is identified by three parameters:\n",
    "- $L$: this is the number of transformer blocks. You can think of it as number of \"layers\";\n",
    "- $H$: this is the dimension of embedding. We called this $D$ in our discussion of transformer;\n",
    "- $A$: this is the number of heads in multi-head self-attention. This means cutting the embedding into $A$ pieces and apply self-attention to each piece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access pre-trained BERT models and potentially fine-tune them for your own ML tasks via [Hugging Face](https://huggingface.co/), an online platform that hosts many commonly used pre-trained models. In the following example, we access a basic BERT model and use it to encode some text. See this [page](https://huggingface.co/bert-base-uncased) for detailed documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (20.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: requests in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (2020.10.15)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp38-cp38-win_amd64.whl (155 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.7.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers) (1.26.2)\n",
      "Installing collected packages: pyyaml, packaging, filelock, tokenizers, huggingface-hub, transformers\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.8\n",
      "    Uninstalling packaging-20.8:\n",
      "      Successfully uninstalled packaging-20.8\n",
      "Successfully installed filelock-3.7.0 huggingface-hub-0.6.0 packaging-21.3 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.1\n"
     ]
    }
   ],
   "source": [
    "# install transformer package from Hugging Face\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# fetch the pre-trained model (it will download a model file ~500M)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input text and encode\n",
    "text = \"We are using the BERT model!\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = bert_model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[  101,  2057,  2024,  2478,  1996, 14324,  2944,   999,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1]])>}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at the tokenized input\n",
    "# Question: what are tokens 101 and 102?\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(1, 9, 768), dtype=float32, numpy=\n",
       "array([[[ 0.10261209,  0.18043919, -0.00554929, ..., -0.166134  ,\n",
       "          0.26679957,  0.35773745],\n",
       "        [ 0.263622  , -0.21110201, -0.57594675, ..., -0.20186077,\n",
       "          1.308478  , -0.14822024],\n",
       "        [ 0.12224663, -0.15183868, -0.36246365, ..., -0.56034166,\n",
       "          0.18197185,  0.45692527],\n",
       "        ...,\n",
       "        [ 0.487611  ,  0.05848615, -0.26846886, ..., -0.64023006,\n",
       "         -0.01316616, -0.00961822],\n",
       "        [-0.16868652, -0.17555293, -0.15778571, ...,  0.54957277,\n",
       "          0.45626837, -0.39924195],\n",
       "        [ 0.52467674,  0.37009996, -0.21517405, ...,  0.00148578,\n",
       "         -0.5219994 , -0.30393368]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(1, 768), dtype=float32, numpy=\n",
       "array([[-8.78734827e-01, -3.25698197e-01, -3.28317106e-01,\n",
       "         6.70523882e-01,  6.76294491e-02, -4.97857258e-02,\n",
       "         8.80656004e-01,  2.76587784e-01, -1.80702090e-01,\n",
       "        -9.99952853e-01,  4.34783325e-02,  5.05748391e-01,\n",
       "         9.82219696e-01,  2.04030082e-01,  9.44258571e-01,\n",
       "        -6.23518169e-01,  7.85685144e-03, -5.62641799e-01,\n",
       "         2.75413692e-01, -6.43890440e-01,  6.54973030e-01,\n",
       "         9.98014152e-01,  5.39189219e-01,  2.80568451e-01,\n",
       "         3.46919954e-01,  6.95186794e-01, -6.27278507e-01,\n",
       "         9.35333967e-01,  9.50950146e-01,  7.03973353e-01,\n",
       "        -7.07702696e-01,  1.71931267e-01, -9.81890202e-01,\n",
       "        -1.79219425e-01, -4.41820800e-01, -9.87813234e-01,\n",
       "         2.80857116e-01, -7.27158844e-01,  6.23343885e-02,\n",
       "         1.89604927e-02, -8.98371339e-01,  1.58929959e-01,\n",
       "         9.99590158e-01, -4.11883116e-01,  9.89006758e-02,\n",
       "        -3.57154489e-01, -9.99994516e-01,  2.21773237e-01,\n",
       "        -8.59246016e-01,  2.33305633e-01,  2.69120753e-01,\n",
       "        -1.78335607e-01,  1.46656051e-01,  3.96779269e-01,\n",
       "         3.90325457e-01,  8.13974217e-02, -2.25540981e-01,\n",
       "         8.19753185e-02, -1.70295686e-01, -5.39923191e-01,\n",
       "        -5.92916071e-01,  2.91853637e-01, -4.13579017e-01,\n",
       "        -8.97091568e-01,  1.72661781e-01,  1.28206715e-01,\n",
       "         6.20134594e-03, -2.14740783e-01, -1.77664440e-02,\n",
       "        -3.99033651e-02,  8.69764864e-01,  2.00019509e-01,\n",
       "         2.01246832e-02, -8.21547568e-01,  1.00624897e-01,\n",
       "         1.92495257e-01, -5.63699186e-01,  1.00000000e+00,\n",
       "        -2.21787632e-01, -9.65783954e-01,  3.35388988e-01,\n",
       "         2.84605920e-01,  4.33147043e-01,  3.54774445e-01,\n",
       "         1.21714659e-01, -1.00000000e+00,  2.54565060e-01,\n",
       "        -8.17635730e-02, -9.87588823e-01,  2.04371825e-01,\n",
       "         3.83364707e-01, -1.52973607e-01,  2.03943223e-01,\n",
       "         5.20069897e-01, -4.06468898e-01, -2.53391236e-01,\n",
       "        -2.38453716e-01, -2.81073183e-01, -1.54782653e-01,\n",
       "        -2.11132560e-02, -1.77784823e-02, -2.03800425e-01,\n",
       "        -1.55137390e-01, -3.46282095e-01,  1.47095636e-01,\n",
       "        -3.79600376e-01, -3.69906008e-01,  4.54690695e-01,\n",
       "        -1.24441147e-01,  6.93087399e-01,  3.74282241e-01,\n",
       "        -3.42686892e-01,  2.79299527e-01, -9.45988655e-01,\n",
       "         5.96868396e-01, -2.80452639e-01, -9.82781827e-01,\n",
       "        -4.90857422e-01, -9.85587358e-01,  6.12973213e-01,\n",
       "        -1.43317699e-01, -6.91471174e-02,  9.56331015e-01,\n",
       "         2.70761698e-01,  3.20910186e-01, -2.27423124e-02,\n",
       "        -1.88038304e-01, -1.00000000e+00, -2.23407596e-01,\n",
       "        -3.91020864e-01, -8.88535604e-02, -1.46055534e-01,\n",
       "        -9.69378948e-01, -9.50656414e-01,  5.77531338e-01,\n",
       "         9.55189168e-01,  1.85606480e-01,  9.99256492e-01,\n",
       "        -2.20427796e-01,  9.24856126e-01, -4.45280969e-03,\n",
       "        -2.08308801e-01, -1.76956788e-01, -3.98821056e-01,\n",
       "         5.95907867e-01,  2.28622675e-01, -6.36695802e-01,\n",
       "         2.32059166e-01,  4.05228809e-02, -1.79734126e-01,\n",
       "        -3.67709398e-01, -2.29818821e-01, -2.95393378e-01,\n",
       "        -9.38730478e-01, -3.68636638e-01,  9.38845456e-01,\n",
       "        -3.78510989e-02, -3.31998199e-01,  4.98391122e-01,\n",
       "        -1.76862538e-01, -3.84551495e-01,  8.11352372e-01,\n",
       "         4.57894355e-01,  3.15411806e-01, -9.31794345e-02,\n",
       "         2.73526967e-01,  7.80324414e-02,  4.89669710e-01,\n",
       "        -8.58586490e-01,  2.35194296e-01,  3.33799511e-01,\n",
       "        -2.74861544e-01, -3.01866442e-01, -9.73188698e-01,\n",
       "        -3.18678290e-01,  5.61900198e-01,  9.84158099e-01,\n",
       "         7.78745353e-01,  2.15006962e-01,  3.41620505e-01,\n",
       "        -1.67594388e-01,  2.34875783e-01, -9.35596228e-01,\n",
       "         9.71766651e-01, -1.57090798e-01,  2.67671078e-01,\n",
       "        -4.58420590e-02,  1.93604544e-01, -8.29781353e-01,\n",
       "        -1.57395571e-01,  8.27280283e-01, -7.33340755e-02,\n",
       "        -8.31848741e-01,  9.53281522e-02, -4.05152172e-01,\n",
       "        -3.69469523e-01, -3.16484511e-01,  4.88623649e-01,\n",
       "        -2.12188363e-01, -3.50075871e-01,  3.26649733e-02,\n",
       "         9.33767915e-01,  9.68594491e-01,  7.72753537e-01,\n",
       "        -3.76413137e-01,  6.08025908e-01, -8.72552097e-01,\n",
       "        -4.03848231e-01,  1.10359572e-01,  2.27107257e-01,\n",
       "         7.50229508e-02,  9.90339756e-01, -3.98373038e-01,\n",
       "        -5.27714193e-02, -9.32333529e-01, -9.80521321e-01,\n",
       "        -1.19610526e-01, -8.63848269e-01, -1.65523347e-02,\n",
       "        -7.12563157e-01,  4.53721911e-01,  3.56207103e-01,\n",
       "         1.33278117e-01,  3.37267667e-01, -9.73698258e-01,\n",
       "        -7.70551562e-01,  3.74536067e-01, -3.18802953e-01,\n",
       "         4.42139983e-01, -2.47391507e-01,  6.06002510e-01,\n",
       "         4.42190766e-01, -5.73994458e-01,  6.87730789e-01,\n",
       "         8.98433566e-01, -3.32827747e-01, -7.47683465e-01,\n",
       "         8.23455632e-01, -2.36669004e-01,  8.63408983e-01,\n",
       "        -5.65931082e-01,  9.84694064e-01,  3.72442126e-01,\n",
       "         4.87329841e-01, -9.25008297e-01, -2.22613752e-01,\n",
       "        -8.78267169e-01, -2.02192426e-01,  2.92679667e-03,\n",
       "        -2.49615416e-01,  4.52272654e-01,  5.65947652e-01,\n",
       "         3.51830661e-01,  5.92193782e-01, -4.22784150e-01,\n",
       "         9.94401455e-01, -7.13852286e-01, -9.35386717e-01,\n",
       "         1.31308109e-01, -1.66538686e-01, -9.81504440e-01,\n",
       "         3.95584941e-01,  2.60453850e-01, -3.70292187e-01,\n",
       "        -4.29365933e-01, -6.02952719e-01, -9.48078215e-01,\n",
       "         8.63633931e-01,  1.10244907e-01,  9.86000419e-01,\n",
       "        -8.75240043e-02, -8.56879354e-01, -3.48551184e-01,\n",
       "        -9.09709930e-01, -2.01060414e-01, -1.44988850e-01,\n",
       "         2.75220066e-01, -1.36509523e-01, -9.56022561e-01,\n",
       "         4.27542418e-01,  5.26469767e-01,  4.66602892e-01,\n",
       "        -2.00794473e-01,  9.96310353e-01,  9.99979138e-01,\n",
       "         9.68557298e-01,  8.92553687e-01,  9.10764754e-01,\n",
       "        -9.90309119e-01, -2.08085820e-01,  9.99980450e-01,\n",
       "        -8.28513801e-01, -9.99999642e-01, -9.31876540e-01,\n",
       "        -3.99393380e-01,  4.16211486e-01, -1.00000000e+00,\n",
       "        -1.37056887e-01,  4.65024523e-02, -9.09568906e-01,\n",
       "         6.47307858e-02,  9.73025858e-01,  9.86061275e-01,\n",
       "        -1.00000000e+00,  8.46311450e-01,  9.31820989e-01,\n",
       "        -5.81116378e-01,  5.70267320e-01, -1.97303370e-01,\n",
       "         9.68780875e-01,  5.69642603e-01,  3.79259855e-01,\n",
       "        -2.04666719e-01,  3.69923919e-01, -5.84048033e-01,\n",
       "        -8.61861348e-01, -1.03053106e-02, -8.65144134e-02,\n",
       "         9.50026870e-01,  7.70883113e-02, -6.97549164e-01,\n",
       "        -9.16969001e-01,  5.69302402e-02, -6.45725727e-02,\n",
       "        -3.13418001e-01, -9.58576977e-01, -2.09968925e-01,\n",
       "        -2.51608610e-01,  6.16406798e-01, -2.28298791e-02,\n",
       "         1.80578277e-01, -7.21990168e-01,  2.18542159e-01,\n",
       "        -3.48271072e-01,  3.80749822e-01,  6.25714302e-01,\n",
       "        -9.38922167e-01, -5.99819243e-01,  3.32828574e-02,\n",
       "        -4.51116860e-01, -1.63040295e-01, -9.53859985e-01,\n",
       "         9.58749712e-01, -3.02904159e-01,  9.24464688e-02,\n",
       "         1.00000000e+00, -1.62672520e-01, -8.51902664e-01,\n",
       "         4.90244776e-01,  1.78810179e-01, -4.50366139e-01,\n",
       "         1.00000000e+00,  5.80830216e-01, -9.72714484e-01,\n",
       "        -5.01797080e-01,  3.75872195e-01, -4.14121807e-01,\n",
       "        -4.22003090e-01,  9.98691797e-01, -1.82080939e-01,\n",
       "        -1.20563649e-01,  2.07324669e-01,  9.62340772e-01,\n",
       "        -9.85113740e-01,  8.89728248e-01, -9.06691194e-01,\n",
       "        -9.59492743e-01,  9.53451514e-01,  9.28224027e-01,\n",
       "        -2.19139546e-01, -6.33601785e-01,  9.73953959e-03,\n",
       "        -4.24671978e-01,  2.30816096e-01, -9.53832746e-01,\n",
       "         5.05898058e-01,  4.64295417e-01, -2.92715486e-02,\n",
       "         8.66881609e-01, -7.86206365e-01, -4.74578291e-01,\n",
       "         2.97276676e-01, -7.05367848e-02,  2.77469695e-01,\n",
       "         4.53787029e-01,  4.71232116e-01, -1.64594144e-01,\n",
       "         4.72750999e-02, -1.92405149e-01, -3.66292715e-01,\n",
       "        -9.69866037e-01,  5.55830188e-02,  1.00000000e+00,\n",
       "        -1.19822342e-02, -1.82129443e-04, -2.85125434e-01,\n",
       "        -4.64347526e-02, -3.08597296e-01,  4.30776179e-01,\n",
       "         5.04439175e-01, -1.92210749e-01, -8.72224867e-01,\n",
       "         2.10603610e-01, -9.33181465e-01, -9.80421126e-01,\n",
       "         6.81359887e-01,  1.40876845e-01, -2.95872420e-01,\n",
       "         9.99806762e-01,  2.78796524e-01,  1.62152648e-01,\n",
       "        -2.52243690e-03,  7.11778164e-01,  2.15992425e-02,\n",
       "         5.62938333e-01,  2.97641546e-01,  9.69979882e-01,\n",
       "        -1.88880622e-01,  4.88082379e-01,  8.20398033e-01,\n",
       "        -3.50302339e-01, -2.83560812e-01, -6.12565398e-01,\n",
       "        -1.54352551e-02, -9.12919223e-01,  1.25817940e-01,\n",
       "        -9.36130702e-01,  9.58291292e-01,  1.21257074e-01,\n",
       "         3.43669832e-01,  1.40896946e-01,  1.68940544e-01,\n",
       "         1.00000000e+00, -2.85069913e-01,  5.92145979e-01,\n",
       "        -2.80246139e-01,  7.94635534e-01, -9.86922204e-01,\n",
       "        -7.84085691e-01, -2.62509823e-01,  6.59338906e-02,\n",
       "        -1.58151209e-01, -3.14305753e-01,  2.20784366e-01,\n",
       "        -9.58403230e-01,  2.56395519e-01,  1.25021935e-02,\n",
       "        -9.79425430e-01, -9.86912072e-01,  4.38348323e-01,\n",
       "         7.55907476e-01, -1.42517211e-02, -8.38730633e-01,\n",
       "        -6.34958088e-01, -5.94622850e-01,  1.47147089e-01,\n",
       "        -1.06524996e-01, -9.17863846e-01,  3.64055544e-01,\n",
       "        -2.05472842e-01,  4.54669982e-01, -2.39828452e-01,\n",
       "         4.91848916e-01,  2.35310212e-01,  7.26663113e-01,\n",
       "         4.38361131e-02,  5.74373975e-02, -1.28145062e-03,\n",
       "        -7.46204972e-01,  7.52747834e-01, -7.80600667e-01,\n",
       "        -3.78396809e-01, -1.41046494e-01,  1.00000000e+00,\n",
       "        -4.99441266e-01,  5.26705384e-01,  7.41574466e-01,\n",
       "         5.85063875e-01, -1.36753604e-01,  1.90162152e-01,\n",
       "         5.56856811e-01,  1.28343463e-01, -2.21230760e-01,\n",
       "        -2.11827576e-01, -6.68248892e-01, -3.48785311e-01,\n",
       "         4.95144486e-01, -3.12088039e-02,  2.19363078e-01,\n",
       "         7.48234987e-01,  5.40513098e-01,  7.21349344e-02,\n",
       "         4.29088585e-02, -1.00242995e-01,  9.98324156e-01,\n",
       "        -6.53915433e-03, -4.12907377e-02, -4.87146556e-01,\n",
       "        -3.57575505e-03, -2.69600600e-01, -4.04136598e-01,\n",
       "         1.00000000e+00,  3.01777363e-01,  1.71233073e-01,\n",
       "        -9.84637022e-01, -2.41307542e-01, -9.09752011e-01,\n",
       "         9.99922574e-01,  8.06832492e-01, -8.09211612e-01,\n",
       "         4.72138733e-01,  2.98552185e-01, -4.52987812e-02,\n",
       "         7.47803211e-01, -1.44971177e-01, -2.46243000e-01,\n",
       "         2.18701512e-01,  1.29333556e-01,  9.53508556e-01,\n",
       "        -4.58412379e-01, -9.55183566e-01, -6.00613952e-01,\n",
       "         3.09518337e-01, -9.59915340e-01,  9.94016469e-01,\n",
       "        -4.43690896e-01, -2.07088172e-01, -3.03067625e-01,\n",
       "         3.57810915e-01,  5.09513378e-01, -6.86136857e-02,\n",
       "        -9.77442324e-01, -6.21347539e-02,  3.58104822e-03,\n",
       "         9.57711220e-01,  1.13723092e-01, -4.93515849e-01,\n",
       "        -9.06929135e-01, -1.00530379e-01,  1.33775160e-01,\n",
       "        -2.13098109e-01, -9.15971696e-01,  9.65532660e-01,\n",
       "        -9.79793191e-01,  4.09691811e-01,  9.99999523e-01,\n",
       "         3.05352211e-01, -6.46948278e-01,  5.35247959e-02,\n",
       "        -3.98079813e-01,  2.45531783e-01,  2.19445884e-01,\n",
       "         5.48078120e-01, -9.49364185e-01, -2.64547527e-01,\n",
       "        -1.41302705e-01,  2.11955592e-01, -1.21996567e-01,\n",
       "         4.30372626e-01,  6.55721903e-01,  2.51634300e-01,\n",
       "        -4.27288145e-01, -5.06994188e-01,  7.39996042e-03,\n",
       "         3.67606819e-01,  8.17710817e-01, -2.35767588e-01,\n",
       "        -5.30820861e-02,  2.92331260e-02, -1.22121580e-01,\n",
       "        -9.15593028e-01, -1.86418876e-01, -2.00486958e-01,\n",
       "        -9.99089420e-01,  6.34987772e-01, -1.00000000e+00,\n",
       "        -1.73069268e-01, -1.95214495e-01, -1.84107035e-01,\n",
       "         7.88663924e-01,  3.78994077e-01,  2.39148498e-01,\n",
       "        -7.13645697e-01, -2.39147753e-01,  7.18260825e-01,\n",
       "         6.80139780e-01, -1.17763393e-01,  2.29843318e-01,\n",
       "        -6.68980658e-01,  1.63184345e-01, -1.02425866e-01,\n",
       "         2.34207973e-01, -5.40278628e-02,  7.63468981e-01,\n",
       "        -1.10938117e-01,  1.00000000e+00,  1.09856166e-01,\n",
       "        -4.92708892e-01, -9.65841711e-01,  2.00337559e-01,\n",
       "        -1.63144931e-01,  9.99994397e-01, -8.80369008e-01,\n",
       "        -9.28947151e-01,  3.34288716e-01, -5.61198175e-01,\n",
       "        -8.62057030e-01,  1.85853511e-01, -7.70698413e-02,\n",
       "        -5.62556922e-01, -4.45770025e-01,  9.46737647e-01,\n",
       "         8.51094663e-01, -5.07376373e-01,  4.18717742e-01,\n",
       "        -3.20580512e-01, -4.25964355e-01, -3.78064513e-02,\n",
       "         1.14818774e-01,  9.81518865e-01,  3.28122944e-01,\n",
       "         8.90303433e-01,  4.65355814e-01, -1.13290399e-01,\n",
       "         9.62152898e-01,  1.72192141e-01,  5.09450972e-01,\n",
       "         4.06965092e-02,  1.00000000e+00,  2.59593844e-01,\n",
       "        -9.03993368e-01,  4.32891279e-01, -9.79339659e-01,\n",
       "        -1.81019053e-01, -9.46508408e-01,  1.77162021e-01,\n",
       "         1.46241590e-01,  8.90613556e-01, -2.52500504e-01,\n",
       "         9.52218890e-01, -4.15646285e-02,  3.20132775e-03,\n",
       "         1.02826804e-01,  2.84712434e-01,  2.71747261e-01,\n",
       "        -9.20610309e-01, -9.79607940e-01, -9.78938580e-01,\n",
       "         4.01243567e-01, -4.38529223e-01, -7.24833608e-02,\n",
       "         2.08293498e-01,  7.91824013e-02,  3.48617733e-01,\n",
       "         4.22364265e-01, -1.00000000e+00,  9.22120214e-01,\n",
       "         3.31415415e-01,  3.71091276e-01,  9.49504852e-01,\n",
       "         5.12404621e-01,  2.54232824e-01,  1.99976891e-01,\n",
       "        -9.81030881e-01, -9.72250521e-01, -3.25401336e-01,\n",
       "        -1.75557598e-01,  7.48545408e-01,  5.41270435e-01,\n",
       "         8.78954351e-01,  3.10390055e-01, -4.92143959e-01,\n",
       "        -2.79587626e-01,  1.12942476e-02, -4.91583049e-01,\n",
       "        -9.89385664e-01,  3.41896355e-01,  2.61993911e-02,\n",
       "        -9.51224983e-01,  9.50833619e-01, -1.99915230e-01,\n",
       "        -1.23899862e-01,  3.68572533e-01, -3.34178716e-01,\n",
       "         9.31527972e-01,  7.80175447e-01,  3.76220316e-01,\n",
       "         1.22112580e-01,  4.49008912e-01,  8.69153857e-01,\n",
       "         9.50493395e-01,  9.81632352e-01, -2.91600466e-01,\n",
       "         7.71657646e-01,  1.38718843e-01,  4.56029594e-01,\n",
       "         5.64257264e-01, -9.23859417e-01,  5.78307807e-02,\n",
       "         2.09333897e-01, -1.36404604e-01,  1.48712873e-01,\n",
       "        -1.20140165e-01, -9.59581792e-01,  4.89261240e-01,\n",
       "        -6.45713434e-02,  4.62750584e-01, -4.08875883e-01,\n",
       "         1.55632809e-01, -3.88409287e-01, -1.53906092e-01,\n",
       "        -6.93470180e-01, -5.08693099e-01,  5.52573025e-01,\n",
       "         2.31786638e-01,  9.06549990e-01,  5.04951537e-01,\n",
       "         2.36577559e-02, -5.28415084e-01, -2.21986882e-02,\n",
       "        -1.49413615e-01, -9.03401852e-01,  9.08470929e-01,\n",
       "         1.36968372e-02,  8.12373981e-02,  3.34383398e-01,\n",
       "        -1.22592524e-01,  7.57111371e-01, -1.72285438e-01,\n",
       "        -2.82213509e-01, -2.87530512e-01, -7.19594300e-01,\n",
       "         8.73281300e-01, -1.72636658e-01, -5.15439034e-01,\n",
       "        -5.05485117e-01,  6.00731075e-01,  2.76002467e-01,\n",
       "         9.98461604e-01, -2.58334219e-01, -3.24087113e-01,\n",
       "        -1.79645687e-01, -3.20625514e-01,  3.12705100e-01,\n",
       "        -2.12510139e-01, -1.00000000e+00,  3.46802562e-01,\n",
       "         6.10367283e-02,  4.06966239e-01, -5.14536947e-02,\n",
       "         1.22119337e-01,  2.68277638e-02, -9.77253914e-01,\n",
       "        -2.17439011e-01,  2.50956684e-01,  1.32614583e-01,\n",
       "        -5.04280031e-01, -1.83595166e-01,  4.99527067e-01,\n",
       "         5.41158319e-01,  6.44520938e-01,  8.50675702e-01,\n",
       "         6.62853941e-02,  3.54359329e-01,  5.68527460e-01,\n",
       "        -1.32894665e-01, -6.37096703e-01,  8.88132870e-01]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at the encoded input\n",
    "# Question: what is the dimension of encoding?\n",
    "# Question: why are there two encoding outputs? What are they?\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next example shows how BERT can be used as an embedding layer to build a classification model. For illustration, the [sentiment classification dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences) is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and process data\n",
    "text = []\n",
    "label = []\n",
    "for line in open(\"datasets/sentiment.txt\"):\n",
    "    line = line.rstrip('\\n').split('\\t')\n",
    "    text.append(line[0])\n",
    "    label.append(int(line[1]))\n",
    "text = np.array(text)\n",
    "label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BERT to encode texts\n",
    "vectorized_text = tokenizer(text.tolist(), return_tensors='tf', padding=True)\n",
    "bert_embeddings = bert_model(vectorized_text)['last_hidden_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For illustration, build a LSTM model with BERT embeddings\n",
    "embeddings = keras.layers.Input(shape = (bert_embeddings.shape[1], bert_embeddings.shape[2]))\n",
    "masked_embeddings = tf.keras.layers.Masking(mask_value=0)(embeddings)\n",
    "h_all, h_final, c_final = keras.layers.LSTM(units = 128,\n",
    "                                            return_state = True)(masked_embeddings)\n",
    "pred = keras.layers.Dense(units = 1,\n",
    "                          activation='sigmoid')(h_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble model\n",
    "model_bert_lstm = keras.Model(inputs = embeddings,\n",
    "                              outputs = pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure training / optimization\n",
    "model_bert_lstm.compile(loss = keras.losses.BinaryCrossentropy(),\n",
    "                        optimizer='adam',\n",
    "                        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 100, 768)]        0         \n",
      "_________________________________________________________________\n",
      "masking_4 (Masking)          (None, 100, 768)          0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                [(None, 128), (None, 128) 459264    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,393\n",
      "Trainable params: 459,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bert_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 7s 99ms/step - loss: 0.3688 - accuracy: 0.8313 - val_loss: 0.2423 - val_accuracy: 0.9033\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 6s 85ms/step - loss: 0.2231 - accuracy: 0.9142 - val_loss: 0.2508 - val_accuracy: 0.8983\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 6s 85ms/step - loss: 0.1931 - accuracy: 0.9283 - val_loss: 0.2370 - val_accuracy: 0.9050\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 6s 85ms/step - loss: 0.1470 - accuracy: 0.9450 - val_loss: 0.2318 - val_accuracy: 0.9067\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 7s 87ms/step - loss: 0.1195 - accuracy: 0.9592 - val_loss: 0.2716 - val_accuracy: 0.8867\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 7s 88ms/step - loss: 0.0976 - accuracy: 0.9646 - val_loss: 0.2464 - val_accuracy: 0.9050\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 7s 92ms/step - loss: 0.0735 - accuracy: 0.9762 - val_loss: 0.3454 - val_accuracy: 0.8850\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 7s 92ms/step - loss: 0.0568 - accuracy: 0.9800 - val_loss: 0.3162 - val_accuracy: 0.8917\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 7s 92ms/step - loss: 0.0491 - accuracy: 0.9837 - val_loss: 0.3581 - val_accuracy: 0.9000\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 7s 93ms/step - loss: 0.0456 - accuracy: 0.9825 - val_loss: 0.3133 - val_accuracy: 0.8917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e10b3bdc70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training with 20% validation and 10 epochs.\n",
    "model_bert_lstm.fit(x = bert_embeddings,\n",
    "                    y = label,\n",
    "                    batch_size = 32,\n",
    "                    epochs = 10,\n",
    "                    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources <a name=\"resource\"></a>\n",
    "\n",
    "- LLMs:\n",
    "    - [Hyung Won Chung (OpenAI) Stanford lecture](https://www.youtube.com/watch?v=orDKvo8h71o)\n",
    "    - [Paper that proposes the MoE architecture for LLM inference](https://www.jmlr.org/papers/volume23/21-0998/21-0998.pdf)\n",
    "\n",
    "- BERT:\n",
    "    - Original research paper that proposed BERT: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805). In particular, Section 3 talks about BERT model architecture;\n",
    "    - [Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html);\n",
    "    - [Text Classification with BERT](https://www.tensorflow.org/tutorials/text/classify_text_with_bert)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Some of my personal opinions: </font> A general trend in the development of language models is to _build extremely large models_, i.e., take the state-of-the-art architecture and train it with more and more parameters and on larger and larger datasets. However, looking back on what we have learned so far, you really need _fundamentally new ideas_ (e.g., from bag-of-words to embeddings, from simple RNNs to LSTMs, from RNNs + attention to transformers) to achieve significant (non-incremental) improvement. Therefore, although the transformer architecture is the current state-of-the-art, it is fundamentally unclear what else we need for the next breakthrough."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
