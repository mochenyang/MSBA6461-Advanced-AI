{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> MSBA 6461: Advanced AI for Business Applications </center>\n",
    "<center> Spring 2024, Mochen Yang </center>\n",
    "\n",
    "## <center> Attention Mechanism, Transformer Architecture, and Large Language Models </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Attention Mechanism](#attention)\n",
    "    - [What is it and Why do We Need it?](#attention_motivation)\n",
    "    - [Technical Details of Attention Mechanism](#attention_tech)\n",
    "    - [Implement Attention Mechanism in Keras](#attention_implement)\n",
    "    - [(Optional) A General Formulation of Attention Mechanism](#attention_general)\n",
    "1. [Transformer Architecture](#transformer)\n",
    "    - [What is the Transformer Architecture?](#transformer_intro)\n",
    "    - [Key Component of Transformer: Self Attention](#transformer_components)\n",
    "    - [Other Components of Transformer and its Implementation](#transformer_other)\n",
    "1. [Application Case: BERT](#bert)\n",
    "    - [What is BERT?](#bert_intro)\n",
    "    - [Use BERT](#bert_example)\n",
    "1. [Application Case: GPT](#gpt)\n",
    "1. [Large Language Models](#llm)\n",
    "1. [Additional Resources](#resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism <a name=\"attention\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it and Why do We Need it? <a name=\"attention_motivation\"></a>\n",
    "\n",
    "The basic encoder-decoder architecture discussed in the last module has achieved significant successes in applications such as machine translation. However, it also has some notable limitations. One important limitation that motivated the attention mechanism is the observation that **different parts of the input sequence are not equally important for predicting the output sequence** (see the simple example below). The basic encoder-decoder architecture cannot capture this aspect, because for each input sequence we only get a _single_ context vector that is used to generate the entire output sequence.\n",
    "\n",
    "![An Example of Why We Need Attention](images/attention.gif)\n",
    "\n",
    "image credit: https://medium.com/eleks-labs/neural-machine-translation-with-attention-mechanism-step-by-step-guide-989adb12127b\n",
    "\n",
    "This also naturally gives rise to the basic idea behind the attention mechanism: Instead of a single context vector, we now compute one context vector specifically for generating one word in the output sequence. That context vector should encode the information from input sequence that is most useful for predicting the target word in output sequence. In other words, we \"align\" the context vector for each target word in the output sequence with words in the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Details of Attention Mechanism <a name=\"attention_tech\"></a>\n",
    "\n",
    "![Illustration of Attention Mechanism](images/attention_detail.png)\n",
    "\n",
    "image credit: Figure 1 in https://arxiv.org/pdf/1409.0473.pdf. \n",
    "\n",
    "<font color=\"blue\">Note:</font> I'm going to use slightly different notations than what's in the above figure, in order to be consistent with other parts of this notebook. Specifically, I will use $t$ to index positions in the input sequence and $i$ to index positions in the output sequence (whereas they are $1, \\ldots, T$ and $t$ in the figure). Accordingly, I use $h_t^{(encoder)}$ and $h_i^{(decoder)}$ to represent encoder/decoder hidden states, whereas the same things are denoted as $h_t$ and $s_t$ in the above figure.\n",
    "\n",
    "**The encoder RNN**: same as the encoder step in a standard encoder-decoder architecture, except that we often use a bi-directional RNN (rather than a one-directional RNN). <font color=\"blue\">Intuition for using bi-directional RNN:</font> we want the hidden states of the encoder RNN to contain information of both the preceding and following words in the input sequence, to help better learn the \"alignment\" with the target word. Formally, the forward and backward pass are:\n",
    "$$\\overrightarrow{h_t^{(encoder)}} = f(\\overrightarrow{h_{t-1}^{(encoder)}}, x_t)$$\n",
    "$$\\overleftarrow{h_t^{(encoder)}} = f(\\overleftarrow{h_{t+1}^{(encoder)}}, x_t)$$\n",
    "and we concatenate the two to form the hidden state of encoder RNN at time $t$, i.e.,\n",
    "$$h_t^{(encoder)} = \\big[\\overrightarrow{h_t^{(encoder)}}, \\overleftarrow{h_t^{(encoder)}} \\big]$$\n",
    "\n",
    "**The context vector**: the context vector for target word $i$ (in the output sequence) is a **weighted sum** of all encoder hidden states:\n",
    "$$\\boldsymbol{C_i} = \\sum_{t=1}^{T} \\alpha_{it} h_t^{(encoder)}$$\n",
    "Here, $\\alpha_{it}$ are attention weights and, intuitively, they specify \"how much attention\" should be paid to each position in the input sequence when deriving the representation of $i$-th word in the output sequence. So where does the weights, $\\alpha_{it}$, come from? They are learned / trained jointly with other parameters as part of the entire model. More specifically, \n",
    "$$\\alpha_{it} = \\frac{\\exp(e_{it})}{\\sum_{k=1}^T \\exp(e_{ik})}$$\n",
    "where $e_{it}$ are often referred to as \"scores\" (and sometimes \"energy\"). Researchers have proposed multiple types of attention mechanisms, which differ in the way $e_{it}$ are computed. For example:\n",
    "- Additive Attention (aka Bahdanau Attention): $e_{it} = tanh([h_{i-1}^{(decoder)}, h_t^{(encoder)}], \\boldsymbol{W})$. In other words, the scores are computed via a (standard) feed-forward neural network with a single hidden layer and $tanh$ activation.\n",
    "- Dot-Product Attention (often used in transformer architecture): $e_{it} = (h_{i-1}^{(decoder)})^{'} h_t^{(encoder)}$, i.e., the dot product between $h_{i-1}^{(decoder)}$ and $h_t^{(encoder)}$.\n",
    "- Other types of attention mechanisms: I highly recommend taking a look at [this article](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) for details.\n",
    "\n",
    "**The decoder RNN**: at time step $i$ of the decoder RNN, it takes the hidden state from step $i-1$ as well as the context vector $\\boldsymbol{C_i}$ as input to compute the hidden state at step $t$ and then produce a prediction at that step. So,\n",
    "1. Compute next hidden state as $h_i^{(decoder)} = f(h_{i-1}^{(decoder)},y_{i-1},\\boldsymbol{C_i})$. The context vector $\\boldsymbol{C_i}$ is concatenated with the other inputs and feed into the activation function;\n",
    "2. Predict $\\hat{y_i} = softmax(h_i^{(decoder)})$ as the next word in sequence;\n",
    "3. Repeat steps 1-2 until termination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism Implementation <a name=\"attention_implement\"></a>\n",
    "\n",
    "The [`tf.keras.layers.Attention`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention) implements both the additive attention (set `score_mode = \"concat\"`) and the dot product attention (set `score_mode = \"dot\"`). For other types of attention mechanisms, you may need to do some implementation yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) A General Formulation of Attention Mechanism <a name=\"attention_general\"></a>\n",
    "\n",
    "Attention mechanism, in general, can be formulated using the (somewhat abstract and not very intuitive) terms of \"query\", \"key\", and \"value\". In NLP settings:\n",
    "- Query $Q$ typically refers to words in the output / target sequence;\n",
    "- Key $K$ and Value $V$ typically refer to words in the input sequence.\n",
    "\n",
    "Then, the context vector (aka context embeddings) produced by the attention mechanism is computed as:\n",
    "\n",
    "$$Attention(Q, K, V) = \\boldsymbol{\\alpha} V$$\n",
    "\n",
    "where the attention weights $\\boldsymbol{\\alpha}$ is a softmax transformation of certain function over $(Q, K)$ that quantifies the \"alignment\" between $Q$ and $K$, i.e.,\n",
    "\n",
    "$$\\boldsymbol{\\alpha} = softmax(align(Q, K))$$\n",
    "\n",
    "In additive attention, $Q$ is the decoder hidden states and $K,V$ are encoder hidden states, and $align(.)$ is the $tanh$ function applied over a concatenation of $Q$ and $K$.\n",
    "\n",
    "In dot-product attention, $Q$ is the decoder hidden states and $K,V$ are encoder hidden states, and $align(.)$ is simply the dot product.\n",
    "\n",
    "In general, $align(.)$ can be any function that is capable of describing the degree of \"alignment\", or very loosely speaking, similarity / association, between the query and key vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer <a name=\"transformer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Transformer Architecture? <a name=\"transformer_intro\"></a>\n",
    "\n",
    "The transformer architecture represents one of the most recent key breakthroughs in AI research. It has been applied to representation learning tasks for various different types of data, including text, image, video, time series, etc. In addition to its wide applicability, it is also responsible for many state-of-the-art results / performances in AI. Here, we focus on the transformer architecture for language understanding.\n",
    "\n",
    "The transformer architecture largely follows the same encoder-decoder structure, but seeks to completely throw away the RNNs for encoder/decoder, and only uses (a particular kind of) attention mechanism combined with fully-connected feed-forward neural networks (i.e., non-recurrent). \n",
    "\n",
    "<font color=\"red\">But why would you want to throw away the RNNs?</font> One of the key reasons is computational complexity. In a RNN, computations have to be done sequentially (e.g., processing one word after another), which prohibits parallelization. As a result, large-scale tasks with RNNs may become very slow. As you will see, most of the computations in a transformer (especially the self-attention component) can be done in a one-shot or parallel manner.\n",
    "\n",
    "There are a number of technical components to a transformer architecture (see figure below), including self-attention, positional encoding, layer normalization + residual connection. I explain the intuition behind these components, with an emphasis on the self-attention mechanism. The goal here is not to understand every single detail of a transformer model (which is still actively evolving as the field progresses), but to get a basic sense of it.\n",
    "\n",
    "![Transformer Architecture](images/transformer.png)\n",
    "\n",
    "image credit: [Attention is all You Need](https://arxiv.org/pdf/1706.03762.pdf) (Figure 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Component of Transformer: Self-Attention <a name=\"transformer_components\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention mechanism that we discussed before can be thought of as a \"layer\" that sits between an encoder and a decoder, which allows the decoder RNN to \"pay attention to\" different positions of the encoder hidden states. The transformer relies on the same attention mechanism (with a twist), namely **self-attention**.\n",
    "\n",
    "![Self-Attention Visual Illustration](images/self_attention.png)\n",
    "\n",
    "image credit: [Self-Attention For Generative Models](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture14-transformers.pdf)\n",
    "\n",
    "You can think of self-attention as a mechanism that applies to an input sequence _itself_ (like the visualization above), in order to generate a representation of the sequence that encodes information about how different words in the sequence are related to each other. In a (non-rigorous) sense, it allows the representation of the input sequence to contain information about \"interactions\" among different words in the sequence. Importantly, the entire process of calculating self-attention representation of an input sequence does NOT involve any RNNs or word-by-word recurrence. That's the point of transformer - it is a highly parallel architecture.\n",
    "\n",
    "Technically speaking, self-attention is achieved by applying one of the attention mechanisms discussed before and using the same sequence as both input and output. As a result, you get **contextual vector / embedding** for each token in the sequence. You can also create an embedding representation of the entire input sequence, by simply averaging across the embeddings of all words in that sequence. This is a form of **pooling**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) More technical details about self-attention and how it is used in the transformer architecture:\n",
    "In a transformer architecture, encoder and decoder each contains several \"blocks\" (note that the original transformer paper calls these \"layers\"). Each block contains a self-attention component and a fully-connected feed-forward neural net. These blocks are stacked; meaning the outputs of a previous block become the inputs of the next block. In other words, from the original input tokens to the final embedding representations, you will go through several times of self-attention and non-linear transformation.\n",
    "\n",
    "More formally, Let $e_t$ denote the input embedding representations at the $t$-th position to a given transformer block, then, (dot-product) self-attention transforms the representations to\n",
    "$$e_t' = softmax\\left( \\frac{e_t^T e_1}{\\sqrt{D}}, \\ldots, \\frac{e_t^T e_T}{\\sqrt{D}} \\right) e_t$$\n",
    "where $\\sqrt{D}$ is a scaling parameter based on the embedding dimension to make sure that the embeddings don't \"blow up\" when dimension is high ($e_t^T e_i$ tends to grow as $d$ increases). If you re-write the above in matrix terms, you will see that it's basically the dot-product attention mechanism where key, query, and value are all the same input embeddings. Next, the self-attention transformed embeddings go through a feed-forward neural net layer with a RELU activation followed by a linear projection:\n",
    "$$e_t'' = b_2 + W_2 RELU(b_1 + W_1 e_t')$$\n",
    "The transformed embedding $e_t''$ passes on to the next block (or to the decoder if this is the last encoder block).\n",
    "\n",
    "One more nuance: to enable even more parallelism, people may use something called a **Multi-Head Self-Attention**. The technical details of it are less relevant here, but here's the high-level analogy: you first cut the $D$-dimension embedding into $h$ smaller pieces (each with $D/h$ dimensions), apply the same self-attention mechanism to each piece (in parallel), and then concatenate the $h$ pieces into the final embedding. See [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) Section 3.2.2 for more information if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Components of Transformer and its Implementation <a name=\"transformer_other\"></a>\n",
    "\n",
    "In addition to self-attention, the transformer architecture also uses several other technical elements, such as positional encoding, layer normalization, and residual connection. Below are some optional content on these elements. The [Additional Resources](#resource) section lists articles you can read for mroe information, and for a detailed demonstration of how to implement a transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Positional Encoding\n",
    "\n",
    "Remember that we throw away the encoder and decoder RNNs, and only rely on self-attention to generate representations of the sequences? Without the sequential RNNs, the model now does not know the sequence of words in the input or output. To counter this loss of information, we try to encode the position of a word in a sequence into the embedding, using **Positional Encoding**. The positional encoding for each word at each position is another vector of the same dimension as the word embedding.\n",
    "\n",
    "In the original paper that proposed transformer, the positional encoding is calculated as follows:\n",
    "$$PE(pos, 2i) = \\sin(\\frac{pos}{10000^{2i/D}})$$\n",
    "$$PE(pos, 2i+1) = \\cos(\\frac{pos}{10000^{2i/D}})$$\n",
    "where $pos$ is a particular position in a sequence and $i \\in {0, ..., D/2}$ is a running index. <font color=\"red\">What does it mean? Let me explain with a small example.</font> \n",
    "\n",
    "Suppose you have an input sequence of 5 words, $(e_1,\\ldots, e_5)$, and each $e_t$ is a $4$-dimensional embedding (i.e. $D = 4$). Now you want to also encode the positions of each word. For the sake of demonstration, let's say you want to encode the second position, i.e., $pos=2$. You would use the formula above to compute the following:\n",
    "- Set $i=0$, $PE(2, 0) = \\sin(\\frac{2}{10000^0})=\\sin(2) \\approx 0.91$ and $PE(2, 1) = \\cos(\\frac{2}{10000^0})=\\cos(2) \\approx -0.42$;\n",
    "- Set $i=1$, $PE(2, 2) = \\sin(\\frac{2}{10000^{0.5}}) \\approx 0.02$ and $PE(2, 3) = \\cos(\\frac{2}{10000^{0.5}})=\\cos(2) \\approx 1.00$. Stop here because your embedding only has 4 dimensions.\n",
    "Then, the embedding with positional encoding for the second word in this sequence will become:\n",
    "$$e_2 + [0.91, -0.42, 0.02, 1.00]$$\n",
    "\n",
    "This works because, after injecting the positional encoding, _the second word in this sequence will have a different embedding than the same word appearing at a different position in a different sequence_. Essentially, this allows the embedding to contain position-specific information that can help learning. Finally, why using the trigonometry functions? It's mostly for mathematical convenience and it works in practice.\n",
    "\n",
    "<font color=\"blue\">If you are comfortable with trigonometry... </font> Basically, the above positional encoding function adds a position-specific vector of the following form:\n",
    "$$\\left[\\sin\\left( \\frac{pos}{10000^0} \\right), \\cos\\left( \\frac{pos}{10000^0} \\right), \\sin\\left( \\frac{pos}{10000^{2/D}} \\right), \\cos\\left( \\frac{pos}{10000^{2/D}} \\right), \\ldots, \\sin\\left( \\frac{pos}{10000} \\right), \\cos\\left( \\frac{pos}{10000} \\right) \\right]$$\n",
    "Due to the shapes of sine and cosine functions, this vector will be different for $pos \\in \\{1, \\ldots, 10000\\}$, thereby allowing you to differentiate positions up to length 10000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Layer Normalization and Residual Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both layer normalization and residual connection are tricks in deep learning to aid with training large / deep networks. Their intuitions are as follows:\n",
    "\n",
    "1. **Layer Normalization** performs a standardization (i.e., $\\frac{x - E(x)}{SD(x)}$) over all inputs in a given layer, so that the \"normalized\" inputs have mean 0 and sd 1. In the transformer architecture, within each block, the input embeddings (corresponding to all tokens in a single sequence) to the self-attention and to the feed-forward layers each go through a layer normalization operation. As a result, the normalized embeddings have mean 0 and sd 1.\n",
    "2. **Residual Connection** allows the original inputs to a layer to directly contribute to the outputs of that layer _in addition_ to any transformations imposed by the layer (i.e., allowing the inputs to \"skip\" the transformations). Informally, consider some inputs $X$ to a hidden layer in MLP that applies a nonlinear transformation $f()$. Without residual connection, the outputs from this layer would be $f(X)$. With residual connection, it will be $X + f(X)$. <font color=\"red\">Why doing this?</font> Because it allows the gradient (during training) to directly connect with the original inputs $X$ in addition to through $f(X)$.\n",
    "\n",
    "Putting everything together, what actually goes on inside each transformer block is the following: suppose $E$ represents the matrix of (positionally encoded) embedding inputs to the block. It first goes through self-attention:\n",
    "$$E' = \\text{self-attention}(E)$$\n",
    "Then, apply residual connection and layer normalization, you get:\n",
    "$$E'' = \\text{LayerNorm}(E + E')$$\n",
    "Next, it goes through the feed-forward neural net:\n",
    "$$E''' = FFNN(E'')$$\n",
    "Finally, apply residual connection and layer normalization again:\n",
    "$$E'''' = \\text{LayerNorm}(E'' + E''')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Case: BERT <a name=\"bert\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is BERT? <a name=\"bert_intro\"></a>\n",
    "\n",
    "BERT stands for _**B**idirectional **E**ncoder **R**epresentations from **T**ransformers_. It is a **language representation model**, which means it takes raw text and generate a meaningful representation (e.g., embedding) of it. It was developed by Google in 2018. With everything we have discussed so far, you are ready to make sense of all the key components of BERT:\n",
    "\n",
    "1. **B**idirectional means that, during training, the input sequences and its reverse sequence are both used;\n",
    "2. **E**ncoder **R**epresentations means that the model is aiming to generate representation of the input sequence, i.e., it acts like an encoder;\n",
    "3. **T**ransformers means that BERT uses a transformer architecture with self-attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use BERT <a name=\"bert_example\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google has released a number of different BERT models, trained with different hyperparameters. [Here is a directory of all those models](https://www.tensorflow.org/tutorials/text/classify_text_with_bert#choose_a_bert_model_to_fine-tune). You see that each model is identified by three parameters:\n",
    "- $L$: this is the number of transformer blocks. You can think of it as number of \"layers\";\n",
    "- $H$: this is the dimension of embedding. We called this $D$ in our discussion of transformer;\n",
    "- $A$: this is the number of heads in multi-head self-attention. This means cutting the embedding into $A$ pieces and apply self-attention to each piece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access pre-trained BERT models and potentially fine-tune them for your own ML tasks via [Hugging Face](https://huggingface.co/), an online platform that hosts many commonly used pre-trained models. In the following example, we access a basic BERT model and use it to encode some text. See this [page](https://huggingface.co/bert-base-uncased) for detailed documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (20.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (4.50.2)\n",
      "Requirement already satisfied: requests in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from transformers) (2020.10.15)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp38-cp38-win_amd64.whl (155 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.7.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\yang3653\\appdata\\local\\continuum\\anaconda3\\envs\\tf\\lib\\site-packages (from requests->transformers) (1.26.2)\n",
      "Installing collected packages: pyyaml, packaging, filelock, tokenizers, huggingface-hub, transformers\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.8\n",
      "    Uninstalling packaging-20.8:\n",
      "      Successfully uninstalled packaging-20.8\n",
      "Successfully installed filelock-3.7.0 huggingface-hub-0.6.0 packaging-21.3 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.1\n"
     ]
    }
   ],
   "source": [
    "# install transformer package from Hugging Face\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# fetch the pre-trained model (it will download a model file ~500M)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input text and encode\n",
    "text = \"We are using the BERT model!\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = bert_model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[  101,  2057,  2024,  2478,  1996, 14324,  2944,   999,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 9), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1]])>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the tokenized input\n",
    "# Question: what are tokens 101 and 102?\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(1, 9, 768), dtype=float32, numpy=\n",
       "array([[[ 0.10261209,  0.18043919, -0.00554929, ..., -0.166134  ,\n",
       "          0.26679957,  0.35773745],\n",
       "        [ 0.263622  , -0.21110201, -0.57594675, ..., -0.20186077,\n",
       "          1.308478  , -0.14822024],\n",
       "        [ 0.12224663, -0.15183868, -0.36246365, ..., -0.56034166,\n",
       "          0.18197185,  0.45692527],\n",
       "        ...,\n",
       "        [ 0.487611  ,  0.05848615, -0.26846886, ..., -0.64023006,\n",
       "         -0.01316616, -0.00961822],\n",
       "        [-0.16868652, -0.17555293, -0.15778571, ...,  0.54957277,\n",
       "          0.45626837, -0.39924195],\n",
       "        [ 0.52467674,  0.37009996, -0.21517405, ...,  0.00148578,\n",
       "         -0.5219994 , -0.30393368]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(1, 768), dtype=float32, numpy=\n",
       "array([[-8.78734827e-01, -3.25698197e-01, -3.28317106e-01,\n",
       "         6.70523882e-01,  6.76294491e-02, -4.97857258e-02,\n",
       "         8.80656004e-01,  2.76587784e-01, -1.80702090e-01,\n",
       "        -9.99952853e-01,  4.34783325e-02,  5.05748391e-01,\n",
       "         9.82219696e-01,  2.04030082e-01,  9.44258571e-01,\n",
       "        -6.23518169e-01,  7.85685144e-03, -5.62641799e-01,\n",
       "         2.75413692e-01, -6.43890440e-01,  6.54973030e-01,\n",
       "         9.98014152e-01,  5.39189219e-01,  2.80568451e-01,\n",
       "         3.46919954e-01,  6.95186794e-01, -6.27278507e-01,\n",
       "         9.35333967e-01,  9.50950146e-01,  7.03973353e-01,\n",
       "        -7.07702696e-01,  1.71931267e-01, -9.81890202e-01,\n",
       "        -1.79219425e-01, -4.41820800e-01, -9.87813234e-01,\n",
       "         2.80857116e-01, -7.27158844e-01,  6.23343885e-02,\n",
       "         1.89604927e-02, -8.98371339e-01,  1.58929959e-01,\n",
       "         9.99590158e-01, -4.11883116e-01,  9.89006758e-02,\n",
       "        -3.57154489e-01, -9.99994516e-01,  2.21773237e-01,\n",
       "        -8.59246016e-01,  2.33305633e-01,  2.69120753e-01,\n",
       "        -1.78335607e-01,  1.46656051e-01,  3.96779269e-01,\n",
       "         3.90325457e-01,  8.13974217e-02, -2.25540981e-01,\n",
       "         8.19753185e-02, -1.70295686e-01, -5.39923191e-01,\n",
       "        -5.92916071e-01,  2.91853637e-01, -4.13579017e-01,\n",
       "        -8.97091568e-01,  1.72661781e-01,  1.28206715e-01,\n",
       "         6.20134594e-03, -2.14740783e-01, -1.77664440e-02,\n",
       "        -3.99033651e-02,  8.69764864e-01,  2.00019509e-01,\n",
       "         2.01246832e-02, -8.21547568e-01,  1.00624897e-01,\n",
       "         1.92495257e-01, -5.63699186e-01,  1.00000000e+00,\n",
       "        -2.21787632e-01, -9.65783954e-01,  3.35388988e-01,\n",
       "         2.84605920e-01,  4.33147043e-01,  3.54774445e-01,\n",
       "         1.21714659e-01, -1.00000000e+00,  2.54565060e-01,\n",
       "        -8.17635730e-02, -9.87588823e-01,  2.04371825e-01,\n",
       "         3.83364707e-01, -1.52973607e-01,  2.03943223e-01,\n",
       "         5.20069897e-01, -4.06468898e-01, -2.53391236e-01,\n",
       "        -2.38453716e-01, -2.81073183e-01, -1.54782653e-01,\n",
       "        -2.11132560e-02, -1.77784823e-02, -2.03800425e-01,\n",
       "        -1.55137390e-01, -3.46282095e-01,  1.47095636e-01,\n",
       "        -3.79600376e-01, -3.69906008e-01,  4.54690695e-01,\n",
       "        -1.24441147e-01,  6.93087399e-01,  3.74282241e-01,\n",
       "        -3.42686892e-01,  2.79299527e-01, -9.45988655e-01,\n",
       "         5.96868396e-01, -2.80452639e-01, -9.82781827e-01,\n",
       "        -4.90857422e-01, -9.85587358e-01,  6.12973213e-01,\n",
       "        -1.43317699e-01, -6.91471174e-02,  9.56331015e-01,\n",
       "         2.70761698e-01,  3.20910186e-01, -2.27423124e-02,\n",
       "        -1.88038304e-01, -1.00000000e+00, -2.23407596e-01,\n",
       "        -3.91020864e-01, -8.88535604e-02, -1.46055534e-01,\n",
       "        -9.69378948e-01, -9.50656414e-01,  5.77531338e-01,\n",
       "         9.55189168e-01,  1.85606480e-01,  9.99256492e-01,\n",
       "        -2.20427796e-01,  9.24856126e-01, -4.45280969e-03,\n",
       "        -2.08308801e-01, -1.76956788e-01, -3.98821056e-01,\n",
       "         5.95907867e-01,  2.28622675e-01, -6.36695802e-01,\n",
       "         2.32059166e-01,  4.05228809e-02, -1.79734126e-01,\n",
       "        -3.67709398e-01, -2.29818821e-01, -2.95393378e-01,\n",
       "        -9.38730478e-01, -3.68636638e-01,  9.38845456e-01,\n",
       "        -3.78510989e-02, -3.31998199e-01,  4.98391122e-01,\n",
       "        -1.76862538e-01, -3.84551495e-01,  8.11352372e-01,\n",
       "         4.57894355e-01,  3.15411806e-01, -9.31794345e-02,\n",
       "         2.73526967e-01,  7.80324414e-02,  4.89669710e-01,\n",
       "        -8.58586490e-01,  2.35194296e-01,  3.33799511e-01,\n",
       "        -2.74861544e-01, -3.01866442e-01, -9.73188698e-01,\n",
       "        -3.18678290e-01,  5.61900198e-01,  9.84158099e-01,\n",
       "         7.78745353e-01,  2.15006962e-01,  3.41620505e-01,\n",
       "        -1.67594388e-01,  2.34875783e-01, -9.35596228e-01,\n",
       "         9.71766651e-01, -1.57090798e-01,  2.67671078e-01,\n",
       "        -4.58420590e-02,  1.93604544e-01, -8.29781353e-01,\n",
       "        -1.57395571e-01,  8.27280283e-01, -7.33340755e-02,\n",
       "        -8.31848741e-01,  9.53281522e-02, -4.05152172e-01,\n",
       "        -3.69469523e-01, -3.16484511e-01,  4.88623649e-01,\n",
       "        -2.12188363e-01, -3.50075871e-01,  3.26649733e-02,\n",
       "         9.33767915e-01,  9.68594491e-01,  7.72753537e-01,\n",
       "        -3.76413137e-01,  6.08025908e-01, -8.72552097e-01,\n",
       "        -4.03848231e-01,  1.10359572e-01,  2.27107257e-01,\n",
       "         7.50229508e-02,  9.90339756e-01, -3.98373038e-01,\n",
       "        -5.27714193e-02, -9.32333529e-01, -9.80521321e-01,\n",
       "        -1.19610526e-01, -8.63848269e-01, -1.65523347e-02,\n",
       "        -7.12563157e-01,  4.53721911e-01,  3.56207103e-01,\n",
       "         1.33278117e-01,  3.37267667e-01, -9.73698258e-01,\n",
       "        -7.70551562e-01,  3.74536067e-01, -3.18802953e-01,\n",
       "         4.42139983e-01, -2.47391507e-01,  6.06002510e-01,\n",
       "         4.42190766e-01, -5.73994458e-01,  6.87730789e-01,\n",
       "         8.98433566e-01, -3.32827747e-01, -7.47683465e-01,\n",
       "         8.23455632e-01, -2.36669004e-01,  8.63408983e-01,\n",
       "        -5.65931082e-01,  9.84694064e-01,  3.72442126e-01,\n",
       "         4.87329841e-01, -9.25008297e-01, -2.22613752e-01,\n",
       "        -8.78267169e-01, -2.02192426e-01,  2.92679667e-03,\n",
       "        -2.49615416e-01,  4.52272654e-01,  5.65947652e-01,\n",
       "         3.51830661e-01,  5.92193782e-01, -4.22784150e-01,\n",
       "         9.94401455e-01, -7.13852286e-01, -9.35386717e-01,\n",
       "         1.31308109e-01, -1.66538686e-01, -9.81504440e-01,\n",
       "         3.95584941e-01,  2.60453850e-01, -3.70292187e-01,\n",
       "        -4.29365933e-01, -6.02952719e-01, -9.48078215e-01,\n",
       "         8.63633931e-01,  1.10244907e-01,  9.86000419e-01,\n",
       "        -8.75240043e-02, -8.56879354e-01, -3.48551184e-01,\n",
       "        -9.09709930e-01, -2.01060414e-01, -1.44988850e-01,\n",
       "         2.75220066e-01, -1.36509523e-01, -9.56022561e-01,\n",
       "         4.27542418e-01,  5.26469767e-01,  4.66602892e-01,\n",
       "        -2.00794473e-01,  9.96310353e-01,  9.99979138e-01,\n",
       "         9.68557298e-01,  8.92553687e-01,  9.10764754e-01,\n",
       "        -9.90309119e-01, -2.08085820e-01,  9.99980450e-01,\n",
       "        -8.28513801e-01, -9.99999642e-01, -9.31876540e-01,\n",
       "        -3.99393380e-01,  4.16211486e-01, -1.00000000e+00,\n",
       "        -1.37056887e-01,  4.65024523e-02, -9.09568906e-01,\n",
       "         6.47307858e-02,  9.73025858e-01,  9.86061275e-01,\n",
       "        -1.00000000e+00,  8.46311450e-01,  9.31820989e-01,\n",
       "        -5.81116378e-01,  5.70267320e-01, -1.97303370e-01,\n",
       "         9.68780875e-01,  5.69642603e-01,  3.79259855e-01,\n",
       "        -2.04666719e-01,  3.69923919e-01, -5.84048033e-01,\n",
       "        -8.61861348e-01, -1.03053106e-02, -8.65144134e-02,\n",
       "         9.50026870e-01,  7.70883113e-02, -6.97549164e-01,\n",
       "        -9.16969001e-01,  5.69302402e-02, -6.45725727e-02,\n",
       "        -3.13418001e-01, -9.58576977e-01, -2.09968925e-01,\n",
       "        -2.51608610e-01,  6.16406798e-01, -2.28298791e-02,\n",
       "         1.80578277e-01, -7.21990168e-01,  2.18542159e-01,\n",
       "        -3.48271072e-01,  3.80749822e-01,  6.25714302e-01,\n",
       "        -9.38922167e-01, -5.99819243e-01,  3.32828574e-02,\n",
       "        -4.51116860e-01, -1.63040295e-01, -9.53859985e-01,\n",
       "         9.58749712e-01, -3.02904159e-01,  9.24464688e-02,\n",
       "         1.00000000e+00, -1.62672520e-01, -8.51902664e-01,\n",
       "         4.90244776e-01,  1.78810179e-01, -4.50366139e-01,\n",
       "         1.00000000e+00,  5.80830216e-01, -9.72714484e-01,\n",
       "        -5.01797080e-01,  3.75872195e-01, -4.14121807e-01,\n",
       "        -4.22003090e-01,  9.98691797e-01, -1.82080939e-01,\n",
       "        -1.20563649e-01,  2.07324669e-01,  9.62340772e-01,\n",
       "        -9.85113740e-01,  8.89728248e-01, -9.06691194e-01,\n",
       "        -9.59492743e-01,  9.53451514e-01,  9.28224027e-01,\n",
       "        -2.19139546e-01, -6.33601785e-01,  9.73953959e-03,\n",
       "        -4.24671978e-01,  2.30816096e-01, -9.53832746e-01,\n",
       "         5.05898058e-01,  4.64295417e-01, -2.92715486e-02,\n",
       "         8.66881609e-01, -7.86206365e-01, -4.74578291e-01,\n",
       "         2.97276676e-01, -7.05367848e-02,  2.77469695e-01,\n",
       "         4.53787029e-01,  4.71232116e-01, -1.64594144e-01,\n",
       "         4.72750999e-02, -1.92405149e-01, -3.66292715e-01,\n",
       "        -9.69866037e-01,  5.55830188e-02,  1.00000000e+00,\n",
       "        -1.19822342e-02, -1.82129443e-04, -2.85125434e-01,\n",
       "        -4.64347526e-02, -3.08597296e-01,  4.30776179e-01,\n",
       "         5.04439175e-01, -1.92210749e-01, -8.72224867e-01,\n",
       "         2.10603610e-01, -9.33181465e-01, -9.80421126e-01,\n",
       "         6.81359887e-01,  1.40876845e-01, -2.95872420e-01,\n",
       "         9.99806762e-01,  2.78796524e-01,  1.62152648e-01,\n",
       "        -2.52243690e-03,  7.11778164e-01,  2.15992425e-02,\n",
       "         5.62938333e-01,  2.97641546e-01,  9.69979882e-01,\n",
       "        -1.88880622e-01,  4.88082379e-01,  8.20398033e-01,\n",
       "        -3.50302339e-01, -2.83560812e-01, -6.12565398e-01,\n",
       "        -1.54352551e-02, -9.12919223e-01,  1.25817940e-01,\n",
       "        -9.36130702e-01,  9.58291292e-01,  1.21257074e-01,\n",
       "         3.43669832e-01,  1.40896946e-01,  1.68940544e-01,\n",
       "         1.00000000e+00, -2.85069913e-01,  5.92145979e-01,\n",
       "        -2.80246139e-01,  7.94635534e-01, -9.86922204e-01,\n",
       "        -7.84085691e-01, -2.62509823e-01,  6.59338906e-02,\n",
       "        -1.58151209e-01, -3.14305753e-01,  2.20784366e-01,\n",
       "        -9.58403230e-01,  2.56395519e-01,  1.25021935e-02,\n",
       "        -9.79425430e-01, -9.86912072e-01,  4.38348323e-01,\n",
       "         7.55907476e-01, -1.42517211e-02, -8.38730633e-01,\n",
       "        -6.34958088e-01, -5.94622850e-01,  1.47147089e-01,\n",
       "        -1.06524996e-01, -9.17863846e-01,  3.64055544e-01,\n",
       "        -2.05472842e-01,  4.54669982e-01, -2.39828452e-01,\n",
       "         4.91848916e-01,  2.35310212e-01,  7.26663113e-01,\n",
       "         4.38361131e-02,  5.74373975e-02, -1.28145062e-03,\n",
       "        -7.46204972e-01,  7.52747834e-01, -7.80600667e-01,\n",
       "        -3.78396809e-01, -1.41046494e-01,  1.00000000e+00,\n",
       "        -4.99441266e-01,  5.26705384e-01,  7.41574466e-01,\n",
       "         5.85063875e-01, -1.36753604e-01,  1.90162152e-01,\n",
       "         5.56856811e-01,  1.28343463e-01, -2.21230760e-01,\n",
       "        -2.11827576e-01, -6.68248892e-01, -3.48785311e-01,\n",
       "         4.95144486e-01, -3.12088039e-02,  2.19363078e-01,\n",
       "         7.48234987e-01,  5.40513098e-01,  7.21349344e-02,\n",
       "         4.29088585e-02, -1.00242995e-01,  9.98324156e-01,\n",
       "        -6.53915433e-03, -4.12907377e-02, -4.87146556e-01,\n",
       "        -3.57575505e-03, -2.69600600e-01, -4.04136598e-01,\n",
       "         1.00000000e+00,  3.01777363e-01,  1.71233073e-01,\n",
       "        -9.84637022e-01, -2.41307542e-01, -9.09752011e-01,\n",
       "         9.99922574e-01,  8.06832492e-01, -8.09211612e-01,\n",
       "         4.72138733e-01,  2.98552185e-01, -4.52987812e-02,\n",
       "         7.47803211e-01, -1.44971177e-01, -2.46243000e-01,\n",
       "         2.18701512e-01,  1.29333556e-01,  9.53508556e-01,\n",
       "        -4.58412379e-01, -9.55183566e-01, -6.00613952e-01,\n",
       "         3.09518337e-01, -9.59915340e-01,  9.94016469e-01,\n",
       "        -4.43690896e-01, -2.07088172e-01, -3.03067625e-01,\n",
       "         3.57810915e-01,  5.09513378e-01, -6.86136857e-02,\n",
       "        -9.77442324e-01, -6.21347539e-02,  3.58104822e-03,\n",
       "         9.57711220e-01,  1.13723092e-01, -4.93515849e-01,\n",
       "        -9.06929135e-01, -1.00530379e-01,  1.33775160e-01,\n",
       "        -2.13098109e-01, -9.15971696e-01,  9.65532660e-01,\n",
       "        -9.79793191e-01,  4.09691811e-01,  9.99999523e-01,\n",
       "         3.05352211e-01, -6.46948278e-01,  5.35247959e-02,\n",
       "        -3.98079813e-01,  2.45531783e-01,  2.19445884e-01,\n",
       "         5.48078120e-01, -9.49364185e-01, -2.64547527e-01,\n",
       "        -1.41302705e-01,  2.11955592e-01, -1.21996567e-01,\n",
       "         4.30372626e-01,  6.55721903e-01,  2.51634300e-01,\n",
       "        -4.27288145e-01, -5.06994188e-01,  7.39996042e-03,\n",
       "         3.67606819e-01,  8.17710817e-01, -2.35767588e-01,\n",
       "        -5.30820861e-02,  2.92331260e-02, -1.22121580e-01,\n",
       "        -9.15593028e-01, -1.86418876e-01, -2.00486958e-01,\n",
       "        -9.99089420e-01,  6.34987772e-01, -1.00000000e+00,\n",
       "        -1.73069268e-01, -1.95214495e-01, -1.84107035e-01,\n",
       "         7.88663924e-01,  3.78994077e-01,  2.39148498e-01,\n",
       "        -7.13645697e-01, -2.39147753e-01,  7.18260825e-01,\n",
       "         6.80139780e-01, -1.17763393e-01,  2.29843318e-01,\n",
       "        -6.68980658e-01,  1.63184345e-01, -1.02425866e-01,\n",
       "         2.34207973e-01, -5.40278628e-02,  7.63468981e-01,\n",
       "        -1.10938117e-01,  1.00000000e+00,  1.09856166e-01,\n",
       "        -4.92708892e-01, -9.65841711e-01,  2.00337559e-01,\n",
       "        -1.63144931e-01,  9.99994397e-01, -8.80369008e-01,\n",
       "        -9.28947151e-01,  3.34288716e-01, -5.61198175e-01,\n",
       "        -8.62057030e-01,  1.85853511e-01, -7.70698413e-02,\n",
       "        -5.62556922e-01, -4.45770025e-01,  9.46737647e-01,\n",
       "         8.51094663e-01, -5.07376373e-01,  4.18717742e-01,\n",
       "        -3.20580512e-01, -4.25964355e-01, -3.78064513e-02,\n",
       "         1.14818774e-01,  9.81518865e-01,  3.28122944e-01,\n",
       "         8.90303433e-01,  4.65355814e-01, -1.13290399e-01,\n",
       "         9.62152898e-01,  1.72192141e-01,  5.09450972e-01,\n",
       "         4.06965092e-02,  1.00000000e+00,  2.59593844e-01,\n",
       "        -9.03993368e-01,  4.32891279e-01, -9.79339659e-01,\n",
       "        -1.81019053e-01, -9.46508408e-01,  1.77162021e-01,\n",
       "         1.46241590e-01,  8.90613556e-01, -2.52500504e-01,\n",
       "         9.52218890e-01, -4.15646285e-02,  3.20132775e-03,\n",
       "         1.02826804e-01,  2.84712434e-01,  2.71747261e-01,\n",
       "        -9.20610309e-01, -9.79607940e-01, -9.78938580e-01,\n",
       "         4.01243567e-01, -4.38529223e-01, -7.24833608e-02,\n",
       "         2.08293498e-01,  7.91824013e-02,  3.48617733e-01,\n",
       "         4.22364265e-01, -1.00000000e+00,  9.22120214e-01,\n",
       "         3.31415415e-01,  3.71091276e-01,  9.49504852e-01,\n",
       "         5.12404621e-01,  2.54232824e-01,  1.99976891e-01,\n",
       "        -9.81030881e-01, -9.72250521e-01, -3.25401336e-01,\n",
       "        -1.75557598e-01,  7.48545408e-01,  5.41270435e-01,\n",
       "         8.78954351e-01,  3.10390055e-01, -4.92143959e-01,\n",
       "        -2.79587626e-01,  1.12942476e-02, -4.91583049e-01,\n",
       "        -9.89385664e-01,  3.41896355e-01,  2.61993911e-02,\n",
       "        -9.51224983e-01,  9.50833619e-01, -1.99915230e-01,\n",
       "        -1.23899862e-01,  3.68572533e-01, -3.34178716e-01,\n",
       "         9.31527972e-01,  7.80175447e-01,  3.76220316e-01,\n",
       "         1.22112580e-01,  4.49008912e-01,  8.69153857e-01,\n",
       "         9.50493395e-01,  9.81632352e-01, -2.91600466e-01,\n",
       "         7.71657646e-01,  1.38718843e-01,  4.56029594e-01,\n",
       "         5.64257264e-01, -9.23859417e-01,  5.78307807e-02,\n",
       "         2.09333897e-01, -1.36404604e-01,  1.48712873e-01,\n",
       "        -1.20140165e-01, -9.59581792e-01,  4.89261240e-01,\n",
       "        -6.45713434e-02,  4.62750584e-01, -4.08875883e-01,\n",
       "         1.55632809e-01, -3.88409287e-01, -1.53906092e-01,\n",
       "        -6.93470180e-01, -5.08693099e-01,  5.52573025e-01,\n",
       "         2.31786638e-01,  9.06549990e-01,  5.04951537e-01,\n",
       "         2.36577559e-02, -5.28415084e-01, -2.21986882e-02,\n",
       "        -1.49413615e-01, -9.03401852e-01,  9.08470929e-01,\n",
       "         1.36968372e-02,  8.12373981e-02,  3.34383398e-01,\n",
       "        -1.22592524e-01,  7.57111371e-01, -1.72285438e-01,\n",
       "        -2.82213509e-01, -2.87530512e-01, -7.19594300e-01,\n",
       "         8.73281300e-01, -1.72636658e-01, -5.15439034e-01,\n",
       "        -5.05485117e-01,  6.00731075e-01,  2.76002467e-01,\n",
       "         9.98461604e-01, -2.58334219e-01, -3.24087113e-01,\n",
       "        -1.79645687e-01, -3.20625514e-01,  3.12705100e-01,\n",
       "        -2.12510139e-01, -1.00000000e+00,  3.46802562e-01,\n",
       "         6.10367283e-02,  4.06966239e-01, -5.14536947e-02,\n",
       "         1.22119337e-01,  2.68277638e-02, -9.77253914e-01,\n",
       "        -2.17439011e-01,  2.50956684e-01,  1.32614583e-01,\n",
       "        -5.04280031e-01, -1.83595166e-01,  4.99527067e-01,\n",
       "         5.41158319e-01,  6.44520938e-01,  8.50675702e-01,\n",
       "         6.62853941e-02,  3.54359329e-01,  5.68527460e-01,\n",
       "        -1.32894665e-01, -6.37096703e-01,  8.88132870e-01]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the encoded input\n",
    "# Question: what is the dimension of encoding?\n",
    "# Question: why are there two encoding outputs? What are they?\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next example shows how BERT can be used as an embedding layer to build a classification model. For illustration, the [sentiment classification dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences) is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and process data\n",
    "text = []\n",
    "label = []\n",
    "for line in open(\"datasets/sentiment.txt\"):\n",
    "    line = line.rstrip('\\n').split('\\t')\n",
    "    text.append(line[0])\n",
    "    label.append(int(line[1]))\n",
    "text = np.array(text)\n",
    "label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BERT to encode texts\n",
    "vectorized_text = tokenizer(text.tolist(), return_tensors='tf', padding=True)\n",
    "bert_embeddings = bert_model(vectorized_text)['last_hidden_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For illustration, build a LSTM model with BERT embeddings\n",
    "embeddings = keras.layers.Input(shape = (bert_embeddings.shape[1], bert_embeddings.shape[2]))\n",
    "masked_embeddings = tf.keras.layers.Masking(mask_value=0)(embeddings)\n",
    "h_all, h_final, c_final = keras.layers.LSTM(units = 128,\n",
    "                                            return_state = True)(masked_embeddings)\n",
    "pred = keras.layers.Dense(units = 1,\n",
    "                          activation='sigmoid')(h_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble model\n",
    "model_bert_lstm = keras.Model(inputs = embeddings,\n",
    "                              outputs = pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure training / optimization\n",
    "model_bert_lstm.compile(loss = keras.losses.BinaryCrossentropy(),\n",
    "                        optimizer='adam',\n",
    "                        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 100, 768)]        0         \n",
      "_________________________________________________________________\n",
      "masking_4 (Masking)          (None, 100, 768)          0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                [(None, 128), (None, 128) 459264    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 459,393\n",
      "Trainable params: 459,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bert_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 7s 99ms/step - loss: 0.3688 - accuracy: 0.8313 - val_loss: 0.2423 - val_accuracy: 0.9033\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 6s 85ms/step - loss: 0.2231 - accuracy: 0.9142 - val_loss: 0.2508 - val_accuracy: 0.8983\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 6s 85ms/step - loss: 0.1931 - accuracy: 0.9283 - val_loss: 0.2370 - val_accuracy: 0.9050\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 6s 85ms/step - loss: 0.1470 - accuracy: 0.9450 - val_loss: 0.2318 - val_accuracy: 0.9067\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 7s 87ms/step - loss: 0.1195 - accuracy: 0.9592 - val_loss: 0.2716 - val_accuracy: 0.8867\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 7s 88ms/step - loss: 0.0976 - accuracy: 0.9646 - val_loss: 0.2464 - val_accuracy: 0.9050\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 7s 92ms/step - loss: 0.0735 - accuracy: 0.9762 - val_loss: 0.3454 - val_accuracy: 0.8850\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 7s 92ms/step - loss: 0.0568 - accuracy: 0.9800 - val_loss: 0.3162 - val_accuracy: 0.8917\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 7s 92ms/step - loss: 0.0491 - accuracy: 0.9837 - val_loss: 0.3581 - val_accuracy: 0.9000\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 7s 93ms/step - loss: 0.0456 - accuracy: 0.9825 - val_loss: 0.3133 - val_accuracy: 0.8917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e10b3bdc70>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training with 20% validation and 10 epochs.\n",
    "model_bert_lstm.fit(x = bert_embeddings,\n",
    "                    y = label,\n",
    "                    batch_size = 32,\n",
    "                    epochs = 10,\n",
    "                    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Case: GPT <a name=\"gpt\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another set of famous applications of transformers is the Generative Pre-Training (GPT) models developed by [OpenAI](openai.com). Here are some resources for you to read:\n",
    "- GPT-1: [Blog Post](https://openai.com/blog/language-unsupervised/), [Reserach Paper](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf);\n",
    "- GPT-2: [Blog Post](https://openai.com/blog/better-language-models/), [Research Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). GPT-2 is basically GPT-1 trained on much more data with much more parameters (1.5 Billion parameters), but the underlying model structure is largely the same;\n",
    "- GPT-3: [Blog Post](https://openai.com/blog/openai-api/), [Research Paper](https://arxiv.org/pdf/2005.14165.pdf). Again, GPT-3 largely uses the same architecture as GPT-2, with much more parameters (175 billion parameters).\n",
    "- GPT-4: [Blog Post](https://openai.com/research/gpt-4), [Research Paper](https://arxiv.org/abs/2303.08774). Detailed architecture is (intentionally) unclear. We know it's trained with the additional RLHF (Reinforcement Learning with Human Feedback). \n",
    "\n",
    "You can use GPT-family models via [Hugging Face](https://huggingface.co/gpt2) or [OpenAI API](https://platform.openai.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Models <a name=\"llm\"></a>\n",
    "\n",
    "BERT and GPT are both examples of Large Language Models (LLMs), which are language models that can process, \"understand\", and generate texts. They are \"large\" because they are often trained on enormous amount of textual data and have a huge number of parameters. They learn representations of a language, and can be further fine-tuned for a variety of different language tasks.\n",
    "\n",
    "LLMs are perhaps the most exciting major advancement in AI currently. Here are some articles in popular press that discuss its implications and impact:\n",
    "- [The emerging types of language models and why they matter](https://tcrn.ch/3Kj0njm)\n",
    "- [How Large Language Models Will Transform Science, Society, and AI](https://hai.stanford.edu/news/how-large-language-models-will-transform-science-society-and-ai)\n",
    "- [Codify Intelligence with Large Language Models](https://www.nvidia.com/en-us/deep-learning-ai/solutions/large-language-models/)\n",
    "- [Large Language Models Will Define Artificial Intelligence](https://www.forbes.com/sites/garydrenik/2023/01/11/large-language-models-will-define-artificial-intelligence/?sh=44c9d418b60f)\n",
    "\n",
    "Some pretty impressive recent LLMs:\n",
    "- [Google LaMDA](https://blog.google/technology/ai/lamda/)\n",
    "- [DeepMind Chinchilla](https://gpt3demo.com/apps/chinchilla-deepmind)\n",
    "- [OpenAI ChatGPT](https://openai.com/blog/chatgpt/) and [GPT-4](https://openai.com/research/gpt-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Some of my personal opinions: </font> A general trend in the development of language models is to _build extremely large models_, i.e., take the state-of-the-art architecture and train it with more and more parameters and on larger and larger datasets. However, looking back on what we have learned so far, you really need _fundamentally new ideas_ (e.g., from bag-of-words to embeddings, from simple RNNs to LSTMs, from RNNs + attention to transformers) to achieve significant (non-incremental) improvement. Therefore, although the transformer architecture is the current state-of-the-art, it is fundamentally unclear what else we need for the next breakthrough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources <a name=\"resource\"></a>\n",
    "\n",
    "- Attention Mechanism:\n",
    "    - Original research paper that proposed the attention mechanism: [Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473.pdf?utm_source=ColumnsChannel);\n",
    "    - Implementation of attention: [Neural machine translation with attention](https://www.tensorflow.org/tutorials/text/nmt_with_attention);\n",
    "- Transformer:\n",
    "    - Original research paper that proposed the transformer architecture: [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf);\n",
    "    - Original paper on self-attention: [Long Short-Term Memory-Networks for Machine Reading](https://arxiv.org/pdf/1601.06733.pdf);\n",
    "    - Additional articles to learn about self-attention: [Illustrated: Self-Attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a), [Introduction of Self-Attention Layer in Transformer](https://medium.com/lsc-psd/introduction-of-self-attention-layer-in-transformer-fc7bff63f3bc);\n",
    "    - Additional articles on other components in a transformer: [Layer Normalization](https://arxiv.org/abs/1607.06450), [Normalization Techniques in Deep Neural Networks](https://medium.com/techspace-usict/normalization-techniques-in-deep-neural-networks-9121bf100d8), [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385);\n",
    "    - Implementation of Transformer: [Transformer model for language understanding](https://www.tensorflow.org/tutorials/text/transformer);\n",
    "    - [Transformer for text classification](https://keras.io/examples/nlp/text_classification_with_transformer/)\n",
    "    - Andrej Karpathy's [YouTube Tutorial](https://www.youtube.com/watch?v=kCc8FmEb1nY)\n",
    "- BERT:\n",
    "    - Original research paper that proposed BERT: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805). In particular, Section 3 talks about BERT model architecture;\n",
    "    - [Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html);\n",
    "    - [Text Classification with BERT](https://www.tensorflow.org/tutorials/text/classify_text_with_bert)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
