{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> MSBA 6461: Advanced AI for Natural Language Processing </center>\n",
    "<center> Summer 2025, Mochen Yang </center>\n",
    "\n",
    "## <center> Attention Mechanism, Transformer Architecture, and Large Language Models </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Large Language Models](#llm)\n",
    "1. [Additional Resources](#resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Models <a name=\"llm\"></a>\n",
    "\n",
    "BERT and GPT are both examples of Large Language Models (LLMs), which are language models that can process, \"understand\", and generate texts. They are \"large\" because they are often trained on enormous amount of textual data and have a huge number of parameters. They learn representations of a language, and can be further fine-tuned for a variety of different language tasks.\n",
    "\n",
    "LLMs are perhaps the most exciting major advancement in AI currently. Here are some articles in popular press that discuss its implications and impact:\n",
    "- [The emerging types of language models and why they matter](https://tcrn.ch/3Kj0njm)\n",
    "- [How Large Language Models Will Transform Science, Society, and AI](https://hai.stanford.edu/news/how-large-language-models-will-transform-science-society-and-ai)\n",
    "- [Codify Intelligence with Large Language Models](https://www.nvidia.com/en-us/deep-learning-ai/solutions/large-language-models/)\n",
    "- [Large Language Models Will Define Artificial Intelligence](https://www.forbes.com/sites/garydrenik/2023/01/11/large-language-models-will-define-artificial-intelligence/?sh=44c9d418b60f)\n",
    "\n",
    "Some pretty impressive recent LLMs:\n",
    "- [OpenAI ChatGPT](https://openai.com/blog/chatgpt/) and [GPT-4](https://openai.com/research/gpt-4)\n",
    "- [Llama](https://llama.meta.com/llama2/)\n",
    "- [Mistral](https://mistral.ai/)\n",
    "- [Claude](https://www.claude.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">Some of my personal opinions: </font> A general trend in the development of language models is to _build extremely large models_, i.e., take the state-of-the-art architecture and train it with more and more parameters and on larger and larger datasets. However, looking back on what we have learned so far, you really need _fundamentally new ideas_ (e.g., from bag-of-words to embeddings, from simple RNNs to LSTMs, from RNNs + attention to transformers) to achieve significant (non-incremental) improvement. Therefore, although the transformer architecture is the current state-of-the-art, it is fundamentally unclear what else we need for the next breakthrough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources <a name=\"resource\"></a>\n",
    "\n",
    "- TBD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
