{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> MSBA 6461: Advanced AI for Natural Language Processing </center>\n",
    "<center> Summer 2025, Mochen Yang </center>\n",
    "\n",
    "## <center> Transformer Architecture </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Transformer Architecture](#transformer)\n",
    "    - [What is the Transformer Architecture?](#transformer_intro)\n",
    "    - [Self Attention](#transformer_components)\n",
    "        - [Multi-Head Attention](#multihead)\n",
    "        - [Feedforward Neural Network](#FFNN)\n",
    "    - [Other Components of Transformer](#transformer_other)\n",
    "        - [Positional Encoding](#transformer_other_pe)\n",
    "        - [Layer Normalization and Residual Connection](#transformer_other_lnrc)\n",
    "        - [Putting Everything Together](#transformer_all)\n",
    "    - [Encoder vs. Decoder](#transformer_encoder_decoder)\n",
    "    - [Temperature](#transformer_temperature)\n",
    "1. [Transformer Implementation: A Step-by-Step Explanation](#implementation)\n",
    "1. [Additional Resources](#resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer <a name=\"transformer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer architecture is arguably one of the most important deep learning architectures we have right now. It is the bedrock of virtually all large language models on the market. It has been applied to representation learning tasks for various different types of data, including text, image, video, time series, etc. In addition to its wide applicability, it is also responsible for many state-of-the-art results / performances in AI. The goal of this notebook is to offer an in-depth yet accessible exposition of the transformer architecture (mostly based on [this seminal paper](https://arxiv.org/pdf/1706.03762)) with small-scale demonstrations (for actual implementations, please refer to ```pytorch/Transformer.ipynb```)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Transformer Architecture? <a name=\"transformer_intro\"></a>\n",
    "\n",
    "The transformer architecture we will discuss here largely follows the same encoder-decoder structure, but seeks to completely throw away the RNNs for encoder/decoder, and only uses (a particular kind of) attention mechanism combined with fully-connected feed-forward neural networks (i.e., non-recurrent). \n",
    "\n",
    "<font color=\"red\">But why would you want to throw away the RNNs?</font> One of the key reasons is computational complexity. In a RNN, computations have to be done sequentially (e.g., processing one word after another), which prohibits parallelization. As a result, large-scale tasks with RNNs may become very slow. As you will see, most of the computations in a transformer (especially the self-attention component) can be done in a parallel manner.\n",
    "\n",
    "There are a number of technical components to a transformer architecture (see figure below), including self-attention, positional encoding, layer normalization, and residual connection. I will explain the intuition behind these components, with an emphasis on the self-attention mechanism. \n",
    "\n",
    "![Transformer Architecture](images/transformer.png)\n",
    "\n",
    "image credit: [Attention is all You Need](https://arxiv.org/pdf/1706.03762.pdf) (Figure 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention <a name=\"transformer_components\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention mechanism that we discussed before can be thought of as a \"layer\" that sits between an encoder and a decoder, which allows the decoder RNN to \"pay attention to\" different positions of the encoder hidden states. Because the attention layer is between encoder and decoder, it is often referred to as **cross-attention**. The transformer architecture relies on a twist of this attention mechanism, namely **self-attention**.\n",
    "\n",
    "![Self-Attention Visual Illustration](images/self_attention.png)\n",
    "\n",
    "image credit: [Self-Attention For Generative Models](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture14-transformers.pdf)\n",
    "\n",
    "You can think of self-attention as a mechanism that applies to an input sequence _itself_ (like the visualization above), in order to generate a representation of the sequence that encodes information about how different words in the sequence are related to each other. In a (non-rigorous) sense, it allows the representation of the input sequence to contain information about \"interactions\" among different words in the sequence. Importantly, the entire process of calculating self-attention representation of an input sequence does NOT involve any RNNs or word-by-word recurrence. That's the point of transformer - it is a highly parallel architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get technical about self-attention. Given a sequence of tokens $(e_1, \\ldots, e_T)$ where $e_t$ is the embedding representation (dimension = $D$) of the $t$-th token, the self-attention mechanism seeks to \"associate\" each token with all the other tokens and incorporate those associations into the (attention-enriched) representation of the token. Specifically, self-attention based on dot-product transforms $e_t$ to\n",
    "$$e_t^{Attn} = softmax\\left( \\frac{e_t \\cdot e_1}{\\sqrt{D}}, \\ldots, \\frac{e_t \\cdot e_T}{\\sqrt{D}} \\right) \\cdot (e_1, \\ldots, e_T)$$\n",
    "where $\\cdot$ is the dot-product operation. $\\sqrt{D}$ is a scaling parameter based on the embedding dimension to make sure that the embeddings don't \"blow up\" when dimension is high ($e_t^T e_i$ tends to grow as $d$ increases). If you re-write the above in matrix terms, you will see that it's basically the dot-product attention mechanism where key ($K$), query ($Q$), and value ($V$) are all the same input embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention <a name=\"multihead\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable even more parallelism, people often use something called a **Multi-Head Self-Attention**. The high-level idea is you project $Q, K, V$ multiple times with trainable weight matrices, apply the self-attention, then concatenate the results together. More technical details below.\n",
    "\n",
    "For better notations, let's pack all embeddings of the sequence into a matrix of shape $(T, D)$ (i.e., one token embedding per row). The above (single-head) attention mechanism can be represented in the following matrix format:\n",
    "$$ Attention(Q, K, V) = softmax\\left(\\frac{QK'}{\\sqrt{D}} \\right) V $$\n",
    "where $K = Q = V$ are all the same embedding matrix.\n",
    "\n",
    "Then, with multi-head attention, we will first project the key, query, and value matrices into lower-dimensional embedding matrices. This is done by multiplying them with separate weight matrices $W^K$, $W^Q$, $W^V$. Consider, for example, a 4-head self-attention, then the shape of the three weight matrices would be $(D, D/4)$. Next, for each head $i \\in \\{1,2,3,4\\}$, we will compute the regular self-attention as:\n",
    "$$ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "\n",
    "Finally, the 4 heads are concatenated together, followed by another projection by weight matrix, $W^O$ of shape $(D, D)$, to produce the final multi-head self-attention embeddings:\n",
    "$$ MultiHead(Q, K, V) = (head_1, head_2, head_3, head_4) W^O $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network <a name=\"FFNN\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After (multi-head) self-attention, the transformed embeddings will go through a feedforward neural network for additional non-linear transformations. The network uses RELU activation, followed by a linear projection:\n",
    "$$e_t^{Attn+FFNN} = b_2 + W_2 RELU(b_1 + W_1 e_t^{Attn})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a transformer architecture, both encoder and decoder each contains several \"blocks\" (note that the original transformer paper calls these \"layers\"). Each block contains a self-attention component and a fully-connected feed-forward neural net. These blocks are stacked; meaning the outputs of a previous block become the inputs of the next block. In other words, from the original input tokens to the final embedding representations, you will go through several times of self-attention and non-linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Components of Transformer <a name=\"transformer_other\"></a>\n",
    "\n",
    "In addition to self-attention, the transformer architecture also uses several other technical elements, such as positional encoding, layer normalization, and residual connection. Below are some optional content on these elements. The [Additional Resources](#resource) section lists articles you can read for mroe information, and for a detailed demonstration of how to implement a transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding <a name=\"transformer_other_pe\"></a>\n",
    "\n",
    "\n",
    "Remember that we throw away the encoder and decoder RNNs, and only rely on self-attention to generate representations of the sequences? Without the sequential RNNs, the model now does not know the sequence of words in the input or output. To counter this loss of information, we try to encode the position of a word in a sequence into the embedding, using **Positional Encoding**. The positional encoding for each word at each position is another vector of the same dimension as the word embedding.\n",
    "\n",
    "In the original paper that proposed transformer, the positional encoding is calculated as follows:\n",
    "$$PE(pos, 2i) = \\sin(\\frac{pos}{10000^{2i/D}})$$\n",
    "$$PE(pos, 2i+1) = \\cos(\\frac{pos}{10000^{2i/D}})$$\n",
    "where $pos$ is a particular position in a sequence and $i \\in {0, ..., D/2}$ is a running index. <font color=\"red\">What does it mean? Let me explain with a small example.</font> \n",
    "\n",
    "Suppose you have an input sequence of 5 words, $(e_1,\\ldots, e_5)$, and each $e_t$ is a $4$-dimensional embedding (i.e. $D = 4$). Now you want to also encode the positions of each word. For the sake of demonstration, let's say you want to encode the second position, i.e., $pos=2$. You would use the formula above to compute the following:\n",
    "- Set $i=0$, $PE(2, 0) = \\sin(\\frac{2}{10000^0})=\\sin(2) \\approx 0.91$ and $PE(2, 1) = \\cos(\\frac{2}{10000^0})=\\cos(2) \\approx -0.42$;\n",
    "- Set $i=1$, $PE(2, 2) = \\sin(\\frac{2}{10000^{0.5}}) \\approx 0.02$ and $PE(2, 3) = \\cos(\\frac{2}{10000^{0.5}})=\\cos(2) \\approx 1.00$. Stop here because your embedding only has 4 dimensions.\n",
    "Then, the embedding with positional encoding for the second word in this sequence will become:\n",
    "$$e_2 + [0.91, -0.42, 0.02, 1.00]$$\n",
    "\n",
    "This works because, after injecting the positional encoding, _the second word in this sequence will have a different embedding than the same word appearing at a different position in a different sequence_. Essentially, this allows the embedding to contain position-specific information that can help learning. Finally, why using the trigonometry functions? It's mostly for mathematical convenience and it works in practice.\n",
    "\n",
    "<font color=\"blue\">If you are comfortable with trigonometry... </font> Basically, the above positional encoding function adds a position-specific vector of the following form:\n",
    "$$\\left[\\sin\\left( \\frac{pos}{10000^0} \\right), \\cos\\left( \\frac{pos}{10000^0} \\right), \\sin\\left( \\frac{pos}{10000^{2/D}} \\right), \\cos\\left( \\frac{pos}{10000^{2/D}} \\right), \\ldots, \\sin\\left( \\frac{pos}{10000} \\right), \\cos\\left( \\frac{pos}{10000} \\right) \\right]$$\n",
    "Due to the shapes of sine and cosine functions, this vector will be different for $pos \\in \\{1, \\ldots, 10000\\}$, thereby allowing you to differentiate positions up to length 10000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization and Residual Connection <a name=\"transformer_other_lnrc\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both layer normalization and residual connection are tricks in deep learning to aid with training large / deep networks. Their intuitions are as follows:\n",
    "\n",
    "1. **Layer Normalization** performs a standardization (i.e., $\\frac{x - E(x)}{SD(x)}$) over all inputs in a given layer, so that the \"normalized\" inputs have mean 0 and sd 1. In the transformer architecture, within each block, the input embeddings (corresponding to all tokens in a single sequence) to the self-attention and to the feed-forward layers each go through a layer normalization operation. As a result, the normalized embeddings have mean 0 and sd 1.\n",
    "2. **Residual Connection** allows the original inputs to a layer to directly contribute to the outputs of that layer _in addition_ to any transformations imposed by the layer (i.e., allowing the inputs to \"skip\" the transformations). Informally, consider some inputs $X$ to a hidden layer in MLP that applies a nonlinear transformation $f()$. Without residual connection, the outputs from this layer would be $f(X)$. With residual connection, it will be $X + f(X)$. <font color=\"red\">Why doing this?</font> Because it allows the gradient (during training) to directly connect with the original inputs $X$ in addition to through $f(X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Everything Together <a name=\"transformer_all\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting everything together, what actually goes on inside each transformer block (using the encoder side as an example) is the following: suppose $E$ represents the matrix of (positionally encoded) embedding inputs to the block. It first goes through (multi-head) self-attention:\n",
    "$$E' = \\text{self-attention}(E)$$\n",
    "Then, apply residual connection and layer normalization, you get:\n",
    "$$E'' = \\text{LayerNorm}(E + E')$$\n",
    "Next, it goes through the feed-forward neural net:\n",
    "$$E''' = FFNN(E'')$$\n",
    "Finally, apply residual connection and layer normalization again:\n",
    "$$E'''' = \\text{LayerNorm}(E'' + E''')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder vs. Decoder <a name=\"transformer_encoder_decoder\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although both encoder and decoder follows roughly the same stacked architecture, they have some important differences that are worth clarifying. For concreteness, let's consider a translation task (like the English-to-Spanish translation task discussed in the \"sequence-to-sequence modeling\" lecture). Suppose the input sequence (in English) is $(e_1, \\ldots, e_T)$ and the output sequence (in Spanish) is $(s_1, \\ldots, s_{T'})$.\n",
    "\n",
    "The first difference is in the details of self-attention. In the encoder, input sequence go through a **bidirectional** self-attention transformation (as described above), in the sense that every position in the sequence can attend to every other position in the sequence. However, in the decoder, input sequence go through a **masked** self-attention (also called **causal** self-attention), where position $t$ can only attend to positions $i \\leq t$ but not to future positions. This is because, during inference time, we will not know future tokens in the decoding process. The masked self-attention is achieved by replacing the values inside softmax that correspond to illegitimate pairs to $-\\infty$ (which becomes 0 after softmax). Take the 3rd position of the decoder sequence as an example, the \"masked\" softmax values would be $softmax\\left( \\frac{e_3 \\cdot e_1}{\\sqrt{D}}, \\frac{e_3 \\cdot e_2}{\\sqrt{D}}, \\frac{e_3 \\cdot e_3}{\\sqrt{D}}, -\\infty, \\ldots, -\\infty \\right)$.\n",
    "\n",
    "The second difference is that decoder sequence is allowed to attend to encoder sequence via a standard **cross-attention** mechanism (but not vice versa). Specifically, after all the transformations (across multiple blocks) applied to the input sequence, the encoder will emit a final sequence representation. In each decoder block, the embeddings are allowed to attend to all positions of this encoded squence. This is implemented in the same way as discussed in the \"Attention Mechanism\" lecture.\n",
    "\n",
    "The third difference is the output. The outputs of encoder are sequence embedding representations, whereas the outputs of decoder are probability predictions over vocabulary (to predict the next token). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature <a name=\"transformer_temperature\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During decoding (also referred to as \"inference\"), **temperature** ($T$) is an important parameter that controls how deterministic vs. random the generation process is. Roughly speaking, lower / higher temperature typically leads to more deterministic / random output. Specifically,\n",
    "\n",
    "- When $T = 0$, the decoder will simply output the token with the highest predicted probability. This is completely deterministic;\n",
    "- When $T > 0$, the decoder will sample from the predicted distribution over tokens, thereby producing non-determnistic outputs. There are a few ways to sample:\n",
    "    - _Top-k_: select the $k$ token that receive the highest predicted logit values (i.e., value before softmax transformation), then apply the softmax only on those $k$ tokens to obtain sampling probabilities;\n",
    "    - _Nucleus Sampling_: select the tokens whose cumulative probability exceeds a threshold $p$, then apply the softmax on the logits of these tokens to obtain sampling probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Implementation: A Step-by-Step Explanation <a name=\"implementation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we build a transformer model for a simple task. The goal is to understand the implementation of different components in a transformer, and how they are put together. The demonstration seeks to be provide step-by-step explanations to the transformer architecture.\n",
    "\n",
    "The **task** is to predict a numerical output based on a number of input features. Treating a number as a sequence of digits (where each digit is a token), then this task is essentially a sequence-to-sequence prediction task. Even though such \"numeric prediction\" task is typically not where transformers are applied, it does offer several advantages as a tutorial / demonstration: (1) the vocabulary is very restricted (all 10 single digits + blank space) and (2) each input can be represented as a fixed length sequence, thereby removing the need for padding / masking.\n",
    "\n",
    "We will use ```pytorch``` for this demonstration, because it offers a off-the-shelf ```transformer``` module. Its documentation is available on [this page](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step, let's simulate the data used for training and evaluation:\n",
    "- $X_1$, ..., $X_{10}$: 10 numerical input features, each randomly sampled from a uniform distribution.\n",
    "- $Y = \\frac{1}{10} \\sum_i X_i$: the numerical output is simply the average value.\n",
    "- $N=5000$: 5000 samples, 4000 for training and 1000 for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 10) (4000,) (1000, 10) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# set random seed for reproducibility\n",
    "np.random.seed(123)\n",
    "X = np.random.uniform(size = (5000, 10))\n",
    "Y = np.mean(X, axis = 1)\n",
    "X_train = X[:4000]\n",
    "X_test = X[4000:]\n",
    "Y_train = Y[:4000]\n",
    "Y_test = Y[4000:]\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, ```pytorch``` does not take these raw values / arrays as input. We need to tokenize them and convert them into indices in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab has single-digits, space, start, end\n",
    "VOCAB = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' ', 's', 'e']\n",
    "# for simplicity, we restrict each input/output number to 8 digits\n",
    "MAX_DIGITS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With such a restricted vocabulary, tokenizing each number is the same as splitting it into a sequence of single digits. Note that, because both inputs and outputs take value between 0 and 1, every number starts with \"0.\" (followed by 8 decimal digits). Therefore, as a further simplification, we don't need to keep track of the \"0.\" for each number.\n",
    "\n",
    "The following ```CustomDataset``` class performs basic processing and tokenization of input features and output values. It will allow us to convert the raw numpy arrays ```X``` and ```Y``` into a format that can be ingested by the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, Y, vocab):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.vocab = vocab\n",
    "        # the \"index\" method is defined below\n",
    "        self.X_indexed = self.index(X, 'X')\n",
    "        self.Y_indexed = self.index(Y, 'Y')\n",
    "\n",
    "    # The \"index\" method converts either an input vector or an output value to a sequence of token indices\n",
    "    def index(self, data, type):\n",
    "        data_indexed = []\n",
    "        for row in data:\n",
    "            if type == 'Y':\n",
    "                # in this case, row is a scalar, we convert it to a string and remove the \"0.\" prefix\n",
    "                # the '{:8f}'.format(...) part ensures the number has 8 digits after the decimal point, and converts it to a string\n",
    "                # the '[2:]' part removes the \"0.\" prefix\n",
    "                row_str = '{:.8f}'.format(row)[2:]\n",
    "            if type == 'X':\n",
    "                # in this case, we do the same processing to each feature value, then concatenate them to a longer sequence, separated by blank spaces\n",
    "                row_str = ' '.join(['{:.8f}'.format(x)[2:] for x in row])\n",
    "            # also need to prepend 's' and append 'e' to the sequence\n",
    "            row_str = 's' + row_str + 'e'\n",
    "            # convert to indices in vocabulary\n",
    "            row_idx = [self.vocab.index(c) for c in row_str]\n",
    "            data_indexed.append(row_idx)\n",
    "        return np.array(data_indexed)\n",
    "\n",
    "    def __len__(self):\n",
    "        # this is a required method in custom dataset classes, it should return size of data (i.e., number of rows)\n",
    "        return len(self.X_indexed)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # this is also a required method, it should return the item at the given index\n",
    "        src = torch.tensor(self.X_indexed[idx], dtype=torch.long)\n",
    "        tgt = torch.tensor(self.Y_indexed[idx], dtype=torch.long)\n",
    "        return src, tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create the datasets that can be used for training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 1000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(X_train, Y_train, VOCAB)\n",
    "test_dataset = CustomDataset(X_test, Y_test, VOCAB)\n",
    "print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also print out the first data point to see (remember the values you see are indices in the vocabulary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw inputs: [0.69646919 0.28613933 0.22685145 0.55131477 0.71946897 0.42310646\n",
      " 0.9807642  0.68482974 0.4809319  0.39211752]\n",
      "raw output: 0.544199352975335\n",
      "tokenized input sequence: tensor([11,  6,  9,  6,  4,  6,  9,  1,  9, 10,  2,  8,  6,  1,  3,  9,  3,  3,\n",
      "        10,  2,  2,  6,  8,  5,  1,  4,  5, 10,  5,  5,  1,  3,  1,  4,  7,  7,\n",
      "        10,  7,  1,  9,  4,  6,  8,  9,  7, 10,  4,  2,  3,  1,  0,  6,  4,  6,\n",
      "        10,  9,  8,  0,  7,  6,  4,  2,  0, 10,  6,  8,  4,  8,  2,  9,  7,  4,\n",
      "        10,  4,  8,  0,  9,  3,  1,  9,  0, 10,  3,  9,  2,  1,  1,  7,  5,  2,\n",
      "        12])\n",
      "tokenized output sequence: tensor([11,  5,  4,  4,  1,  9,  9,  3,  5, 12])\n"
     ]
    }
   ],
   "source": [
    "print(\"raw inputs:\", X_train[0])\n",
    "print(\"raw output:\", Y_train[0])\n",
    "print(\"tokenized input sequence:\", train_dataset[0][0])\n",
    "print(\"tokenized output sequence:\", train_dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to construct the transformer model. This include several modules:\n",
    "- A ```TokenEmbedding``` class that projects each token to its (trainable) embedding representation;\n",
    "- A ```PositionalEncoding``` class that adds the positional encoding to the token embeddings;\n",
    "- A ```Seq2SeqTransformer``` that implements the actual transformer architecture.\n",
    "\n",
    "We will do them one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        \"\"\"\n",
    "        :param vocab_size: the size of the vocabulary\n",
    "        :param d_model: the embedding dimension\n",
    "        \"\"\"\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: the input tensor with shape (batch_size, seq_len)\n",
    "        :return: the tensor after token embedding with shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        return self.embedding(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 91, 512])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see for yourself: if you apply the TokenEmbedding module to the first input sequence in the training set, you should get a tensor of shape (1, seq_len, d_model)\n",
    "# unsqueeze(0) here adds a batch dimension, so the input tensor conform to the (batch_size, seq_len) shape\n",
    "test_input = train_dataset[0][0].unsqueeze(0)\n",
    "test_emb = TokenEmbedding(len(VOCAB), 512)(test_input)\n",
    "test_emb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        \"\"\"\n",
    "        :param d_model: the embedding dimension\n",
    "        :param max_len: the maximum length of the sentence\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # setting max_len to 100 here, because the largest input sequence is 91 tokens long (10 * 8 digits + 9 spaces + 1 start + 1 end), so 100 is enough\n",
    "        # intialize the positional encoding, pe.shape = (max_len, d_model)        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # generate a tensor of shape (max_len, 1), with values from 0 to max_len - 1, to represent all unique positions\n",
    "        # the unsqueeze(1) operation adds a dimension after the first dimension, so the shape changes from (max_len,) to (max_len, 1)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # calculate scaling factors for each dimension of the positional encoding, see the formula in the first section of this notebook\n",
    "        scaling_factors = torch.tensor([10000.0 ** (-2 * i / d_model) for i in range(d_model // 2)])\n",
    "        # now populate the positional encoding tensor with values, even indices use sine functions, odd indices use cosine functions\n",
    "        pe[:, 0::2] = torch.sin(position * scaling_factors)  # pe[:, 0::2].shape = (max_len, d_model/2)\n",
    "        pe[:, 1::2] = torch.cos(position * scaling_factors)  # pe[:, 1::2].shape = (max_len, d_model/2)\n",
    "        # add a batch dimension to the positional encoding tensor so that it's compatible with the input tensor. pe.shape = (1, max_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # register the positional encoding tensor as a buffer, so that it will be stored as part of the model's \"states\" and won't be updated during training\n",
    "        # this is desirable because we don't want the positional encoding to be trained, we want it to be fixed\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: the input tensor with shape (batch_size, seq_len, d_model)\n",
    "        :return: the tensor after adding positional encoding with shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # for a given input tensor x, add the positional encoding to it\n",
    "        # x.size(1) gets the second dimensions of x, which is dimension that contains the element indices in the sequence\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 91, 512])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see for yourself:\n",
    "test_emb_with_pe = PositionalEncoding(512)(test_emb)\n",
    "test_emb_with_pe.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have the actual ```Seq2SeqTransformer``` module. Things like multi-head attention, feed-foward layers, layer normalziation, and residual connections are all encapsulated in pytorch's ```Transformer``` module, which makes it very straightforward to build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, vocab_size):\n",
    "        \"\"\"\n",
    "        :param d_model: the embedding dimension\n",
    "        :param nhead: the number of heads in multi-head attention\n",
    "        :param num_encoder_layers: the number of blocks in the encoder\n",
    "        :param num_decoder_layers: the number of blocks in the decoder\n",
    "        :param dim_feedforward: the dimension of the feedforward network\n",
    "        \"\"\"\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        # note that, in many other tasks (e.g., translation), you need two different token embeddings for the source and target languages\n",
    "        # here, however, because both input and output use the same vocabulary, we can use the same token embedding for both\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        # the transformer model is constructed with the Transformer module, which takes care of all the details\n",
    "        # the batch_first=True argument means the input and output tensors are of shape (batch_size, seq_len, d_model)\n",
    "        self.transformer = Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, batch_first=True)\n",
    "        # the generator is a simple linear layer that projects the transformer output to the vocabulary size\n",
    "        # it generates the logits for each token in the vocabulary, will be used for computing loss and making predictions\n",
    "        self.generator = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        :param src: the sequence to the encoder (required). with shape (batch_size, seq_len, d_model)\n",
    "        :param tgt: the sequence to the decoder (required). with shape (batch_size, seq_len, d_model)\n",
    "        :param src_mask: the additive mask for the src sequence (optional). with shape (batch_size, seq_len, seq_len)\n",
    "        :param tgt_mask: the additive mask for the tgt sequence (optional). with shape (batch_size, seq_len, seq_len)\n",
    "        :param src_padding_mask: the additive mask for the src sequence (optional). with shape (batch_size, 1, seq_len)\n",
    "        :param tgt_padding_mask: the additive mask for the tgt sequence (optional). with shape (batch_size, 1, seq_len)\n",
    "        :param memory_key_padding_mask: the additive mask for the encoder output (optional). with shape (batch_size, 1, seq_len)\n",
    "        :return: the decoder output tensor with shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # separately embed the source and target sequences\n",
    "        src_emb = self.positional_encoding(self.tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tok_emb(tgt))\n",
    "        # Important: we don't need any masks for source sequence, or any padding masks, nor do we need a mask for decoder attending to the encoder\n",
    "        # but we do need a mask for the target sequence -- this is a \"causal mask\", which prevents the decoder from attending to subsequent tokens during training\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1))\n",
    "        outs = self.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask)\n",
    "        return self.generator(outs)\n",
    "    \n",
    "    # The transformer also have an encode method and a decode method\n",
    "    # the encode method takes the source sequence and produce the context vector (which pytorch calls \"memory\")\n",
    "    # the decoder method takes the target sequence and the context vector, and produce the output sequence\n",
    "    def encode(self, src):\n",
    "        \"\"\"\n",
    "        :param src: the sequence to the encoder (required). with shape (batch_size, seq_len, d_model)\n",
    "        :return: the encoder output tensor with shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        return self.transformer.encoder(self.positional_encoding(self.tok_emb(src)))\n",
    "    \n",
    "    def decode(self, tgt, memory):\n",
    "        \"\"\"\n",
    "        :param tgt: the sequence to the decoder (required). with shape (batch_size, seq_len, d_model)\n",
    "        :param memory: the sequence from the last layer of the encoder (required). with shape (batch_size, seq_len, d_model)\n",
    "        :return: the decoder output tensor with shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        return self.transformer.decoder(self.positional_encoding(self.tok_emb(tgt)), memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start the actual training and evaluation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify model parameters and training parameters\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "EMB_SIZE = 256\n",
    "NHEAD = 4\n",
    "FFN_HID_DIM = 128\n",
    "BATCH_SIZE = 32\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "NUM_EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "model = Seq2SeqTransformer(EMB_SIZE, NHEAD, NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, FFN_HID_DIM, VOCAB_SIZE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batching\n",
    "# for eval_loader, we load data one at a time for better demonstration of what happens -- in practice you can also batch it\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "eval_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # start model training\n",
    "    model.train()\n",
    "    # initialize total loss for the epoch\n",
    "    total_loss = 0\n",
    "    for src, tgt in train_loader:\n",
    "        optimizer.zero_grad()        \n",
    "        # Separate the input and target sequences for teacher forcing\n",
    "        # tgt_input has everything except the last token\n",
    "        # tgt_output has everything except the first token\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        # Forward pass with teacher forcing, logits has shape (batch_size, seq_len, vocab_size)\n",
    "        logits = model(src, tgt_input)\n",
    "        # Calculate loss. The .reshape(-1) flattens the logits to (batch_size * seq_len, vocab_size)\n",
    "        outputs = logits.reshape(-1, logits.shape[-1])\n",
    "        # also flatten the ground truth outputs to shape (batch_size * seq_len)\n",
    "        tgt_out = tgt_output.reshape(-1)\n",
    "        loss = criterion(outputs, tgt_out)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch}, Training Loss: {total_loss}\")\n",
    "    \n",
    "    # monitor loss test set\n",
    "    model.eval()\n",
    "    test_loss = 0      \n",
    "    with torch.no_grad():\n",
    "        for src, tgt in eval_loader:\n",
    "            encoder_output = model.encode(src)\n",
    "            # decoding starts with the \"start\" token\n",
    "            tgt_idx = [VOCAB.index('s')]\n",
    "            pred_num = '0.'\n",
    "            for i in range(MAX_DIGITS):\n",
    "                # prepare the input tensor for the decoder, adding the batch dimension\n",
    "                decoder_input = torch.LongTensor(tgt_idx).unsqueeze(0)\n",
    "                # the decoder output has shape (1, seq_len, d_model) and the last position in sequence is the prediction for next token\n",
    "                decoder_output = model.decode(decoder_input, encoder_output)\n",
    "                # the predicted logits has shape (1, seq_len, vocab_size)\n",
    "                logits = model.generator(decoder_output)\n",
    "                # calculate test loss based on most recent token prediction, that is logits[:, -1, :]\n",
    "                test_loss += criterion(logits[:, -1, :], tgt[0][i].unsqueeze(0)).item()\n",
    "                # the actual predicted token is the one with highest logit score\n",
    "                # here, .argmax(2) makes sure the max is taken on the last dimension, which is the vocabulary dimension, and [:, -1] makes sure that we are looking at the last position in the sequence\n",
    "                pred_token = logits.argmax(2)[:,-1].item()\n",
    "                # append the predicted token to target sequence as you go\n",
    "                tgt_idx.append(pred_token)\n",
    "                pred_num += VOCAB[pred_token]\n",
    "                if pred_token == VOCAB.index('e'):\n",
    "                    break            \n",
    "            # Convert the predicted sequence to a number - if you want, you can use it to compute other metrics such as RMSE\n",
    "            try:\n",
    "                pred_num = float(pred_num)  # Convert the accumulated string to a float\n",
    "            except ValueError:\n",
    "                pred_num = 0.0  # Handle any conversion errors gracefully\n",
    "    print(\"Test Loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have also put the entire pipeline into a separate script ```Transformer.py``` under the ```pytorch``` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources <a name=\"resource\"></a>\n",
    "\n",
    "- Original research paper that proposed the transformer architecture: [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf);\n",
    "- Original paper on self-attention: [Long Short-Term Memory-Networks for Machine Reading](https://arxiv.org/pdf/1601.06733.pdf);\n",
    "- Additional articles to learn about self-attention: [Illustrated: Self-Attention](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a), [Introduction of Self-Attention Layer in Transformer](https://medium.com/lsc-psd/introduction-of-self-attention-layer-in-transformer-fc7bff63f3bc);\n",
    "- Additional articles on other components in a transformer: [Layer Normalization](https://arxiv.org/abs/1607.06450), [Normalization Techniques in Deep Neural Networks](https:/medium.com/techspace-usict/normalization-techniques-in-deep-neural-networks-9121bf100d8), [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385);\n",
    "- Implementation of Transformer: [Transformer model for language understanding](https://www.tensorflow.org/tutorials/text/transformer);\n",
    "- [Transformer for text classification](https://keras.io/examples/nlp/text_classification_with_transformer/)\n",
    "- Andrej Karpathy's [YouTube Tutorial](https://www.youtube.com/watch?v=kCc8FmEb1nY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
