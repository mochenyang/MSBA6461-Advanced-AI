{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> MSBA 6461: Advanced AI for Business Applications </center>\n",
    "<center> Spring 2024, Mochen Yang </center>\n",
    "\n",
    "## <center> Recurrent Neural Network (RNN) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Setup](#setup)\n",
    "1. [Basic RNN Model](#basic_rnn)\n",
    "    - [Why Use RNN for Language-Related Machine Learning Tasks?](#basic_rnn_motivation)\n",
    "    - [Common RNN Architectures for NLP](#basic_rnn_architecture)\n",
    "    - [Animated Illustration of a Simple RNN Unit](#basic_rnn_figure)\n",
    "    - [How does a Simple RNN Unit work?](#basic_rnn_tech)\n",
    "    - [Build Simple RNN in Keras](#basic_rnn_implement)\n",
    "1. [The Long-Term Dependency Problem](#dependency)\n",
    "1. [LSTM Model](#lstm)\n",
    "    - [Animated Illustration of a Single LSTM Unit](#lstm_figure)\n",
    "    - [How does a Single LSTM Unit/Cell Work?](#lstm_tech)\n",
    "    - [Build RNN with LSTM Units in Keras](#lstm_implementation)\n",
    "1. [GRU Model](#gru)\n",
    "    - [Animated Illustration of a Single GRU](#gru_figure)\n",
    "    - [How does a Single GRU Work?](#gru_tech)\n",
    "    - [Build RNN with GRUs in Keras](#gru_implementation)\n",
    "1. [Bidirectional RNN Models](#birnn)\n",
    "    - [Building Bidirectional RNN Model in Keras](#birnn_implementation)\n",
    "1. [Additional Resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: Import Data and Preprocess Text <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a [sentiment classification dataset on UCI](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences). Download this dataset and import it. Create two numpy arrays to store the texts and labels separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "label = []\n",
    "for line in open(\"datasets/sentiment.txt\"):\n",
    "    line = line.rstrip('\\n').split('\\t')\n",
    "    text.append(line[0])\n",
    "    label.append(int(line[1]))\n",
    "text = np.array(text)\n",
    "label = np.array(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the `TextVectorization()` function in `keras` to complete basic text processing tasks. See the function documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization). In particular, you can control the following things:\n",
    "- tokenization (split)\n",
    "- lowercasing and remove puctuation (standardize)\n",
    "- optionally generate ngrams\n",
    "- turn into integer representation (output_mode = 'int')\n",
    "- whether to pad texts of different length to the same sequence length (output_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens = None,\n",
    "    standardize = 'lower_and_strip_punctuation',\n",
    "    split = 'whitespace',\n",
    "    ngrams = None,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply it to the text data with \"adapt\"\n",
    "vectorize_layer.adapt(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'and',\n",
       " 'i',\n",
       " 'a',\n",
       " 'is',\n",
       " 'to',\n",
       " 'it',\n",
       " 'this',\n",
       " 'of',\n",
       " 'was',\n",
       " 'in',\n",
       " 'for',\n",
       " 'not',\n",
       " 'that',\n",
       " 'with',\n",
       " 'my',\n",
       " 'very',\n",
       " 'good',\n",
       " 'on',\n",
       " 'great',\n",
       " 'you',\n",
       " 'but',\n",
       " 'have',\n",
       " 'are',\n",
       " 'movie',\n",
       " 'as',\n",
       " 'so',\n",
       " 'phone',\n",
       " 'film',\n",
       " 'its',\n",
       " 'be',\n",
       " 'all',\n",
       " 'one',\n",
       " 'had',\n",
       " 'at',\n",
       " 'food',\n",
       " 'like',\n",
       " 'just',\n",
       " 'place',\n",
       " 'time',\n",
       " 'were',\n",
       " 'service',\n",
       " 'an',\n",
       " 'really',\n",
       " 'if',\n",
       " 'from',\n",
       " 'there',\n",
       " 'they',\n",
       " 'bad',\n",
       " 'we',\n",
       " 'well',\n",
       " 'out',\n",
       " 'has',\n",
       " 'dont',\n",
       " 'about',\n",
       " 'would',\n",
       " 'your',\n",
       " 'or',\n",
       " 'no',\n",
       " 'only',\n",
       " 'by',\n",
       " 'best',\n",
       " 'ever',\n",
       " 'even',\n",
       " 'here',\n",
       " 'also',\n",
       " 'will',\n",
       " 'back',\n",
       " 'up',\n",
       " 'when',\n",
       " 'me',\n",
       " 'than',\n",
       " 'more',\n",
       " 'quality',\n",
       " 'go',\n",
       " 'what',\n",
       " 'love',\n",
       " 'ive',\n",
       " 'which',\n",
       " 'made',\n",
       " 'he',\n",
       " 'can',\n",
       " 'because',\n",
       " 'product',\n",
       " 'im',\n",
       " 'how',\n",
       " 'too',\n",
       " 'get',\n",
       " 'work',\n",
       " 'their',\n",
       " 'some',\n",
       " 'works',\n",
       " 'nice',\n",
       " 'could',\n",
       " 'better',\n",
       " 'any',\n",
       " 'excellent',\n",
       " 'after',\n",
       " 'never',\n",
       " 'do',\n",
       " 'recommend',\n",
       " 'much',\n",
       " 'been',\n",
       " 'who',\n",
       " 'use',\n",
       " 'our',\n",
       " 'did',\n",
       " 'again',\n",
       " 'sound',\n",
       " 'other',\n",
       " 'think',\n",
       " 'his',\n",
       " 'headset',\n",
       " 'first',\n",
       " 'battery',\n",
       " 'way',\n",
       " 'them',\n",
       " 'see',\n",
       " 'make',\n",
       " 'didnt',\n",
       " 'pretty',\n",
       " 'acting',\n",
       " 'most',\n",
       " 'worst',\n",
       " 'still',\n",
       " 'now',\n",
       " 'got',\n",
       " 'does',\n",
       " 'say',\n",
       " 'over',\n",
       " 'enough',\n",
       " 'characters',\n",
       " 'two',\n",
       " 'little',\n",
       " 'everything',\n",
       " 'every',\n",
       " 'ear',\n",
       " 'disappointed',\n",
       " 'am',\n",
       " 'thing',\n",
       " 'then',\n",
       " 'price',\n",
       " 'being',\n",
       " 'waste',\n",
       " 'these',\n",
       " 'right',\n",
       " 'people',\n",
       " 'going',\n",
       " '2',\n",
       " 'terrible',\n",
       " 'real',\n",
       " 'off',\n",
       " 'minutes',\n",
       " 'definitely',\n",
       " 'case',\n",
       " 'amazing',\n",
       " 'movies',\n",
       " 'money',\n",
       " 'look',\n",
       " 'new',\n",
       " 'know',\n",
       " 'experience',\n",
       " 'cant',\n",
       " 'came',\n",
       " 'both',\n",
       " 'into',\n",
       " 'wont',\n",
       " 'story',\n",
       " 'many',\n",
       " 'her',\n",
       " 'friendly',\n",
       " 'few',\n",
       " 'doesnt',\n",
       " 'worth',\n",
       " 'used',\n",
       " 'poor',\n",
       " 'plot',\n",
       " 'piece',\n",
       " 'films',\n",
       " 'far',\n",
       " 'us',\n",
       " 'seen',\n",
       " 'years',\n",
       " 'wonderful',\n",
       " 'while',\n",
       " 'want',\n",
       " 'restaurant',\n",
       " 'quite',\n",
       " 'nothing',\n",
       " 'lot',\n",
       " 'long',\n",
       " '10',\n",
       " 'she',\n",
       " 'script',\n",
       " 'life',\n",
       " 'happy',\n",
       " 'always',\n",
       " 'wasnt',\n",
       " 'highly',\n",
       " 'give',\n",
       " 'found',\n",
       " 'delicious',\n",
       " 'anyone',\n",
       " 'watching',\n",
       " 'times',\n",
       " 'character',\n",
       " 'worked',\n",
       " 'vegas',\n",
       " 'take',\n",
       " 'should',\n",
       " 'probably',\n",
       " 'loved',\n",
       " 'fine',\n",
       " 'easy',\n",
       " 'car',\n",
       " 'bought',\n",
       " 'awful',\n",
       " 'around',\n",
       " 'another',\n",
       " 'absolutely',\n",
       " 'went',\n",
       " 'since',\n",
       " 'screen',\n",
       " 'same',\n",
       " 'item',\n",
       " 'however',\n",
       " 'horrible',\n",
       " 'funny',\n",
       " 'comfortable',\n",
       " 'camera',\n",
       " 'buy',\n",
       " 'before',\n",
       " 'awesome',\n",
       " 'where',\n",
       " 'watch',\n",
       " 'totally',\n",
       " 'thought',\n",
       " 'those',\n",
       " 'things',\n",
       " 'stars',\n",
       " 'staff',\n",
       " 'scenes',\n",
       " 'makes',\n",
       " 'impressed',\n",
       " 'find',\n",
       " 'eat',\n",
       " 'down',\n",
       " 'couldnt',\n",
       " 'cool',\n",
       " 'charger',\n",
       " 'big',\n",
       " 'though',\n",
       " 'talk',\n",
       " 'such',\n",
       " 'small',\n",
       " 'slow',\n",
       " 'show',\n",
       " 'part',\n",
       " 'night',\n",
       " 'music',\n",
       " 'last',\n",
       " 'job',\n",
       " 'fantastic',\n",
       " 'end',\n",
       " 'come',\n",
       " 'cast',\n",
       " 'actors',\n",
       " 'stupid',\n",
       " 'said',\n",
       " 'perfect',\n",
       " 'ordered',\n",
       " 'old',\n",
       " 'must',\n",
       " 'kind',\n",
       " 'family',\n",
       " 'day',\n",
       " 'cheap',\n",
       " 'bluetooth',\n",
       " 'black',\n",
       " 'beautiful',\n",
       " 'thats',\n",
       " 'sure',\n",
       " 'performance',\n",
       " 'overall',\n",
       " 'next',\n",
       " 'fresh',\n",
       " 'feel',\n",
       " 'chicken',\n",
       " 'avoid',\n",
       " 'actually',\n",
       " '5',\n",
       " 'try',\n",
       " 'tried',\n",
       " 'sucks',\n",
       " 'simply',\n",
       " 'scene',\n",
       " 'salad',\n",
       " 'reception',\n",
       " 'problems',\n",
       " 'problem',\n",
       " 'pizza',\n",
       " 'order',\n",
       " 'menu',\n",
       " 'line',\n",
       " 'least',\n",
       " 'interesting',\n",
       " 'id',\n",
       " 'hard',\n",
       " 'felt',\n",
       " 'especially',\n",
       " 'done',\n",
       " 'bit',\n",
       " 'between',\n",
       " 'writing',\n",
       " 'worse',\n",
       " 'without',\n",
       " 'wait',\n",
       " 'taste',\n",
       " 'steak',\n",
       " 'special',\n",
       " 'purchase',\n",
       " 'man',\n",
       " 'low',\n",
       " 'looks',\n",
       " 'liked',\n",
       " 'ill',\n",
       " 'hear',\n",
       " 'gets',\n",
       " 'fit',\n",
       " 'fast',\n",
       " 'expect',\n",
       " 'everyone',\n",
       " 'enjoyed',\n",
       " 'either',\n",
       " 'each',\n",
       " 'customer',\n",
       " 'completely',\n",
       " 'coming',\n",
       " 'charge',\n",
       " 'cell',\n",
       " 'calls',\n",
       " 'away',\n",
       " 'anything',\n",
       " 'almost',\n",
       " '1',\n",
       " 'year',\n",
       " 'working',\n",
       " 'whole',\n",
       " 'white',\n",
       " 'using',\n",
       " 'through',\n",
       " 'sushi',\n",
       " 'short',\n",
       " 'server',\n",
       " 'seriously',\n",
       " 'rather',\n",
       " 'left',\n",
       " 'hour',\n",
       " 'flavor',\n",
       " 'different',\n",
       " 'dialogue',\n",
       " 'device',\n",
       " 'clear',\n",
       " 'call',\n",
       " 'bland',\n",
       " 'watched',\n",
       " 'volume',\n",
       " 'unfortunately',\n",
       " 'understand',\n",
       " 'took',\n",
       " 'tell',\n",
       " 'super',\n",
       " 'started',\n",
       " 'side',\n",
       " 'several',\n",
       " 'saw',\n",
       " 'return',\n",
       " 'put',\n",
       " 'plug',\n",
       " 'play',\n",
       " 'perfectly',\n",
       " 'may',\n",
       " 'looking',\n",
       " 'incredible',\n",
       " 'having',\n",
       " 'getting',\n",
       " 'full',\n",
       " 'feeling',\n",
       " 'fact',\n",
       " 'extremely',\n",
       " 'enjoy',\n",
       " 'disappointing',\n",
       " 'design',\n",
       " 'deal',\n",
       " 'burger',\n",
       " 'buffet',\n",
       " 'believe',\n",
       " 'art',\n",
       " 'yet',\n",
       " 'truly',\n",
       " 'three',\n",
       " 'theres',\n",
       " 'tasty',\n",
       " 'sucked',\n",
       " 'stay',\n",
       " 'soon',\n",
       " 'received',\n",
       " 'phones',\n",
       " 'once',\n",
       " 'need',\n",
       " 'motorola',\n",
       " 'mess',\n",
       " 'mediocre',\n",
       " 'meal',\n",
       " 'light',\n",
       " 'less',\n",
       " 'kids',\n",
       " 'huge',\n",
       " 'hours',\n",
       " 'hot',\n",
       " 'fits',\n",
       " 'ending',\n",
       " 'during',\n",
       " 'disappointment',\n",
       " 'crap',\n",
       " 'certainly',\n",
       " 'care',\n",
       " 'barely',\n",
       " 'atmosphere',\n",
       " '3',\n",
       " 'wrong',\n",
       " 'why',\n",
       " 'wasted',\n",
       " 'waited',\n",
       " 'together',\n",
       " 'strong',\n",
       " 'selection',\n",
       " 'sauce',\n",
       " 'recommended',\n",
       " 'prices',\n",
       " 'predictable',\n",
       " 'pleased',\n",
       " 'played',\n",
       " 'picture',\n",
       " 'own',\n",
       " 'original',\n",
       " 'none',\n",
       " 'months',\n",
       " 'kept',\n",
       " 'inside',\n",
       " 'home',\n",
       " 'high',\n",
       " 'gave',\n",
       " 'fun',\n",
       " 'friends',\n",
       " 'effects',\n",
       " 'easily',\n",
       " 'director',\n",
       " 'days',\n",
       " 'cold',\n",
       " 'boring',\n",
       " 'area',\n",
       " 'actor',\n",
       " 'wouldnt',\n",
       " 'wear',\n",
       " 'wanted',\n",
       " 'waitress',\n",
       " 'voice',\n",
       " 'turn',\n",
       " 'trying',\n",
       " 'top',\n",
       " 'table',\n",
       " 'style',\n",
       " 'spot',\n",
       " 'something',\n",
       " 'series',\n",
       " 'seems',\n",
       " 'second',\n",
       " 'sandwich',\n",
       " 'places',\n",
       " 'myself',\n",
       " 'meat',\n",
       " 'lunch',\n",
       " 'lost',\n",
       " 'keep',\n",
       " 'junk',\n",
       " 'him',\n",
       " 'helpful',\n",
       " 'hands',\n",
       " 'half',\n",
       " 'guess',\n",
       " 'glad',\n",
       " 'game',\n",
       " 'fries',\n",
       " 'face',\n",
       " 'dropped',\n",
       " 'drama',\n",
       " 'dishes',\n",
       " 'directing',\n",
       " 'decent',\n",
       " 'couple',\n",
       " 'company',\n",
       " 'clean',\n",
       " 'cannot',\n",
       " 'broke',\n",
       " 'breakfast',\n",
       " 'bar',\n",
       " 'amazon',\n",
       " 'ago',\n",
       " 'weak',\n",
       " 'waiting',\n",
       " 'verizon',\n",
       " 'value',\n",
       " 'unit',\n",
       " 'under',\n",
       " 'twice',\n",
       " 'tv',\n",
       " 'told',\n",
       " 'today',\n",
       " 'tasted',\n",
       " 'star',\n",
       " 'spicy',\n",
       " 'someone',\n",
       " 'simple',\n",
       " 'set',\n",
       " 'rude',\n",
       " 'quickly',\n",
       " 'quick',\n",
       " 'point',\n",
       " 'plus',\n",
       " 'playing',\n",
       " 'pictures',\n",
       " 'perhaps',\n",
       " 'pay',\n",
       " 'overpriced',\n",
       " 'others',\n",
       " 'oh',\n",
       " 'obviously',\n",
       " 'let',\n",
       " 'jabra',\n",
       " 'itself',\n",
       " 'important',\n",
       " 'human',\n",
       " 'house',\n",
       " 'headsets',\n",
       " 'hate',\n",
       " 'given',\n",
       " 'garbage',\n",
       " 'finally',\n",
       " 'fails',\n",
       " 'expected',\n",
       " 'entire',\n",
       " 'else',\n",
       " 'eating',\n",
       " 'dish',\n",
       " 'dining',\n",
       " 'comes',\n",
       " 'cinematography',\n",
       " 'cinema',\n",
       " 'check',\n",
       " 'buttons',\n",
       " 'beer',\n",
       " 'average',\n",
       " 'audio',\n",
       " 'ask',\n",
       " 'arrived',\n",
       " 'amount',\n",
       " 'although',\n",
       " '20',\n",
       " 'written',\n",
       " 'world',\n",
       " 'word',\n",
       " 'within',\n",
       " 'whatever',\n",
       " 'week',\n",
       " 'wall',\n",
       " 'waiter',\n",
       " 'useless',\n",
       " 'town',\n",
       " 'tender',\n",
       " 'suspense',\n",
       " 'superb',\n",
       " 'subtle',\n",
       " 'store',\n",
       " 'solid',\n",
       " 'software',\n",
       " 'single',\n",
       " 'sick',\n",
       " 'served',\n",
       " 'room',\n",
       " 'review',\n",
       " 'reasonable',\n",
       " 'priced',\n",
       " 'possible',\n",
       " 'outside',\n",
       " 'nokia',\n",
       " 'needed',\n",
       " 'mostly',\n",
       " 'mind',\n",
       " 'might',\n",
       " 'mention',\n",
       " 'maybe',\n",
       " 'location',\n",
       " 'literally',\n",
       " 'lines',\n",
       " 'later',\n",
       " 'joy',\n",
       " 'john',\n",
       " 'isnt',\n",
       " 'horror',\n",
       " 'hope',\n",
       " 'heart',\n",
       " 'girl',\n",
       " 'front',\n",
       " 'free',\n",
       " 'feels',\n",
       " 'editing',\n",
       " 'drive',\n",
       " 'cover',\n",
       " 'course',\n",
       " 'cooked',\n",
       " 'comedy',\n",
       " 'color',\n",
       " 'classic',\n",
       " 'chips',\n",
       " 'charm',\n",
       " 'cable',\n",
       " 'bring',\n",
       " 'brilliant',\n",
       " 'beyond',\n",
       " 'authentic',\n",
       " 'attentive',\n",
       " 'ambiance',\n",
       " 'action',\n",
       " '30',\n",
       " '12',\n",
       " 'zero',\n",
       " 'youre',\n",
       " 'wow',\n",
       " 'wife',\n",
       " 'whether',\n",
       " 'whatsoever',\n",
       " 'warm',\n",
       " 'walked',\n",
       " 'until',\n",
       " 'unless',\n",
       " 'throughout',\n",
       " 'thin',\n",
       " 'theyre',\n",
       " 'terrific',\n",
       " 'sweet',\n",
       " 'songs',\n",
       " 'sometimes',\n",
       " 'signal',\n",
       " 'shrimp',\n",
       " 'setting',\n",
       " 'seemed',\n",
       " 'seem',\n",
       " 'seeing',\n",
       " 'sat',\n",
       " 'running',\n",
       " 'ridiculous',\n",
       " 'reviews',\n",
       " 'rest',\n",
       " 'rent',\n",
       " 'reason',\n",
       " 'rating',\n",
       " 'rare',\n",
       " 'range',\n",
       " 'potato',\n",
       " 'portrayal',\n",
       " 'poorly',\n",
       " 'player',\n",
       " 'plastic',\n",
       " 'pho',\n",
       " 'performances',\n",
       " 'particular',\n",
       " 'note',\n",
       " 'mistake',\n",
       " 'management',\n",
       " 'making',\n",
       " 'lovely',\n",
       " 'loud',\n",
       " 'lots',\n",
       " 'looked',\n",
       " 'longer',\n",
       " 'leave',\n",
       " 'large',\n",
       " 'lacks',\n",
       " 'lacking',\n",
       " 'joke',\n",
       " 'internet',\n",
       " 'instead',\n",
       " 'idea',\n",
       " 'holes',\n",
       " 'hold',\n",
       " 'hit',\n",
       " 'history',\n",
       " 'hilarious',\n",
       " 'headphones',\n",
       " 'happened',\n",
       " 'gives',\n",
       " 'flick',\n",
       " 'fish',\n",
       " 'features',\n",
       " 'feature',\n",
       " 'extra',\n",
       " 'exactly',\n",
       " 'elsewhere',\n",
       " 'ears',\n",
       " 'dinner',\n",
       " 'difficult',\n",
       " 'despite',\n",
       " 'dead',\n",
       " 'damn',\n",
       " 'costs',\n",
       " 'considering',\n",
       " 'clever',\n",
       " 'casting',\n",
       " 'cases',\n",
       " 'business',\n",
       " 'bother',\n",
       " 'book',\n",
       " 'believable',\n",
       " 'become',\n",
       " 'basically',\n",
       " 'bars',\n",
       " 'annoying',\n",
       " 'above',\n",
       " '\\x96',\n",
       " 'youll',\n",
       " 'youd',\n",
       " 'wish',\n",
       " 'weeks',\n",
       " 'water',\n",
       " 'utterly',\n",
       " 'usual',\n",
       " 'unbelievable',\n",
       " 'turned',\n",
       " 'trip',\n",
       " 'trash',\n",
       " 'total',\n",
       " 'tom',\n",
       " 'thumbs',\n",
       " 'themselves',\n",
       " 'thai',\n",
       " 'tasteless',\n",
       " 'tables',\n",
       " 'sturdy',\n",
       " 'stuff',\n",
       " 'soundtrack',\n",
       " 'silent',\n",
       " 'sides',\n",
       " 'shots',\n",
       " 'shot',\n",
       " 'shipping',\n",
       " 'servers',\n",
       " 'seated',\n",
       " 'seafood',\n",
       " 'satisfied',\n",
       " 'samsung',\n",
       " 'sad',\n",
       " 'roles',\n",
       " 'recently',\n",
       " 'recent',\n",
       " 'reasonably',\n",
       " 'reading',\n",
       " 'razr',\n",
       " 'rate',\n",
       " 'provided',\n",
       " 'portions',\n",
       " 'please',\n",
       " 'plain',\n",
       " 'period',\n",
       " 'pathetic',\n",
       " 'pasta',\n",
       " 'party',\n",
       " 'particularly',\n",
       " 'owners',\n",
       " 'owned',\n",
       " 'options',\n",
       " 'ok',\n",
       " 'often',\n",
       " 'offers',\n",
       " 'minute',\n",
       " 'memorable',\n",
       " 'mean',\n",
       " 'living',\n",
       " 'level',\n",
       " 'leather',\n",
       " 'lead',\n",
       " 'lame',\n",
       " 'lacked',\n",
       " 'involved',\n",
       " 'incredibly',\n",
       " 'impressive',\n",
       " 'imagination',\n",
       " 'ice',\n",
       " 'holds',\n",
       " 'hitchcock',\n",
       " 'havent',\n",
       " 'hand',\n",
       " 'graphics',\n",
       " 'goes',\n",
       " 'generally',\n",
       " 'games',\n",
       " 'fried',\n",
       " 'form',\n",
       " 'forever',\n",
       " 'favorite',\n",
       " 'fans',\n",
       " 'fan',\n",
       " 'fall',\n",
       " 'eyes',\n",
       " 'etc',\n",
       " 'ended',\n",
       " 'empty',\n",
       " 'embarrassing',\n",
       " 'eaten',\n",
       " 'dry',\n",
       " 'dirty',\n",
       " 'direction',\n",
       " 'dessert',\n",
       " 'deserves',\n",
       " 'data',\n",
       " 'cult',\n",
       " 'consider',\n",
       " 'connection',\n",
       " 'computer',\n",
       " 'close',\n",
       " 'clarity',\n",
       " 'choice',\n",
       " 'child',\n",
       " 'charging',\n",
       " 'buying',\n",
       " 'budget',\n",
       " 'break',\n",
       " 'bread',\n",
       " 'blue',\n",
       " 'belt',\n",
       " 'below',\n",
       " 'beef',\n",
       " 'beat',\n",
       " 'bargain',\n",
       " 'bacon',\n",
       " 'audience',\n",
       " 'asked',\n",
       " 'arent',\n",
       " 'anytime',\n",
       " 'along',\n",
       " 'ability',\n",
       " '8',\n",
       " '40',\n",
       " '4',\n",
       " 'yummy',\n",
       " 'yourself',\n",
       " 'young',\n",
       " 'yes',\n",
       " 'worthless',\n",
       " 'words',\n",
       " 'wonderfully',\n",
       " 'wings',\n",
       " 'wine',\n",
       " 'wind',\n",
       " 'weird',\n",
       " 'website',\n",
       " 'wasting',\n",
       " 'visual',\n",
       " 'visit',\n",
       " 'vibe',\n",
       " 'vegetables',\n",
       " 'unreliable',\n",
       " 'type',\n",
       " 'turns',\n",
       " 'true',\n",
       " 'trouble',\n",
       " 'treo',\n",
       " 'towards',\n",
       " 'touch',\n",
       " 'torture',\n",
       " 'tool',\n",
       " 'tmobile',\n",
       " 'thriller',\n",
       " 'thoroughly',\n",
       " 'third',\n",
       " 'thinking',\n",
       " 'theater',\n",
       " 'ten',\n",
       " 'tea',\n",
       " 'tale',\n",
       " 'takes',\n",
       " 'tacos',\n",
       " 'support',\n",
       " 'strip',\n",
       " 'storyline',\n",
       " 'stories',\n",
       " 'steaks',\n",
       " 'station',\n",
       " 'starts',\n",
       " 'space',\n",
       " 'soup',\n",
       " 'song',\n",
       " 'son',\n",
       " 'somewhat',\n",
       " 'shows',\n",
       " 'showed',\n",
       " 'share',\n",
       " 'sets',\n",
       " 'serious',\n",
       " 'sense',\n",
       " 'sending',\n",
       " 'seller',\n",
       " 'says',\n",
       " 'salmon',\n",
       " 'run',\n",
       " 'rolls',\n",
       " 'role',\n",
       " 'rocks',\n",
       " 'ringtones',\n",
       " 'rice',\n",
       " 'results',\n",
       " 'replace',\n",
       " 'remember',\n",
       " 'red',\n",
       " 'ready',\n",
       " 'ray',\n",
       " 'rated',\n",
       " 'purchased',\n",
       " 'pull',\n",
       " 'production',\n",
       " 'previous',\n",
       " 'pretentious',\n",
       " 'premise',\n",
       " 'power',\n",
       " 'pork',\n",
       " 'pleasant',\n",
       " 'plays',\n",
       " 'plantronics',\n",
       " 'phoenix',\n",
       " 'person',\n",
       " 'passed',\n",
       " 'parts',\n",
       " 'palm',\n",
       " 'pair',\n",
       " 'paid',\n",
       " 'ones',\n",
       " 'occasionally',\n",
       " 'number',\n",
       " 'network',\n",
       " 'needs',\n",
       " 'nearly',\n",
       " 'nasty',\n",
       " 'moving',\n",
       " 'mouth',\n",
       " 'missed',\n",
       " 'mic',\n",
       " 'mexican',\n",
       " 'massive',\n",
       " 'market',\n",
       " 'manager',\n",
       " 'main',\n",
       " 'loves',\n",
       " 'live',\n",
       " 'list',\n",
       " 'likes',\n",
       " 'lightweight',\n",
       " 'lg',\n",
       " 'lasts',\n",
       " 'killer',\n",
       " 'keyboard',\n",
       " 'italian',\n",
       " 'issues',\n",
       " 'intelligence',\n",
       " 'insult',\n",
       " 'instructions',\n",
       " 'indeed',\n",
       " 'included',\n",
       " 'immediately',\n",
       " 'husband',\n",
       " 'honestly',\n",
       " 'honest',\n",
       " 'hes',\n",
       " 'heard',\n",
       " 'hated',\n",
       " 'happier',\n",
       " 'hair',\n",
       " 'guy',\n",
       " 'greatest',\n",
       " 'gotten',\n",
       " 'gone',\n",
       " 'genuine',\n",
       " 'gem',\n",
       " 'fx',\n",
       " 'forget',\n",
       " 'follow',\n",
       " 'folks',\n",
       " 'flat',\n",
       " 'five',\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check preprocessing results, such as vocabulary, \n",
    "vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5404"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorize_layer.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the vocabulary contains two special tokens:\n",
    "- '' is called the **padding token**. It is an empty string, correspond to index 0, that can be used to pad texts of different lengths to the same sequence length. When this text vectorization layer is applied to a set of texts, it automatically perform padding (to the longest sequence).\n",
    "- '[UNK]' is called the **Out-of-Vocabulary token**. It is used to represent any word that does not appear in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code is for demonstration purpose only - it is typically NOT needed in actual model building. We want to use `vectorize_layer` to process some texts and represent them as indices in vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n",
       "array([[18, 19, 26],\n",
       "       [ 1,  1,  0]], dtype=int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now use it to process some text\n",
    "input_text = [['very good movie'], ['Mochen Yang']]\n",
    "vectorize_layer(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RNN Model <a name=\"basic_rnn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Use RNN for Language-Related Machine Learning Tasks? <a name=\"basic_rnn_motivation\"></a>\n",
    "\n",
    "1. Natural language is a sequence (of words). RNNs are specifically designed to handle sequential data;\n",
    "2. It is easy to use word embeddings as inputs to RNNs, further boosting the ability to incorporate semantic information;\n",
    "3. Different pieces of texts can have different lengths. RNNs are built to deal with variable-length data (via parameter sharing - discussed later);\n",
    "4. Different NLP tasks require different network architectures. RNNs are versatile enough to accommodate those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common RNN Architectures for NLP <a name=\"basic_rnn_architecture\"></a>\n",
    "\n",
    "### Many Inputs, One Output\n",
    "\n",
    "- **Suitable Task**: Classification and Numeric Prediction\n",
    "- **Typical Architecture** is discussed and demonstrated in this notebook.\n",
    "\n",
    "\n",
    "### Many Inputs, Many Outputs\n",
    "\n",
    "- **Suitable Tasks**:\n",
    "    - Machine Translation\n",
    "    - Document Summarization\n",
    "    - Conversational Model (Q&A, Chatbot, etc.)\n",
    "- **Typical Architecture**: encoder-decoder architecture (discussed in a different notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animated Illustration of a Simple RNN Unit <a name=\"basic_rnn_figure\"></a>\n",
    "\n",
    "![Inside of an RNN Unit](images/Basic_RNN.gif)\n",
    "\n",
    "image credit: https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does a Simple RNN Unit work? <a name=\"basic_rnn_tech\"></a>\n",
    "The inner workings of a simple RNN unit can be represented either as a recurrent equation:\n",
    "\n",
    "$$h_{t} = f(h_{t-1}, X_{t}, \\Theta)$$\n",
    "\n",
    "- $h_{t}$ is the hidden states at time step $t$;\n",
    "- $X_{t}$ is the input (usually a vector) at time step $t$;\n",
    "- $\\Theta$ is (usually matrices) of weights to be learned. Notice that $\\Theta$ does not have any subscript, which means that it is the same set of parameters for **every time step $t$**. <font color='blue'> This is the key idea of **parameter sharing**!</font>. Without parameter sharing, RNNs would not be able to handle texts of different lengths.\n",
    "- $f()$ is the activation function that governs how past states ($h_{t-1}$) combine with current information ($X_{t}$) to determine current states of the network. Typically this is the hyperbolic tangent $tanh$ function (see [here](https://en.wikipedia.org/wiki/Hyperbolic_functions#Exponential_definitions) for technical definition).\n",
    "\n",
    "**Importantly**, the recurrent relationship discussed here is a universal signature of RNNs. <font color='blue'> Even for more complex types of RNNs, we are still trying to characterize how the hidden states at time t-1, combined with input received at time t, jointly determine the hidden states at time t </font>. In the context of NLP, you can conceptually think of this recurrent relationship as a \"reading\" process: the neural network is reading a piece of text word by word, and its hidden states are \"updated\" after the ingestion of every word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Simple RNN in Keras <a name=\"basic_rnn_implement\"></a>\n",
    "\n",
    "Now, let's actually build a basic RNN model, by stacking together the text processing layer, an embedding layer, and an RNN layer. Documentation to RNN layer is [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = keras.Sequential()\n",
    "\n",
    "model_rnn.add(vectorize_layer)\n",
    "\n",
    "model_rnn.add(keras.layers.Embedding(\n",
    "    input_dim = len(vectorize_layer.get_vocabulary()),\n",
    "    output_dim = 64,\n",
    "    mask_zero = True\n",
    "))\n",
    "\n",
    "model_rnn.add(keras.layers.SimpleRNN(128)) # see note below\n",
    "\n",
    "model_rnn.add(keras.layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification task, because we only need a single output at the end of the entire RNN, you only need to specify the total number of RNN units in the `SimpleRNN` function. You might be wondering - don't we need the input_size parameter? It is not necessary here because the preceding embedding layer automatically informs the RNN layer that each input text will have variable length (padded with special value 0 to max sequence length, and padding value 0 is automatically masked) and each word is represented by 64 dimensions in this example (i.e., the output dimension of the word embedding).\n",
    "\n",
    "<font color = \"red\">Question: Looking at the embedding layer, which method is it using to train the word embeddings (e.g., skip-gram, continuous bag-of-words, both, neither)?</font> This method/strategy of training a neural network model is known as **end-to-end training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure training / optimization\n",
    "model_rnn.compile(loss = keras.losses.BinaryCrossentropy(),\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 1s 11ms/step - loss: 0.6897 - accuracy: 0.5579 - val_loss: 0.6677 - val_accuracy: 0.6150\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.4860 - accuracy: 0.8350 - val_loss: 0.5263 - val_accuracy: 0.7500\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.1925 - accuracy: 0.9413 - val_loss: 0.4953 - val_accuracy: 0.7850\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 1s 7ms/step - loss: 0.0818 - accuracy: 0.9792 - val_loss: 0.5727 - val_accuracy: 0.7783\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0389 - accuracy: 0.9925 - val_loss: 0.6064 - val_accuracy: 0.7783\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0156 - accuracy: 0.9983 - val_loss: 0.7091 - val_accuracy: 0.7867\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0104 - accuracy: 0.9996 - val_loss: 0.7453 - val_accuracy: 0.7767\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.8131 - val_accuracy: 0.7683\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.8540 - val_accuracy: 0.7717\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 1s 8ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.8836 - val_accuracy: 0.7733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b128e90280>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training with 20% validation and 10 epochs.\n",
    "model_rnn.fit(x = text, y = label, validation_split = 0.2,\n",
    "              epochs=10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization_1 (TextVe (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 64)          345856    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 128)               24704     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 370,689\n",
      "Trainable params: 370,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03613052],\n",
       "       [0.999736  ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to make some predicitons\n",
    "model_rnn.predict([['I hate this meal!'], ['I love this restaurant']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Long-Term Dependency Problem <a name=\"dependency\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So Why Do We Need Anything More than Simple RNN?\n",
    "\n",
    "Here is a highly simplified (non-rigorous) derivation to help you understand. Suppose there is only 1 parameter, $w$, to learn, and the activitaion function, $f()$, is linear, and there is a constant input value of 0. Considering realistic inputs, non-linear activiation functions, and parameter matrices would make the derivation too hard to track but the underlying intuitions are the same. \n",
    "\n",
    "Think about how the hidden states of an RNN unit change over time:\n",
    "$$h_{t} = w \\cdot h_{t-1}$$\n",
    "in other words,\n",
    "$$h_{t} = w^t \\cdot h_0$$\n",
    "Now, the gradient of $h_t$ with respect to parameter $w$ is\n",
    "$$\\frac{dh_t}{dw} = tw^{t-1} h_0$$\n",
    "\n",
    "For a long sequence (i.e., $t$ is large), the **gradient will explode** (go to $\\infty$) even if $w$ is just slightly larger than 1, and the **gradient will vanish** (go to 0) even if $w$ is just slightly smaller than 1. This makes training RNN to learn from long sequences very hard. If the gradient is extremely large, gradient descent with any regular learning rate will be highly unstable. If the gradient is extremely small, gradient descent won't \"descent\" much at all.\n",
    "\n",
    "<font color='red'>Question: is this a problem for traditional, feed-forward neural networks with many hidden layers? If not, why not?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM) Model <a name=\"lstm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animated Illustration of a Single LSTM Unit <a name=\"lstm_figure\"></a>\n",
    "\n",
    "![Inside of a LSTM Unit](images/lstm.gif)\n",
    "\n",
    "image credit: https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does a Single LSTM Unit/Cell Work? <a name=\"lstm_tech\"></a>\n",
    "\n",
    "Here is a simplified version of how LSTM works to convey the key intuitions. A more technical description can be found [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "A LSTM unit has takes an input $X_t$, process it with a series of operations via **three \"gates\"** (respectively input gate, forget gate, and output gate), and then output $h_{t}$. A LSTM also has an **internal cell state**, $C_t$, which is different from the network's hidden state $h_{t-1}$.\n",
    "\n",
    "- **forget gate**: $forget_t = sigmoid(X_t, h_{t-1}, \\Theta_{forget})$\n",
    "- **input gate**: $input_t = sigmoid(X_t, h_{t-1}, \\Theta_{input})$\n",
    "- **output gate**: $output_t = sigmoid(X_t, h_{t-1}, \\Theta_{output})$\n",
    "\n",
    "<font color='blue'>Notice: The inner workings of the three gates are almost identical - each one applies a sigmoid function over the combination of input $X_t$ and previous hidden state $h_{t-1}$, with its own set of parameters.</font> Therefore, the best way to think about the purpose of these gates is that they act as \"weights\" (between 0 and 1, due to the sigmoid function).\n",
    "\n",
    "- **Update internal cell state**: $C_t = forget_t \\cdot C_{t-1} + input_t \\cdot tahn(X_t, h_{t-1}, \\Theta)$\n",
    "\n",
    "<font color='blue'>Notice: This is nothing but a weighted average!</font> The forget gate controls how much information from LSMT's previous internal state gets passed on to current internal state, and the input gate controls how much information from the new input gets passed on to current internal state. The $tahn(X_t, h_{t-1}, \\Theta)$ function, on its own, is exactly the same as in the simple RNN case. This design of internal cell state also has a fancy name - \"Constant Error Carousel\" (CEC).\n",
    "\n",
    "<font color=\"red\">Question: does this remind you of anything you learned from the time series forecasting course?</font> Hint: think of $forget_t \\cdot C_{t-1}$ as historical information and $input_t \\cdot tahn(X_t, h_{t-1}, \\Theta)$ as new information.\n",
    "\n",
    "- **Produce output**: $h_t = output_t \\cdot tahn(C_t)$\n",
    "\n",
    "The output gate controls how much information from the updated internal state gets passed on to current hidden state.\n",
    "\n",
    "<font color='blue'>In summary</font>, though it might seem complicated, the above steps are still trying to compute an updated hidden state $h_t$ based on the previous hidden state $h_{t-1}$ and the new input $X_t$. The three gates act like weights to control information flow, and the internal cell state _remembers_ information from the past. This is the intuition why LSTM can mitigate the vanishing gradient problem.\n",
    "\n",
    "However, the LSTM design does not necessarily address the exploding gradient problem. It's a bit technical to explain why, and I refer you to this [blog post](https://mochenyang.github.io/mochenyangblog/machine-learning/exposition/2022/04/11/LSTM-Gradient.html) for more details if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RNN with LSTM Units in Keras <a name=\"lstm_implementation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = keras.Sequential()\n",
    "\n",
    "model_lstm.add(vectorize_layer)\n",
    "\n",
    "model_lstm.add(keras.layers.Embedding(\n",
    "    input_dim = len(vectorize_layer.get_vocabulary()),\n",
    "    output_dim = 64,\n",
    "    mask_zero = True\n",
    "))\n",
    "\n",
    "model_lstm.add(keras.layers.LSTM(128))\n",
    "\n",
    "model_lstm.add(keras.layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure training / optimization\n",
    "model_lstm.compile(loss = keras.losses.BinaryCrossentropy(),\n",
    "                   optimizer='adam',\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 11s 75ms/step - loss: 0.6773 - accuracy: 0.5966 - val_loss: 0.5655 - val_accuracy: 0.7583\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.4064 - accuracy: 0.8712 - val_loss: 0.4680 - val_accuracy: 0.7917\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 2s 27ms/step - loss: 0.1905 - accuracy: 0.9502 - val_loss: 0.5194 - val_accuracy: 0.7967\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 2s 26ms/step - loss: 0.0885 - accuracy: 0.9784 - val_loss: 0.5099 - val_accuracy: 0.8083\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0625 - accuracy: 0.9840 - val_loss: 0.5844 - val_accuracy: 0.8167\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 2s 26ms/step - loss: 0.0340 - accuracy: 0.9939 - val_loss: 0.6914 - val_accuracy: 0.8067\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0150 - accuracy: 0.9981 - val_loss: 0.8738 - val_accuracy: 0.7833\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 2s 25ms/step - loss: 0.0144 - accuracy: 0.9974 - val_loss: 1.0673 - val_accuracy: 0.7833\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 2s 26ms/step - loss: 0.0246 - accuracy: 0.9925 - val_loss: 0.8362 - val_accuracy: 0.8083\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 2s 26ms/step - loss: 0.0111 - accuracy: 0.9968 - val_loss: 0.8860 - val_accuracy: 0.8033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x179a9615a00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training with 20% validation and 10 epochs.\n",
    "model_lstm.fit(x = text, y = label, validation_split = 0.2,\n",
    "               epochs=10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02249685],\n",
       "       [0.99999833]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to make some predicitons\n",
    "model_lstm.predict([['I hate this meal!'], ['I love this restaurant']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 64)          345856    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 444,801\n",
      "Trainable params: 444,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Unit (GRU) Model <a name=\"gru\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animated Illustration of a Single GRU <a name=\"gru_figure\"></a>\n",
    "\n",
    "![Inside of a GRU Unit](images/gru.gif)\n",
    "\n",
    "image credit: https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does a Single GRU Work? <a name=\"gru_tech\"></a>\n",
    "\n",
    "A GRU has two gates: an update gate and a reset gate. \n",
    "\n",
    "- **Update gate**: $update_t = sigmoid(X_t, h_{t-1}, \\Theta_{update})$\n",
    "- **Reset gate**: $reset_t = sigmoid(X_t, h_{t-1}, \\Theta_{reset})$\n",
    "- **Produce output**: $h_t = update_t \\cdot h_{t-1} + (1-update_t) \\cdot tahn(X_t, reset_t \\cdot h_{t-1}, \\Theta)$\n",
    "\n",
    "<font color='blue'>Notice: </font> Just like in LSTM, the two gates of GRU are weights. The update gate controls how much information from previous hidden state $h_{t-1}$ gets passed on to current hidden state $h_t$, and the reset gate controls how much information from previous hidden state gets to be combined with current input $X_t$.\n",
    "\n",
    "<font color='blue'>Interestingly</font>, even though GRU seems simpler and works basically as well as LSTM, it was proposed later than LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RNN with GRUs in Keras <a name=\"gru_implementation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = keras.Sequential()\n",
    "\n",
    "model_gru.add(vectorize_layer)\n",
    "\n",
    "model_gru.add(keras.layers.Embedding(\n",
    "    input_dim = len(vectorize_layer.get_vocabulary()),\n",
    "    output_dim = 64,\n",
    "    mask_zero = True\n",
    "))\n",
    "\n",
    "model_gru.add(keras.layers.GRU(128))\n",
    "\n",
    "model_gru.add(keras.layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure training / optimization\n",
    "model_gru.compile(loss = keras.losses.BinaryCrossentropy(),\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 4s 31ms/step - loss: 0.6808 - accuracy: 0.5511 - val_loss: 0.5101 - val_accuracy: 0.7767\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.3536 - accuracy: 0.8766 - val_loss: 0.4265 - val_accuracy: 0.8083\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.1402 - accuracy: 0.9627 - val_loss: 0.4923 - val_accuracy: 0.8150\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0488 - accuracy: 0.9879 - val_loss: 0.5205 - val_accuracy: 0.8167\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0434 - accuracy: 0.9908 - val_loss: 0.5580 - val_accuracy: 0.8017\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 2s 22ms/step - loss: 0.0416 - accuracy: 0.9917 - val_loss: 0.6560 - val_accuracy: 0.8100\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 0.0210 - accuracy: 0.9951 - val_loss: 0.9197 - val_accuracy: 0.7967\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 0.0132 - accuracy: 0.9985 - val_loss: 0.8774 - val_accuracy: 0.7967\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 1.0786 - val_accuracy: 0.7883\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 2s 21ms/step - loss: 0.0182 - accuracy: 0.9941 - val_loss: 1.0432 - val_accuracy: 0.7717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x179b3074610>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training with 20% validation and 10 epochs.\n",
    "model_gru.fit(x = text, y = label, validation_split = 0.2,\n",
    "              epochs=10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01605341],\n",
       "       [0.9999916 ]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to make some predicitons\n",
    "model_gru.predict([['I hate this meal!'], ['I love this restaurant']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 64)          345856    \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 128)               74496     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 420,481\n",
      "Trainable params: 420,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional RNN Models <a name=\"birnn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of text classification, an intuitive understanding of forward (i.e., one-directional) RNN is that it \"reads\" a piece of text from beginning to end, and produces a prediction. By the same analogy, a bidirectional RNN would read the text from beginning to end and also from end to beginning, then produces a prediction.\n",
    "\n",
    "To illustrate how a bidirectional RNN works, let's consider the simple RNN units as an example. Essentially, instead of keeping one set of hidden states $h_t$ that move forward in time, a bidirectional RNN also keep another set of hidden states $g_t$ that move backward in time. Like this:\n",
    "$$h_{t} = f(h_{t-1}, X_{t}, \\Theta_h)$$\n",
    "$$g_{t} = f(g_{t+1}, X_{t}, \\Theta_g)$$\n",
    "\n",
    "Under this general structure, you can replace the simple RNN units with LSTM or GRU, which would give rise to bi-LSTM and bi-GRU models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Bidirectional RNN Model in Keras <a name=\"birnn_implementation\"></a>\n",
    "\n",
    "Specifically, let's build a bidirectional LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bilstm = keras.Sequential()\n",
    "\n",
    "model_bilstm.add(vectorize_layer)\n",
    "\n",
    "model_bilstm.add(keras.layers.Embedding(\n",
    "    input_dim = len(vectorize_layer.get_vocabulary()),\n",
    "    output_dim = 64,\n",
    "    mask_zero = True\n",
    "))\n",
    "\n",
    "model_bilstm.add(keras.layers.Bidirectional(keras.layers.LSTM(128)))\n",
    "\n",
    "model_bilstm.add(keras.layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure training / optimization\n",
    "model_bilstm.compile(loss = keras.losses.BinaryCrossentropy(),\n",
    "                     optimizer='adam',\n",
    "                     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "75/75 [==============================] - 7s 97ms/step - loss: 0.6309 - accuracy: 0.6704 - val_loss: 0.5082 - val_accuracy: 0.7667\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 3s 40ms/step - loss: 0.3136 - accuracy: 0.9008 - val_loss: 0.4528 - val_accuracy: 0.7850\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 3s 42ms/step - loss: 0.1349 - accuracy: 0.9621 - val_loss: 0.4401 - val_accuracy: 0.8117\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 3s 36ms/step - loss: 0.0757 - accuracy: 0.9808 - val_loss: 0.7320 - val_accuracy: 0.7883\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 3s 38ms/step - loss: 0.0476 - accuracy: 0.9871 - val_loss: 0.7814 - val_accuracy: 0.7950\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 3s 38ms/step - loss: 0.0322 - accuracy: 0.9908 - val_loss: 0.7875 - val_accuracy: 0.7900\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 3s 37ms/step - loss: 0.0205 - accuracy: 0.9954 - val_loss: 0.9245 - val_accuracy: 0.8133\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 3s 36ms/step - loss: 0.0115 - accuracy: 0.9983 - val_loss: 1.1594 - val_accuracy: 0.8067\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 3s 39ms/step - loss: 0.0070 - accuracy: 0.9992 - val_loss: 1.3125 - val_accuracy: 0.8083\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 3s 34ms/step - loss: 0.0043 - accuracy: 0.9992 - val_loss: 1.4194 - val_accuracy: 0.8183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bcccd34a90>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training with 20% validation and 10 epochs.\n",
    "model_bilstm.fit(x = text, y = label, validation_split = 0.2,\n",
    "                 epochs = 10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00192887],\n",
       "       [0.9999987 ]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to make some predicitons\n",
    "model_bilstm.predict([['I hate this meal!'], ['I love this restaurant']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources <a name=\"resources\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LSTM paper: [LSTM: A Search Space Odyssey](https://arxiv.org/pdf/1503.04069.pdf?fbclid=IwAR377Jhphz_xGSSThcqGUlAx8OJc_gU6Zwq8dABHOdS4WNOPRXA5LcHOjUg)\n",
    "- GRU paper: [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/pdf/1409.1259.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
